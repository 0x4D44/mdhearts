# High-Level Design: AI-Assisted Training and Playing Enhancements

**Document Version:** 1.1 (Revised)
**Date:** October 5, 2025
**Status:** Proposed
**Author:** Senior Engineering Team
**Reviewers:** TBD
**Changelog:**
- v1.1: Fixed 25 issues from initial review (moon rewards, trick encoding, training backend, parameter counts, etc.)
- v1.0: Initial draft

---

## Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [Goals and Non-Goals](#2-goals-and-non-goals)
3. [Background](#3-background)
4. [Architecture Overview](#4-architecture-overview)
5. [Detailed Design](#5-detailed-design)
   - 5.1 [Policy Abstraction Layer](#51-policy-abstraction-layer)
   - 5.2 [RL Environment Wrapper](#52-rl-environment-wrapper)
   - 5.3 [Observation Encoding](#53-observation-encoding)
   - 5.4 [Embedded Model Architecture](#54-embedded-model-architecture)
   - 5.5 [Training System](#55-training-system)
   - 5.6 [CLI Extensions](#56-cli-extensions)
   - 5.7 [Embedded Weights System](#57-embedded-weights-system)
6. [Key Design Decisions](#6-key-design-decisions)
7. [Implementation Roadmap](#7-implementation-roadmap)
8. [Testing Strategy](#8-testing-strategy)
9. [Performance Targets](#9-performance-targets)
10. [Risks and Mitigations](#10-risks-and-mitigations)
11. [Open Questions](#11-open-questions)
12. [Appendices](#12-appendices)

---

## 1. Executive Summary

This document describes the design for adding reinforcement learning (RL) capabilities to mdhearts while maintaining the existing single-executable architecture. The enhancement enables:

1. **Interactive play** against an embedded neural network policy
2. **Headless training** using self-play PPO (Proximal Policy Optimization)
3. **Evaluation tools** for measuring policy performance
4. **Zero runtime dependencies** - model weights baked into the executable

**Key Principle:** Extend existing codebase (`hearts-core`, `bot/`) rather than duplicate functionality.

**Training Approach:** Use PyTorch for training (Python), export weights to Rust for inference.

### Critical Constraints

- Single executable (no external model files at runtime)
- Reuse existing `hearts-core` game engine
- Integrate with existing `PassPlanner`/`PlayPlanner` bot system
- Maintain backward compatibility with current UI and CLI
- Pass existing `cargo clippy` and `cargo fmt` requirements
- Support both Passing and Playing phases of Hearts

---

## 2. Goals and Non-Goals

### Goals

✅ **G1:** Embed trained neural network policy into single executable
✅ **G2:** Support headless training mode (PPO self-play)
✅ **G3:** Reuse existing `hearts-core` game logic (no duplication)
✅ **G4:** Integrate with existing bot system via unified `Policy` trait
✅ **G5:** Handle both Passing phase (choose 3 cards) and Playing phase (choose 1 card)
✅ **G6:** Support multi-agent self-play with opponent mixing
✅ **G7:** Enable deterministic replay for debugging (extend snapshot system)
✅ **G8:** Provide evaluation tools (Elo rating, win rate vs baselines)

### Non-Goals

❌ **NG1:** Multi-process distributed training (single machine only for MVP)
❌ **NG2:** GPU acceleration (CPU-only inference for simplicity)
❌ **NG3:** Online learning during interactive play (frozen policy)
❌ **NG4:** Advanced RL algorithms beyond PPO (future enhancement)
❌ **NG5:** Cross-platform UI (Windows only for interactive mode; Linux headless OK)

---

## 3. Background

### 3.1 Existing Architecture

mdhearts is a Rust workspace with three crates:

- **hearts-core:** Pure game logic (Card, Hand, RoundState, MatchState)
- **hearts-ui:** Presentation metadata and themes
- **hearts-app:** Platform entry point (Win32/Direct2D UI, CLI, bot AI)

### 3.2 Existing Bot System

`hearts-app/src/bot/` implements sophisticated heuristic AI:

- **BotDifficulty:** `EasyLegacy`, `NormalHeuristic`, `FutureHard`
- **BotStyle:** `Cautious`, `AggressiveMoon`, `HuntLeader` (adaptive)
- **PassPlanner:** 367 lines of tested passing logic
- **PlayPlanner:** Card play decision engine
- **UnseenTracker:** Tracks revealed cards for opponent modeling

### 3.3 Hearts Game Phases

Hearts has **two distinct phases** per round:

1. **Passing Phase:** Each player selects 3 cards to pass to another player
   - Direction cycles: Left → Right → Across → Hold (no passing)
   - Action space: Choose 3 cards from hand
   - All 4 players submit passes before exchanging cards

2. **Playing Phase:** Players play 1 card per trick (13 tricks total)
   - Must follow suit if possible
   - Hearts cannot lead until broken (unless void)
   - Action space: Choose 1 legal card from hand

**Critical:** Any RL solution must handle both phases with different action spaces.

### 3.4 Multi-Player Dynamics

Hearts is a **4-player zero-sum game:**

- Your score depends on all 4 players' actions
- Strategy varies based on cumulative match scores
- Shooting the moon (taking all 26 points → opponents get +26 each) is high-risk/high-reward
- Existing bots already adapt strategy based on score gaps (e.g., "hunt leader" when opponent near 100)

---

## 4. Architecture Overview

### 4.1 Module Structure

```
mdhearts (single Rust executable)
├── main.rs                      # CLI router (extend existing)
├── cli.rs                       # EXTEND: add AI subcommands (clap)
├── controller.rs                # EXTEND: use Policy trait
│
├── policy/                      # NEW: Unified policy interface
│   ├── mod.rs                   # Policy trait + factory
│   ├── heuristic.rs             # Wraps existing bot/* logic
│   ├── embedded.rs              # MLP inference (baked weights)
│   └── adapter.rs               # LegacyBotAdapter
│
├── rl/                          # NEW: RL-specific modules
│   ├── env.rs                   # HeartsEnv (wraps MatchState)
│   ├── observation.rs           # ObservationBuilder
│   ├── trainer.rs               # Training orchestrator (Rust side)
│   └── eval.rs                  # Match runner (metrics)
│
├── weights/                     # NEW: Embedded model
│   ├── generated.rs             # AUTO-GENERATED const arrays
│   └── manifest.rs              # Schema hash, version
│
├── tools/                       # NEW (dev-model-io)
│   └── gen_weights.rs           # .npz → generated.rs converter
│
├── bot/                         # EXISTING (extended)
│   ├── mod.rs, pass.rs, play.rs # Existing planners
│   └── tracker.rs               # EXTEND: add infer_voids()
│
├── platform/                    # EXISTING (unchanged)
│   └── win32.rs                 # Win32/Direct2D UI
│
└── (workspace root)
    ├── hearts-core/             # EXISTING (minimal extension)
    │   └── model/card.rs        # ADD: Card::to_id(), from_id()
    │
    ├── hearts-app/              # THIS CRATE
    │
    └── tools/                   # NEW: Python training scripts
        ├── train.py             # PyTorch PPO trainer
        └── gen_dummy_weights.py # Initial weights generator
```

### 4.2 Data Flow

#### Interactive Play (UI Mode)
```
User Click → GameController::execute_ai_turn()
            → Policy::choose_play(PolicyContext)
            → EmbeddedPolicy::forward_trunk()
            → masked_argmax(logits, legal_moves)
            → RoundState::play_card(card)
            → UI Animation
```

#### Training (Headless Python + Rust Env)
```
Python: PPO Trainer
  ↓
  Calls Rust env via FFI or subprocess
  ↓
Rust: HeartsEnv::step_pass/play()
  ↓
  Returns (obs, reward, done)
  ↓
Python: Collect rollouts, compute PPO update
  ↓
  Export weights to .npz
  ↓
gen-weights: Convert to Rust const arrays
  ↓
cargo build: Embed weights into binary
```

---

## 5. Detailed Design

### 5.1 Policy Abstraction Layer

**Purpose:** Unified interface for AI decision-making (heuristic and learned policies).

#### 5.1.1 Core Trait

```rust
// crates/hearts-app/src/policy/mod.rs

pub trait Policy: Send {
    /// Choose 3 cards to pass (called during Passing phase)
    fn choose_pass(&mut self, ctx: &PolicyContext) -> [Card; 3];

    /// Choose 1 card to play (called during Playing phase)
    fn choose_play(&mut self, ctx: &PolicyContext) -> Card;

    /// Optional: Observe terminal state (for RL policies)
    fn observe_terminal(&mut self, _final_scores: &[u32; 4]) {}
}

pub struct PolicyContext<'a> {
    pub seat: PlayerPosition,
    pub hand: &'a Hand,
    pub round: &'a RoundState,
    pub scores: &'a ScoreBoard,
    pub passing_direction: PassingDirection,
    pub tracker: &'a UnseenTracker,
}
```

**Key Design Decision:** Separate methods for passing and playing phases because action spaces differ fundamentally.

#### 5.1.2 Policy Factory

```rust
pub enum PolicyKind {
    EasyLegacy,
    NormalHeuristic,
    FutureHard,
    Embedded,
}

impl PolicyKind {
    pub fn create(&self) -> Box<dyn Policy> {
        match self {
            Self::EasyLegacy => Box::new(HeuristicPolicy::easy()),
            Self::NormalHeuristic => Box::new(HeuristicPolicy::normal()),
            Self::FutureHard => Box::new(HeuristicPolicy::hard()),
            Self::Embedded => Box::new(EmbeddedPolicy::from_baked_weights()),
        }
    }
}
```

#### 5.1.3 Legacy Bot Adapter

**Purpose:** Wrap existing `PassPlanner`/`PlayPlanner` to implement `Policy` trait (zero code duplication).

```rust
// crates/hearts-app/src/policy/adapter.rs

pub struct LegacyBotAdapter {
    difficulty: BotDifficulty,
}

impl Policy for LegacyBotAdapter {
    fn choose_pass(&mut self, ctx: &PolicyContext) -> [Card; 3] {
        let bot_ctx = BotContext::new(
            ctx.seat, ctx.round, ctx.scores,
            ctx.passing_direction, ctx.tracker, self.difficulty
        );
        PassPlanner::choose(ctx.hand, &bot_ctx)
            .expect("PassPlanner returns valid pass")
    }

    fn choose_play(&mut self, ctx: &PolicyContext) -> Card {
        let bot_ctx = BotContext::new(
            ctx.seat, ctx.round, ctx.scores,
            ctx.passing_direction, ctx.tracker, self.difficulty
        );

        // Compute legal moves on-demand
        let legal_moves = compute_legal_moves(ctx.hand, ctx.round);

        PlayPlanner::choose(&bot_ctx, &legal_moves)
            .expect("PlayPlanner returns valid card")
    }
}

fn compute_legal_moves(hand: &Hand, round: &RoundState) -> Vec<Card> {
    hand.iter()
        .copied()
        .filter(|&card| {
            let mut probe = round.clone();
            probe.play_card(PlayerPosition::South, card).is_ok()
        })
        .collect()
}
```

**Benefit:** Existing 367 lines of tested bot logic reused without modification.

---

### 5.2 RL Environment Wrapper

**Purpose:** Thin RL interface around existing `MatchState`/`RoundState` (no duplicate game logic).

#### 5.2.1 Core API

```rust
// crates/hearts-app/src/rl/env.rs

pub struct HeartsEnv {
    match_state: MatchState,  // REUSE existing type
    current_seat: PlayerPosition,
    step_count: u32,
    config: EnvConfig,
}

pub struct Step {
    pub obs: Observation,
    pub reward: f32,
    pub done: bool,
    pub info: StepInfo,
}

pub struct StepInfo {
    pub phase: PhaseInfo,
    pub round_complete: bool,
    pub final_points: Option<[u32; 4]>,
}

pub enum PhaseInfo {
    Passing { direction: PassingDirection, submitted: usize },
    Playing { trick_leader: PlayerPosition, cards_in_trick: usize },
}

impl HeartsEnv {
    pub fn new(seed: u64, config: EnvConfig) -> Self;
    pub fn reset(&mut self) -> Observation;
    pub fn step_pass(&mut self, cards: [Card; 3]) -> Result<Step, String>;
    pub fn step_play(&mut self, card: Card) -> Result<Step, String>;
    pub fn legal_moves(&self) -> Vec<Card>;
    pub fn phase(&self) -> PhaseInfo;
}
```

#### 5.2.2 Passing Phase Turn Management

**Sequence Diagram:**
```
Passing Phase (PassingDirection != Hold):
1. env.phase() → Passing { submitted: 0/4 }
2. step_pass(seat=South, cards) → done=false, phase: Passing { submitted: 1/4 }
3. step_pass(seat=West, cards)  → done=false, phase: Passing { submitted: 2/4 }
4. step_pass(seat=North, cards) → done=false, phase: Passing { submitted: 3/4 }
5. step_pass(seat=East, cards)  → done=false, phase: Playing (transition!)
6. env.phase() → Playing { leader: South (has 2♣) }
```

**Implementation:**

```rust
impl HeartsEnv {
    pub fn step_pass(&mut self, cards: [Card; 3]) -> Result<Step, String> {
        let round = self.match_state.round_mut();

        // Submit for current seat
        round.submit_pass(self.current_seat, cards)
            .map_err(|e| format!("Invalid pass: {:?}", e))?;

        // Check if all 4 players have submitted
        if let RoundPhase::Passing(ref state) = round.phase() {
            if state.all_submitted() {
                // All passes submitted, execute the exchange
                round.complete_passing()
                    .map_err(|e| format!("Pass completion failed: {:?}", e))?;

                // Phase transitions to Playing
                self.current_seat = round.starting_player();

                return Ok(Step {
                    obs: self.build_observation(),
                    reward: 0.0,  // No reward until round complete
                    done: false,
                    info: StepInfo {
                        phase: PhaseInfo::Playing {
                            trick_leader: self.current_seat,
                            cards_in_trick: 0,
                        },
                        round_complete: false,
                        final_points: None,
                    },
                });
            } else {
                // More passes needed, advance to next seat
                self.current_seat = self.current_seat.next();

                return Ok(Step {
                    obs: self.build_observation(),
                    reward: 0.0,
                    done: false,
                    info: StepInfo {
                        phase: PhaseInfo::Passing {
                            direction: self.match_state.passing_direction(),
                            submitted: state.submitted_count(),
                        },
                        round_complete: false,
                        final_points: None,
                    },
                });
            }
        }

        Err("Not in passing phase".to_string())
    }
}
```

#### 5.2.3 Reward Computation (FIXED)

```rust
pub struct EnvConfig {
    pub reward_mode: RewardMode,
}

pub enum RewardMode {
    /// Relative to average opponent score (recommended)
    Relative,
    /// Rank-based: 1st=+3, 2nd=+1, 3rd=-1, 4th=-3
    Rank,
}

impl HeartsEnv {
    fn compute_round_points(&self) -> [u32; 4] {
        let round = self.match_state.round();
        let mut points = [0u32; 4];

        for seat in PlayerPosition::LOOP.iter().copied() {
            points[seat.index()] = round.points_taken(seat);
        }

        // Return RAW points (don't apply moon transformation here)
        points
    }

    fn compute_reward(&self, final_points: &[u32; 4]) -> f32 {
        // Detect successful moon shooting: one player has 26, others have 0
        let moon_shooter = final_points.iter()
            .position(|&p| p == 26);

        if let Some(shooter_idx) = moon_shooter {
            // Verify it's a real moon (others have 0)
            let others_total: u32 = final_points.iter()
                .enumerate()
                .filter(|(i, _)| *i != shooter_idx)
                .map(|(_, &p)| p)
                .sum();

            if others_total == 0 {
                // Successful moon! Shooter gets huge reward, victims get penalty
                let mut rewards = [-26.0f32; 4];  // Victims get penalized
                rewards[shooter_idx] = 78.0;       // Shooter gets 3×26 advantage
                return rewards[self.current_seat.index()];
            }
        }

        // Normal scoring: relative rewards
        let my_points = final_points[self.current_seat.index()] as f32;

        match self.config.reward_mode {
            RewardMode::Relative => {
                let opponent_avg: f32 = final_points.iter()
                    .enumerate()
                    .filter(|(i, _)| *i != self.current_seat.index())
                    .map(|(_, &p)| p as f32)
                    .sum::<f32>() / 3.0;

                opponent_avg - my_points  // Positive when winning
            }

            RewardMode::Rank => {
                let rank = self.compute_rank(final_points);
                match rank {
                    1 => 3.0,
                    2 => 1.0,
                    3 => -1.0,
                    4 => -3.0,
                    _ => 0.0,
                }
            }
        }
    }

    fn compute_rank(&self, points: &[u32; 4]) -> usize {
        let my_points = points[self.current_seat.index()];
        let better_count = points.iter()
            .filter(|&&p| p < my_points)
            .count();
        better_count + 1  // Rank is 1-indexed
    }
}
```

---

### 5.3 Observation Encoding

**Purpose:** Convert game state to fixed-size feature vector for neural network input.

#### 5.3.1 Feature Schema (Version 1.1) - UPDATED

```rust
// crates/hearts-app/src/rl/observation.rs

pub struct Observation {
    // === Hand features (52) ===
    pub hand_onehot: [f32; 52],  // 1.0 if card in hand

    // === Seen cards (52) ===
    pub seen_onehot: [f32; 52],  // 1.0 if card revealed

    // === Current trick (75) === UPDATED from 60
    pub trick_led_suit: [f32; 4],           // One-hot encoding
    pub trick_cards: [[f32; 17]; 4],        // 4 cards × (4 suit + 13 rank) = 68
    pub trick_count: f32,                   // How many cards played (0-4)
    pub my_trick_position: f32,             // My position in trick (0-1 normalized)

    // === Scores (4) ===
    pub scores_relative: [f32; 4],  // [me, left, across, right] / 100.0

    // === Game state (7) ===
    pub hearts_broken: f32,
    pub tricks_completed: f32,              // 0-13 normalized
    pub passing_phase: f32,
    pub passing_direction: [f32; 4],        // One-hot

    // === Opponent void inference (12) ===
    pub opp_voids: [f32; 12],  // 3 opponents × 4 suits

    // === History (68) ===
    pub last_4_cards: [f32; 68],  // 4 cards × 17 features
}

// UPDATED feature dimension
pub const FEATURE_DIM: usize = 270;  // Was 255, now 270
// Breakdown: 52 + 52 + 75 + 4 + 7 + 12 + 68 = 270
```

#### 5.3.2 Observation Builder

```rust
impl ObservationBuilder {
    /// Build observation for current policy context (phase-agnostic)
    pub fn build(&self, ctx: &PolicyContext) -> Observation {
        Observation {
            hand_onehot: self.encode_hand(ctx.hand),
            seen_onehot: self.encode_seen(ctx.round, ctx.tracker),

            // Trick encoding (handles 0-4 cards)
            ..self.encode_trick(ctx.round.current_trick(), ctx.seat)

            scores_relative: self.encode_scores_relative(ctx.scores, ctx.seat),
            hearts_broken: ctx.round.hearts_broken() as u8 as f32,
            tricks_completed: (ctx.round.tricks_completed() as f32) / 13.0,
            passing_phase: matches!(ctx.round.phase(), RoundPhase::Passing(_)) as u8 as f32,
            passing_direction: self.encode_direction(ctx.passing_direction),
            opp_voids: self.encode_opponent_voids(ctx),
            last_4_cards: self.encode_history(ctx.round),
        }
    }

    fn encode_trick(&self, trick: &Trick, my_seat: PlayerPosition) -> TrickFeatures {
        let mut cards = [[0.0f32; 17]; 4];
        let count = trick.cards_played();

        // Encode each card in play order (relative to trick leader)
        for (idx, (seat, card)) in trick.cards_iter().enumerate() {
            cards[idx] = self.encode_card(card);
        }

        // Empty slots remain all zeros

        let my_position = if trick.has_played(my_seat) {
            // Already played: position is fixed
            trick.position_of(my_seat) as f32 / 4.0
        } else {
            // Haven't played yet: position is current count
            count as f32 / 4.0
        };

        TrickFeatures {
            led_suit: self.encode_suit_onehot(trick.led_suit()),
            cards,
            count: count as f32 / 4.0,
            my_position,
        }
    }

    fn encode_card(&self, card: Card) -> [f32; 17] {
        let mut features = [0.0f32; 17];

        // One-hot suit (4 features)
        features[card.suit as usize] = 1.0;

        // One-hot rank (13 features: 2-A)
        features[4 + (card.rank.value() as usize - 2)] = 1.0;

        features
    }

    fn encode_scores_relative(&self, scores: &ScoreBoard, my_seat: PlayerPosition) -> [f32; 4] {
        [
            scores.score(my_seat) as f32 / 100.0,
            scores.score(my_seat.next()) as f32 / 100.0,
            scores.score(my_seat.opposite()) as f32 / 100.0,
            scores.score(my_seat.previous()) as f32 / 100.0,
        ]
    }

    fn encode_opponent_voids(&self, ctx: &PolicyContext) -> [f32; 12] {
        let voids = ctx.tracker.opponent_voids(ctx.seat, ctx.round);
        let mut flat = [0.0f32; 12];

        for (opp_idx, opp_voids) in voids.iter().enumerate() {
            for (suit_idx, &is_void) in opp_voids.iter().enumerate() {
                flat[opp_idx * 4 + suit_idx] = is_void as u8 as f32;
            }
        }

        flat
    }
}

struct TrickFeatures {
    led_suit: [f32; 4],
    cards: [[f32; 17]; 4],
    count: f32,
    my_position: f32,
}
```

#### 5.3.3 Card ID Mapping

**Extension to `hearts-core`:**

```rust
// crates/hearts-core/src/model/card.rs (extend existing)

impl Card {
    /// Convert to dense ID for ML (0-51)
    /// Mapping: Clubs 2-A (0-12), Diamonds 2-A (13-25), Hearts 2-A (26-38), Spades 2-A (39-51)
    pub const fn to_id(self) -> u8 {
        (self.suit as u8) * 13 + self.rank.value() - 2
    }

    /// Convert from dense ID
    pub const fn from_id(id: u8) -> Self {
        let suit = Suit::from_index((id / 13) as usize);
        let rank = Rank::from_value((id % 13) + 2);
        Card::new(rank, suit)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_card_id_mapping() {
        // Test all 52 cards
        for suit_idx in 0..4 {
            for rank_val in 2..=14 {
                let suit = Suit::from_index(suit_idx).unwrap();
                let rank = Rank::from_value(rank_val);
                let card = Card::new(rank, suit);

                let id = card.to_id();
                assert!(id < 52);

                let reconstructed = Card::from_id(id);
                assert_eq!(reconstructed, card);
            }
        }

        // Test boundaries
        assert_eq!(Card::new(Rank::Two, Suit::Clubs).to_id(), 0);
        assert_eq!(Card::new(Rank::Ace, Suit::Spades).to_id(), 51);
    }
}
```

#### 5.3.4 Void Inference (UnseenTracker Extension)

```rust
// crates/hearts-app/src/bot/tracker.rs (extend existing)

impl UnseenTracker {
    /// Deduce which suits an opponent is void in based on trick history
    pub fn infer_voids(&self, seat: PlayerPosition, round: &RoundState) -> [bool; 4] {
        let mut voids = [false; 4];

        for trick in round.trick_history() {
            if let Some(led_suit) = trick.led_suit() {
                if let Some(card) = trick.card_played_by(seat) {
                    // If player didn't follow suit, they must be void
                    if card.suit != led_suit {
                        voids[led_suit as usize] = true;
                    }
                }
            }
        }

        voids
    }

    /// Get void information for all opponents relative to a seat
    pub fn opponent_voids(&self, my_seat: PlayerPosition, round: &RoundState)
        -> [[bool; 4]; 3]
    {
        [
            self.infer_voids(my_seat.next(), round),
            self.infer_voids(my_seat.opposite(), round),
            self.infer_voids(my_seat.previous(), round),
        ]
    }
}
```

#### 5.3.5 Schema Versioning (AUTOMATED)

```rust
// build.rs (NEW)

use sha2::{Sha256, Digest};

fn main() {
    // Compute schema hash at build time
    let schema_desc = concat!(
        "v1.1.0:",  // Updated version
        "hand_onehot[52],",
        "seen_onehot[52],",
        "trick_led_suit[4],",
        "trick_cards[4][17],",     // Changed from [3][17]
        "trick_count[1],",         // NEW
        "my_trick_position[1],",   // NEW
        "scores_relative[4],",
        "hearts_broken[1],",
        "tricks_completed[1],",
        "passing_phase[1],",
        "passing_direction[4],",
        "opp_voids[12],",
        "last_4_cards[68]",
    );

    let mut hasher = Sha256::new();
    hasher.update(schema_desc.as_bytes());
    let hash = format!("{:x}", hasher.finalize());

    // Expose as environment variable
    println!("cargo:rustc-env=SCHEMA_HASH={}", hash);
    println!("cargo:rustc-env=SCHEMA_VERSION=v1.1.0");
    println!("cargo:rustc-env=FEATURE_DIM=270");

    // Rebuild if schema changes
    println!("cargo:rerun-if-changed=build.rs");
    println!("cargo:rerun-if-changed=src/rl/observation.rs");
}
```

```rust
// crates/hearts-app/src/weights/manifest.rs

pub const SCHEMA_VERSION: &str = env!("SCHEMA_VERSION");
pub const SCHEMA_HASH: &str = env!("SCHEMA_HASH");
pub const FEATURE_DIM: usize = 270;

pub struct ModelManifest {
    pub schema_version: &'static str,
    pub schema_hash: &'static str,
    pub feature_dim: usize,
    pub architecture: &'static str,
    pub total_params: usize,
    pub export_time: &'static str,
    pub training_commit: &'static str,
}

impl ModelManifest {
    pub const fn default() -> Self {
        Self {
            schema_version: SCHEMA_VERSION,
            schema_hash: SCHEMA_HASH,
            feature_dim: FEATURE_DIM,
            architecture: "mlp(270→512→512→256, dual_heads)",
            total_params: 558_601,  // CORRECTED
            export_time: "unknown",
            training_commit: "unknown",
        }
    }
}
```

**CI Test:**

```rust
// tests/schema_stability.rs

#[test]
fn test_schema_unchanged_since_last_model_train() {
    // This hash matches the model currently embedded in generated.rs
    // Update this constant when you retrain the model
    const TRAINED_MODEL_SCHEMA_HASH: &str =
        "UPDATE_AFTER_FIRST_TRAINING";

    let current_hash = env!("SCHEMA_HASH");

    if TRAINED_MODEL_SCHEMA_HASH != "UPDATE_AFTER_FIRST_TRAINING" {
        assert_eq!(
            current_hash,
            TRAINED_MODEL_SCHEMA_HASH,
            "\n\nSCHEMA CHANGED!\n\
             Current: {}\n\
             Trained model expects: {}\n\n\
             Options:\n\
             1. Revert observation changes\n\
             2. Retrain model with new schema\n\
             3. Update TRAINED_MODEL_SCHEMA_HASH\n",
            current_hash,
            TRAINED_MODEL_SCHEMA_HASH
        );
    }
}
```

---

### 5.4 Embedded Model Architecture

**Purpose:** Lightweight MLP for policy and value estimation.

#### 5.4.1 Network Architecture (CORRECTED)

```
Input: Observation (270 features)  ← UPDATED from 255
  ↓
Dense(270 → 512) + SiLU  ← UPDATED input dim
  ↓
Dense(512 → 512) + SiLU
  ↓
Dense(512 → 256) + SiLU
  ↓ (split into 3 heads)
  ├→ Pass Head: Dense(256 → 52)    [score each card for passing]
  ├→ Play Head: Dense(256 → 52)    [logits for card selection]
  └→ Value Head: Dense(256 → 1)    [state value estimate]

Total Parameters: 558,601 (fp32: ~2.2MB, int8: ~0.56MB)
```

**Parameter Breakdown:**
- w1: 270 × 512 = 138,240
- b1: 512
- w2: 512 × 512 = 262,144
- b2: 512
- w3: 512 × 256 = 131,072
- b3: 256
- w_pass: 256 × 52 = 13,312
- b_pass: 52
- w_play: 256 × 52 = 13,312
- b_play: 52
- w_value: 256 × 1 = 256
- b_value: 1
- **Total: 558,601 parameters**

#### 5.4.2 Weights Structure

```rust
// crates/hearts-app/src/weights/mod.rs

pub struct Weights {
    // Shared trunk
    pub w1: &'static [f32],  // [512, 270] in row-major
    pub b1: &'static [f32],
    pub w2: &'static [f32],  // [512, 512]
    pub b2: &'static [f32],
    pub w3: &'static [f32],  // [512, 256]
    pub b3: &'static [f32],

    // Dual heads
    pub w_play: &'static [f32],  // [52, 256]
    pub b_play: &'static [f32],
    pub w_pass: &'static [f32],  // [52, 256]
    pub b_pass: &'static [f32],
    pub w_value: &'static [f32], // [1, 256]
    pub b_value: &'static [f32],
}
```

#### 5.4.3 Inference Implementation

```rust
// crates/hearts-app/src/policy/embedded.rs

pub struct EmbeddedPolicy {
    weights: &'static Weights,
    obs_builder: ObservationBuilder,
    rng: StdRng,
    deterministic: bool,
}

impl EmbeddedPolicy {
    pub fn from_baked_weights() -> Self {
        // Validate schema compatibility
        let runtime_hash = env!("SCHEMA_HASH");
        if runtime_hash != MODEL_MANIFEST.schema_hash {
            panic!(
                "Schema mismatch!\n\
                 Expected: {}\n\
                 Got: {}\n\
                 Retrain model or revert observation changes.",
                MODEL_MANIFEST.schema_hash,
                runtime_hash
            );
        }

        Self {
            weights: &BAKED_WEIGHTS,
            obs_builder: ObservationBuilder::new(),
            rng: StdRng::from_entropy(),
            deterministic: false,
        }
    }

    pub fn with_rng_seed(seed: u64, deterministic: bool) -> Self {
        Self {
            weights: &BAKED_WEIGHTS,
            obs_builder: ObservationBuilder::new(),
            rng: StdRng::seed_from_u64(seed),
            deterministic,
        }
    }

    fn forward_trunk(&self, obs: &Observation) -> [f32; 256] {
        let obs_flat = obs.flatten();  // -> [f32; 270]

        // Layer 1: 270 -> 512
        let mut h1 = [0.0f32; 512];
        linear(&obs_flat, self.weights.w1, self.weights.b1, &mut h1);
        silu_inplace(&mut h1);

        // Layer 2: 512 -> 512
        let mut h2 = [0.0f32; 512];
        linear(&h1, self.weights.w2, self.weights.b2, &mut h2);
        silu_inplace(&mut h2);

        // Layer 3: 512 -> 256
        let mut h3 = [0.0f32; 256];
        linear(&h2, self.weights.w3, self.weights.b3, &mut h3);
        silu_inplace(&mut h3);

        h3
    }

    fn forward_play_head(&self, features: &[f32; 256]) -> [f32; 52] {
        let mut logits = [0.0f32; 52];
        linear(features, self.weights.w_play, self.weights.b_play, &mut logits);
        logits
    }

    fn forward_pass_head(&self, features: &[f32; 256]) -> [f32; 52] {
        let mut scores = [0.0f32; 52];
        linear(features, self.weights.w_pass, self.weights.b_pass, &mut scores);
        scores
    }
}

impl Policy for EmbeddedPolicy {
    fn choose_pass(&mut self, ctx: &PolicyContext) -> [Card; 3] {
        let obs = self.obs_builder.build(ctx);
        let features = self.forward_trunk(&obs);
        let card_scores = self.forward_pass_head(&features);

        // Mask illegal cards
        let mut masked = card_scores;
        for (id, score) in masked.iter_mut().enumerate() {
            let card = Card::from_id(id as u8);
            if !ctx.hand.contains(card) {
                *score = f32::NEG_INFINITY;
            }
        }

        // Select top 3
        let mut indices: Vec<usize> = (0..52).collect();
        indices.sort_by(|&a, &b| {
            masked[b].partial_cmp(&masked[a]).unwrap()
        });

        [
            Card::from_id(indices[0] as u8),
            Card::from_id(indices[1] as u8),
            Card::from_id(indices[2] as u8),
        ]
    }

    fn choose_play(&mut self, ctx: &PolicyContext) -> Card {
        let obs = self.obs_builder.build(ctx);
        let features = self.forward_trunk(&obs);
        let logits = self.forward_play_head(&features);

        // Compute legal moves on-demand
        let legal_moves = compute_legal_moves(ctx.hand, ctx.round);

        // Mask illegal moves
        let mut masked = logits;
        for (id, logit) in masked.iter_mut().enumerate() {
            let card = Card::from_id(id as u8);
            if !legal_moves.contains(&card) {
                *logit = f32::NEG_INFINITY;
            }
        }

        // Sample or argmax
        let card_id = if self.deterministic {
            argmax(&masked)
        } else {
            categorical_sample(&masked, &mut self.rng)
        };

        Card::from_id(card_id)
    }
}
```

#### 5.4.4 Math Kernels (Pure Rust)

```rust
/// Matrix multiplication: output = input × weight^T + bias
/// Weights stored in row-major order: [out_dim, in_dim]
/// Example: w[512, 270] stored as [row0[270], row1[270], ..., row511[270]]
fn linear(input: &[f32], weight: &[f32], bias: &[f32], output: &mut [f32]) {
    let out_dim = output.len();
    let in_dim = input.len();

    for i in 0..out_dim {
        let mut sum = bias[i];
        let row_start = i * in_dim;
        for j in 0..in_dim {
            sum += input[j] * weight[row_start + j];
        }
        output[i] = sum;
    }
}

fn silu_inplace(x: &mut [f32]) {
    for val in x.iter_mut() {
        *val = *val / (1.0 + (-*val).exp());  // x * sigmoid(x)
    }
}

fn argmax(logits: &[f32; 52]) -> u8 {
    logits.iter()
        .enumerate()
        .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
        .map(|(i, _)| i as u8)
        .unwrap()
}

fn categorical_sample(logits: &[f32; 52], rng: &mut StdRng) -> u8 {
    // Softmax + weighted sampling
    let max = logits.iter().copied().fold(f32::NEG_INFINITY, f32::max);
    let mut probs = [0.0f32; 52];
    let mut sum = 0.0;

    for i in 0..52 {
        probs[i] = (logits[i] - max).exp();
        sum += probs[i];
    }
    for p in probs.iter_mut() {
        *p /= sum;
    }

    let r: f32 = rng.gen();
    let mut cumsum = 0.0;
    for (i, &p) in probs.iter().enumerate() {
        cumsum += p;
        if r <= cumsum {
            return i as u8;
        }
    }
    51
}
```

---

### 5.5 Training System

**Purpose:** Python-based PPO trainer with Rust environment backend.

#### 5.5.1 Training Architecture

```
┌─────────────────────────────────────┐
│  Python: tools/train.py             │
│  - PyTorch model (HeartsPolicy)     │
│  - PPO optimizer                    │
│  - Rollout collection orchestrator  │
└──────────────┬──────────────────────┘
               │ (calls via subprocess/FFI)
               ↓
┌─────────────────────────────────────┐
│  Rust: HeartsEnv (compiled binary)  │
│  - Game logic (hearts-core)         │
│  - Observation builder              │
│  - Reward computation               │
└─────────────────────────────────────┘
```

#### 5.5.2 PyTorch Model Definition

```python
# tools/train.py

import torch
import torch.nn as nn
import torch.nn.functional as F

class HeartsPolicy(nn.Module):
    def __init__(self, feature_dim=270):
        super().__init__()

        # Shared trunk
        self.trunk = nn.Sequential(
            nn.Linear(feature_dim, 512),
            nn.SiLU(),
            nn.Linear(512, 512),
            nn.SiLU(),
            nn.Linear(512, 256),
            nn.SiLU(),
        )

        # Dual heads
        self.pass_head = nn.Linear(256, 52)
        self.play_head = nn.Linear(256, 52)
        self.value_head = nn.Linear(256, 1)

    def forward(self, obs, legal_mask=None):
        """
        Args:
            obs: [batch, 270] observations
            legal_mask: [batch, 52] boolean mask (True = legal)

        Returns:
            dict with 'pass_logits', 'play_logits', 'value'
        """
        features = self.trunk(obs)

        pass_logits = self.pass_head(features)
        play_logits = self.play_head(features)
        value = self.value_head(features)

        # Apply legal mask
        if legal_mask is not None:
            play_logits = play_logits.masked_fill(~legal_mask, float('-inf'))

        return {
            'pass_logits': pass_logits,
            'play_logits': play_logits,
            'value': value,
        }
```

#### 5.5.3 Rollout Collection (Rust Side with Rayon)

```rust
// crates/hearts-app/src/rl/trainer.rs

use rayon::prelude::*;

pub struct Trainer {
    envs: Vec<HeartsEnv>,
    config: TrainerConfig,
    opponent_pool: OpponentPool,
    base_seed: u64,
}

pub struct TrainerConfig {
    pub num_envs: usize,              // E.g., 512
    pub opponent_mixing: OpponentMixing,
}

pub struct OpponentMixing {
    pub self_play_rate: f32,      // 0.7
    pub snapshot_rate: f32,       // 0.2
    pub heuristic_rate: f32,      // 0.1
}

impl Trainer {
    pub fn collect_rollout_batch(&mut self) -> Vec<EnvRollout> {
        // PARALLEL rollout collection
        self.envs
            .par_iter_mut()
            .enumerate()
            .map(|(env_id, env)| {
                self.collect_env_rollout(env, env_id)
            })
            .collect()
    }

    fn collect_env_rollout(&self, env: &mut HeartsEnv, env_id: usize)
        -> EnvRollout
    {
        let mut rollout = EnvRollout::new();

        // Deterministic opponent sampling per env
        let mut rng = StdRng::seed_from_u64(self.base_seed + env_id as u64);
        let seat_policies = self.sample_seat_policies(&mut rng);

        // === PASSING PHASE ===
        if env.phase().is_passing() {
            for seat_idx in 0..4 {
                let seat = PlayerPosition::from_index(seat_idx).unwrap();

                let pass_action = match &seat_policies[seat_idx] {
                    SeatPolicy::Training => {
                        // Request action from Python trainer
                        let obs = env.build_observation(seat);
                        rollout.push_pass_request(seat, obs);
                        continue;  // Will be filled by Python
                    }
                    SeatPolicy::Snapshot(frozen) => {
                        frozen.choose_pass(env.build_context(seat))
                    }
                    SeatPolicy::Heuristic(bot) => {
                        bot.choose_pass(env.build_context(seat))
                    }
                };

                env.step_pass(pass_action).unwrap();
            }
        }

        // === PLAYING PHASE ===
        while env.phase().is_playing() && !env.is_round_complete() {
            let seat = env.current_seat();

            let card = match &seat_policies[seat.index()] {
                SeatPolicy::Training => {
                    let obs = env.build_observation(seat);
                    rollout.push_play_request(seat, obs);
                    continue;  // Will be filled by Python
                }
                SeatPolicy::Snapshot(frozen) => {
                    frozen.choose_play(env.build_context(seat))
                }
                SeatPolicy::Heuristic(bot) => {
                    bot.choose_play(env.build_context(seat))
                }
            };

            let step = env.step_play(card).unwrap();

            if step.done {
                rollout.set_final_rewards(step.info.final_points.unwrap());
                break;
            }
        }

        rollout
    }

    fn sample_seat_policies(&self, rng: &mut StdRng) -> [SeatPolicy; 4] {
        let mut policies = [SeatPolicy::Training; 4];

        // Always train seat 0 (South)
        policies[0] = SeatPolicy::Training;

        // Mix opponents
        for i in 1..4 {
            let r: f32 = rng.gen();

            if r < self.config.opponent_mixing.self_play_rate {
                policies[i] = SeatPolicy::Training;
            } else if r < self.config.opponent_mixing.self_play_rate +
                          self.config.opponent_mixing.snapshot_rate {
                let snapshot = self.opponent_pool.sample_snapshot(rng);
                policies[i] = SeatPolicy::Snapshot(snapshot);
            } else {
                let bot = self.opponent_pool.sample_heuristic(rng);
                policies[i] = SeatPolicy::Heuristic(bot);
            }
        }

        policies
    }
}

enum SeatPolicy {
    Training,
    Snapshot(FrozenPolicy),
    Heuristic(Box<dyn Policy>),
}
```

#### 5.5.4 PPO Update (Python Side - Pseudocode)

```python
# tools/train.py

def ppo_update(policy, optimizer, rollout_batch, config):
    """
    Pseudocode for PPO update (actual implementation uses stable-baselines3 or similar)
    """
    # Compute advantages using GAE
    advantages = compute_gae(rollout_batch, gamma=0.99, lambda_=0.95)
    returns = advantages + rollout_batch.values

    # Normalize advantages
    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

    # PPO epochs
    for epoch in range(config.ppo_epochs):
        for minibatch in rollout_batch.shuffle_into_minibatches(256):
            # Forward pass
            outputs = policy(minibatch.obs, minibatch.legal_mask)
            new_log_probs = get_log_probs(outputs, minibatch.actions)
            new_values = outputs['value']
            entropy = compute_entropy(outputs)

            # Policy loss (PPO clip)
            ratio = (new_log_probs - minibatch.old_log_probs).exp()
            clipped_ratio = ratio.clamp(1 - config.clip_epsilon, 1 + config.clip_epsilon)
            policy_loss = -torch.min(
                ratio * minibatch.advantages,
                clipped_ratio * minibatch.advantages
            ).mean()

            # Value loss
            value_loss = F.mse_loss(new_values, minibatch.returns)

            # Entropy bonus
            entropy_loss = -entropy.mean()

            # Total loss
            loss = (policy_loss +
                    config.value_loss_coef * value_loss +
                    config.entropy_coef * entropy_loss)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(policy.parameters(), 0.5)
            optimizer.step()
```

---

### 5.6 CLI Extensions

**Purpose:** Add AI subcommands while preserving existing snapshot CLI.

#### 5.6.1 CLI Implementation (Using clap)

```rust
// crates/hearts-app/src/cli.rs

use clap::{Parser, Subcommand};
use std::path::PathBuf;

#[derive(Parser)]
#[command(name = "mdhearts")]
#[command(about = "Modern Hearts card game with AI")]
#[command(version)]
struct Cli {
    #[command(subcommand)]
    command: Option<Commands>,

    // Legacy flags (backward compatibility)
    #[arg(long)]
    export_snapshot: Option<String>,

    #[arg(long)]
    import_snapshot: Option<String>,
}

#[derive(Subcommand)]
enum Commands {
    /// Run interactive game (default if no subcommand)
    Play {
        #[arg(long, default_value = "embedded")]
        ai: String,

        #[arg(long)]
        seed: Option<u64>,
    },

    /// Evaluate policy performance
    Eval {
        #[arg(long, default_value = "10000")]
        games: u64,

        #[arg(long, default_value = "normal")]
        opponent: String,

        #[arg(long, default_value = "fixed")]
        seeds: String,
    },

    /// Generate selfplay logs
    Selfplay {
        #[arg(long, default_value = "1000")]
        games: u64,

        #[cfg(feature = "dev-model-io")]
        #[arg(long)]
        output: Option<PathBuf>,
    },

    /// Show model information
    Info {
        #[arg(value_parser = ["model"])]
        topic: String,
    },

    /// Generate Rust weights from .npz checkpoint (dev-model-io only)
    #[cfg(feature = "dev-model-io")]
    GenWeights {
        #[arg(long)]
        from: PathBuf,
    },
}

pub enum CliOutcome {
    Handled,
    NotHandled,
    AiCommand(AiCommandResult),
}

pub fn run_cli() -> Result<CliOutcome, String> {
    let cli = Cli::parse();

    // Handle legacy flags first (backward compatibility)
    if let Some(path) = cli.export_snapshot {
        return handle_export_snapshot(&path).map(|_| CliOutcome::Handled);
    }
    if let Some(path) = cli.import_snapshot {
        return handle_import_snapshot(&path).map(|_| CliOutcome::Handled);
    }

    // Handle subcommands
    match cli.command {
        Some(Commands::Play { ai, seed }) => {
            // Will launch UI with specified AI
            Ok(CliOutcome::NotHandled)
        }

        Some(Commands::Eval { games, opponent, seeds }) => {
            let result = run_eval(games, &opponent, &seeds)?;
            Ok(CliOutcome::AiCommand(result))
        }

        Some(Commands::Selfplay { games, output }) => {
            let result = run_selfplay(games, output.as_deref())?;
            Ok(CliOutcome::AiCommand(result))
        }

        Some(Commands::Info { topic }) => {
            if topic == "model" {
                print_model_info();
                Ok(CliOutcome::Handled)
            } else {
                Err(format!("Unknown info topic: {}", topic))
            }
        }

        #[cfg(feature = "dev-model-io")]
        Some(Commands::GenWeights { from }) => {
            generate_weights_file(&from)?;
            Ok(CliOutcome::Handled)
        }

        None => {
            // No subcommand = run UI
            Ok(CliOutcome::NotHandled)
        }
    }
}

fn print_model_info() {
    use crate::weights::MODEL_MANIFEST;

    println!("Model Information:");
    println!("  Schema Version: {}", MODEL_MANIFEST.schema_version);
    println!("  Schema Hash: {}", MODEL_MANIFEST.schema_hash);
    println!("  Feature Dim: {}", MODEL_MANIFEST.feature_dim);
    println!("  Architecture: {}", MODEL_MANIFEST.architecture);
    println!("  Total Params: {}", MODEL_MANIFEST.total_params);
    println!("  Export Time: {}", MODEL_MANIFEST.export_time);
    println!("  Training Commit: {}", MODEL_MANIFEST.training_commit);
}
```

---

### 5.7 Embedded Weights System

**Purpose:** Compile model weights into binary as const arrays.

#### 5.7.1 Checkpoint Format (NumPy .npz)

```python
# tools/train.py

import numpy as np

def export_weights(model: HeartsPolicy, output_path: str):
    """Export PyTorch model to .npz for Rust consumption"""
    state = model.state_dict()

    # Extract weights as NumPy arrays (row-major order)
    np.savez(
        output_path,
        # Trunk (stored as [out_dim, in_dim])
        w1=state['trunk.0.weight'].cpu().numpy(),  # [512, 270]
        b1=state['trunk.0.bias'].cpu().numpy(),     # [512]
        w2=state['trunk.2.weight'].cpu().numpy(),  # [512, 512]
        b2=state['trunk.2.bias'].cpu().numpy(),     # [512]
        w3=state['trunk.4.weight'].cpu().numpy(),  # [256, 512]
        b3=state['trunk.4.bias'].cpu().numpy(),     # [256]
        # Heads
        w_pass=state['pass_head.weight'].cpu().numpy(),  # [52, 256]
        b_pass=state['pass_head.bias'].cpu().numpy(),    # [52]
        w_play=state['play_head.weight'].cpu().numpy(),  # [52, 256]
        b_play=state['play_head.bias'].cpu().numpy(),    # [52]
        w_value=state['value_head.weight'].cpu().numpy(), # [1, 256]
        b_value=state['value_head.bias'].cpu().numpy(),   # [1]
    )
    print(f"Exported weights to {output_path}")
```

#### 5.7.2 Weights Generation Tool

```rust
// crates/hearts-app/src/tools/gen_weights.rs

#[cfg(feature = "dev-model-io")]
pub fn generate_weights_file(checkpoint_path: &Path) -> Result<(), String> {
    use npyz::NpyFile;
    use std::collections::HashMap;

    println!("Loading checkpoint: {}", checkpoint_path.display());

    // Load .npz file
    let npz = npyz::npz::NpzArchive::open(checkpoint_path)
        .map_err(|e| format!("Failed to open .npz: {}", e))?;

    let tensor_names = ["w1", "b1", "w2", "b2", "w3", "b3",
                        "w_pass", "b_pass", "w_play", "b_play",
                        "w_value", "b_value"];

    let mut tensors = HashMap::new();
    for name in &tensor_names {
        let data = npz.by_name(name)
            .ok_or_else(|| format!("Missing tensor: {}", name))?
            .into_vec::<f32>()
            .map_err(|e| format!("Failed to read {}: {}", name, e))?;
        tensors.insert(name.to_string(), data);
    }

    // Generate Rust source
    let mut output = String::new();
    output.push_str("// AUTO-GENERATED by gen-weights tool\n");
    output.push_str("// Source: PyTorch checkpoint (.npz format)\n");
    output.push_str("// DO NOT EDIT MANUALLY\n\n");
    output.push_str("use super::Weights;\n\n");

    for name in &tensor_names {
        let data = &tensors[*name];
        write_const_array(&mut output, &name.to_uppercase(), data);
    }

    output.push_str("\npub const BAKED_WEIGHTS: Weights = Weights {\n");
    output.push_str("    w1: W1,\n    b1: B1,\n");
    output.push_str("    w2: W2,\n    b2: B2,\n");
    output.push_str("    w3: W3,\n    b3: B3,\n");
    output.push_str("    w_pass: W_PASS,\n    b_pass: B_PASS,\n");
    output.push_str("    w_play: W_PLAY,\n    b_play: B_PLAY,\n");
    output.push_str("    w_value: W_VALUE,\n    b_value: B_VALUE,\n");
    output.push_str("};\n");

    // Write to file
    let out_path = Path::new("src/weights/generated.rs");
    std::fs::write(out_path, output)
        .map_err(|e| format!("Write failed: {}", e))?;

    println!("✓ Generated: {}", out_path.display());
    println!("✓ Rebuild binary: cargo build --release");

    Ok(())
}

fn write_const_array(output: &mut String, name: &str, data: &[f32]) {
    output.push_str(&format!("const {}: &[f32] = &[\n", name));

    for chunk in data.chunks(8) {
        output.push_str("    ");
        for &val in chunk {
            output.push_str(&format!("{:.8}, ", val));
        }
        output.push_str("\n");
    }

    output.push_str("];\n\n");
}
```

#### 5.7.3 Dummy Weights (Initial Development)

```python
# tools/gen_dummy_weights.py

import numpy as np

def xavier_init(fan_in, fan_out):
    """Xavier/Glorot initialization"""
    limit = np.sqrt(6.0 / (fan_in + fan_out))
    return np.random.uniform(-limit, limit, (fan_out, fan_in)).astype(np.float32)

# Generate dummy weights for initial development
np.savez(
    'out/dummy_weights.npz',
    w1=xavier_init(270, 512),
    b1=np.zeros(512, dtype=np.float32),
    w2=xavier_init(512, 512),
    b2=np.zeros(512, dtype=np.float32),
    w3=xavier_init(512, 256),
    b3=np.zeros(256, dtype=np.float32),
    w_pass=xavier_init(256, 52),
    b_pass=np.zeros(52, dtype=np.float32),
    w_play=xavier_init(256, 52),
    b_play=np.zeros(52, dtype=np.float32),
    w_value=xavier_init(256, 1),
    b_value=np.zeros(1, dtype=np.float32),
)

print("Generated dummy_weights.npz")
print("Usage: cargo run --features dev-model-io --bin gen-weights -- --from out/dummy_weights.npz")
```

---

## 6. Key Design Decisions

### 6.1 Why Dual Policy Heads?

**Decision:** Separate `choose_pass()` and `choose_play()` methods instead of single `act()`.

**Rationale:**
- Passing requires selecting **3 cards simultaneously** (top-3 from scores)
- Playing requires selecting **1 card** (categorical from logits)
- Different semantics warrant different interfaces
- Clearer error handling (compile-time enforcement of phase correctness)

### 6.2 Why Wrap MatchState Instead of Rebuild?

**Decision:** `HeartsEnv` wraps existing `MatchState` and `RoundState`.

**Rationale:**
- Avoids code duplication (game rules already implemented and tested)
- Reduces bug surface (legality checks, trick scoring, moon detection all reused)
- Simpler maintenance (rule changes propagate automatically)

### 6.3 Why Relative Rewards?

**Decision:** Use `opponent_avg - my_points` instead of `-my_points`.

**Rationale:**
- Hearts is competitive (you want to minimize **relative** score)
- Absolute rewards don't capture "winning" (getting 10 points when others get 20 is good)
- Encourages competitive play instead of purely defensive play

### 6.4 Why Opponent Mixing?

**Decision:** Train against mix of self (70%), snapshots (20%), heuristics (10%).

**Rationale:**
- Pure self-play can lead to policy collapse (rock-paper-scissors cycling)
- Snapshots prevent catastrophic forgetting of past strategies
- Heuristics ensure policy never falls below existing bot performance

### 6.5 Why Seat-Relative Observations?

**Decision:** All observations encoded relative to current player (left, across, right).

**Rationale:**
- Prevents seat-specific overfitting (e.g., "seat 0 always wins")
- Enables shared policy across all 4 seats (reduces parameters by 4×)
- Generalizes better (policy learns positional strategy, not seat-specific quirks)

### 6.6 Why Feature-Gate Checkpoint I/O?

**Decision:** Hide `gen-weights` and checkpoint features behind `dev-model-io` flag.

**Rationale:**
- Shipping binary doesn't need file I/O code (cleaner, smaller)
- Separates production concerns from development tools
- Prevents users from accidentally trying to load external models

### 6.7 Training Backend Choice (RESOLVED)

**Decision:** Use PyTorch for training, pure Rust for inference.

**Rationale:**
- Faster iteration (mature PPO libraries in Python)
- Easier debugging (TensorBoard, visualization)
- Can migrate to Rust training later if needed
- Clean separation of concerns

**Implementation:**
- Python: `tools/train.py` (PPO training)
- Export: `.npz` checkpoint format
- Import: `gen-weights` tool converts to Rust
- Inference: Pure Rust (no runtime dependencies)

---

## 7. Implementation Roadmap

### Phase 1: Foundation (No ML) — 3 PRs

**PR1: Policy trait + adapter**
- Files: `policy/mod.rs`, `policy/adapter.rs`, `policy/heuristic.rs`
- Changes: Wrap existing `PassPlanner`/`PlayPlanner` as `HeuristicPolicy`
- Update: `GameController` to use `Policy` trait
- Tests: Verify adapter produces same decisions as old bots
- Acceptance: `cargo test --workspace` passes, `clippy` clean

**PR2: Card ID conversion + observation builder**
- Files: `hearts-core/model/card.rs`, `rl/observation.rs`, `build.rs`, `bot/tracker.rs`
- Changes:
  - Add `Card::to_id()`/`from_id()` with tests
  - Add `ObservationBuilder` with schema v1.1 (270 features)
  - Add build.rs for schema hash computation
  - Extend `UnseenTracker` with `infer_voids()`
- Tests: Verify seat-relative observations, void inference
- Acceptance: Schema hash computed at build time

**PR3: RL environment wrapper**
- Files: `rl/env.rs`, `rl/mod.rs`
- Changes: Wrap `MatchState` with `HeartsEnv::step_pass/play()`
- Tests: Property tests (determinism, legality, 26 points total)
- Acceptance: Fuzz test (10k random legal games without panic)

### Phase 2: Model Infrastructure (No Training) — 2 PRs

**PR4: Embedded policy stub**
- Files: `policy/embedded.rs`, `weights/generated.rs`, `weights/manifest.rs`, `tools/gen_dummy_weights.py`
- Changes:
  - MLP inference (fp32), dummy weights via Python script
  - Schema validation at runtime
- Tests: Verify inference runs, respects legal mask
- Acceptance: `mdhearts play --ai embedded` runs without crash

**PR5: CLI + eval mode**
- Files: `cli.rs` (migrate to clap), `rl/eval.rs`
- Changes: Add `eval` and `info model` subcommands
- Tests: `mdhearts eval --games 100` completes
- Acceptance: Headless eval runs, reports metrics

### Phase 3: Training (Python + Rust) — 2 PRs

**PR6: PyTorch trainer**
- Files: `tools/train.py`, `tools/requirements.txt`
- Changes: PPO implementation in PyTorch, exports .npz
- Tests: Train for 1000 steps, verify checkpoint format
- Acceptance: `python tools/train.py --steps 1000` completes

**PR7: Checkpoint I/O + gen-weights**
- Files: `tools/gen_weights.rs`, `Cargo.toml` (add npyz dependency)
- Changes: .npz reader, `gen-weights` subcommand
- Tests: Round-trip (train → export → gen-weights → rebuild)
- Acceptance: Generated weights load correctly

### Phase 4: Optimization (Optional) — 3 PRs

**PR8: Int8 quantization**
- Files: `policy/embedded.rs` (add quantized path)
- Changes: Per-channel int8 inference
- Tests: Verify <1% accuracy loss vs fp32
- Acceptance: Binary size <50% of fp32

**PR9: SIMD optimization**
- Files: `policy/embedded.rs` (add AVX2 path)
- Changes: `#[cfg(target_feature = "avx2")]` dispatch
- Tests: Verify fp32 SIMD matches scalar
- Acceptance: Inference <1ms per decision on AVX2

**PR10: CI + cross-platform**
- Files: `.github/workflows/`, `main.rs` (headless mode)
- Changes: Linux headless build, CI matrix
- Tests: All platforms pass test suite
- Acceptance: Linux runs `eval`, `info model`

---

## 8. Testing Strategy

### 8.1 Unit Tests

**Engine Integration:**
```rust
#[test]
fn test_card_id_roundtrip() {
    for id in 0..52 {
        let card = Card::from_id(id);
        assert_eq!(card.to_id(), id);
    }
}

#[test]
fn test_legal_moves_non_empty() {
    // Every non-empty hand in valid game state has ≥1 legal move
}

#[test]
fn test_passing_phase_requires_all_4() {
    // step_pass() must be called 4 times before playing starts
}
```

**Observation:**
```rust
#[test]
fn test_seat_relative_invariance() {
    // Rotating game state + rotating seat produces same obs
}

#[test]
fn test_observation_dimension() {
    // All observations are exactly 270 features
}

#[test]
fn test_void_inference() {
    // Correctly detects opponent voids from trick history
}
```

**Policy:**
```rust
#[test]
fn test_heuristic_adapter_equivalence() {
    // HeuristicPolicy::normal() matches old PassPlanner
}

#[test]
fn test_embedded_respects_legal_mask() {
    // Embedded policy never selects illegal cards
}
```

### 8.2 Property Tests

**Invariants:**
```rust
use proptest::prelude::*;

proptest! {
    #[test]
    fn prop_total_points_always_26(seed: u64) {
        let mut env = HeartsEnv::new(seed, EnvConfig::default());
        // Play random legal game
        while !env.is_done() {
            let legal = env.legal_moves();
            let card = legal[0];  // Pick first legal
            env.step_play(card).unwrap();
        }

        let points = env.final_points().unwrap();
        prop_assert_eq!(points.iter().sum::<u32>(), 26);
    }
}
```

**Fuzz Testing:**
```rust
#[test]
fn fuzz_random_legal_games() {
    use rand::Rng;

    for seed in 0..10000 {
        let mut env = HeartsEnv::new(seed, EnvConfig::default());
        let mut rng = rand::thread_rng();

        while !env.is_done() {
            let legal = env.legal_moves();
            assert!(!legal.is_empty());

            let card = legal[rng.gen_range(0..legal.len())];
            env.step_play(card).expect("Legal move rejected");
        }
    }
}
```

### 8.3 Integration Tests

**Selfplay Determinism:**
```rust
#[test]
fn test_selfplay_deterministic() {
    let policy1 = EmbeddedPolicy::with_rng_seed(42, true);
    let policy2 = EmbeddedPolicy::with_rng_seed(42, true);

    let env1 = HeartsEnv::new(1234, EnvConfig::default());
    let env2 = HeartsEnv::new(1234, EnvConfig::default());

    let log1 = run_selfplay_game(env1, policy1);
    let log2 = run_selfplay_game(env2, policy2);

    assert_eq!(log1, log2);
}
```

---

## 9. Performance Targets

### 9.1 Inference Latency

| Configuration | Target | Rationale |
|---------------|--------|-----------|
| fp32 (AVX2) | <1 ms/decision | Interactive play feels instant |
| fp32 (scalar) | <2 ms/decision | Acceptable for CPU-only |
| int8 (AVX2) | <0.5 ms/decision | Optimization goal |

### 9.2 Training Throughput (CORRECTED)

| Configuration | Target | Rationale |
|---------------|--------|-----------|
| 512 envs, CPU | >100k steps/s | Reasonable training time |
| 1024 envs, CPU | >150k steps/s | Stretch goal |

**Step Counting:**
- Passing phase: 4 players × 1 pass = **4 steps**
- Playing phase: 13 tricks × 4 cards = **52 steps**
- **Total: 56 steps per round**

**Training Duration Target:**
- **Competency:** Beat NormalHeuristic 60% of games in <8 hours
- **Strong play:** Beat FutureHard 55% of games in <24 hours
- **Hardware:** Mid-range CPU (Ryzen 5800X / i7-12700K, 8 cores)
- **Total steps:** 1-3M for competency, 5-10M for strong play

### 9.3 Binary Size (CORRECTED)

| Configuration | Target | Notes |
|---------------|--------|-------|
| Baseline (no AI) | ~500KB-1MB | Current binary |
| +fp32 weights | <3MB | Model ~2.2MB, total ~2.7MB |
| +int8 weights | <1.5MB | Model ~0.56MB, total ~1.1MB |
| +UPX compression | <1MB | Optional post-processing |

**Corrected Calculations:**
- 558,601 parameters (not 700K)
- fp32: 559K × 4 bytes = 2.2MB
- int8: 559K × 1 byte + scales = 0.56MB

---

## 10. Risks and Mitigations

### 10.1 Technical Risks

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| **Training instability** | Medium | High | Opponent mixing (70/20/10), KL divergence monitoring, curriculum learning |
| **Illegal action from policy** | Low | High | Masked logits + engine validation + debug asserts |
| **Obs/model schema drift** | Low | High | Build.rs computes hash, CI check, runtime validation |
| **Binary size creep (>5MB)** | Low | Medium | Int8 quantization, profile regularly, strip symbols |
| **Training too slow (>24hrs)** | Medium | Medium | Rayon parallelization, profile hot paths |
| **Moon shooting never learned** | Medium | Low | Explicit reward (+78 for shooter), curriculum with moon-favorable deals |

### 10.2 Integration Risks

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| **Breaking existing UI** | Low | High | Integration tests, manual QA |
| **Regression in bot performance** | Low | Medium | Freeze bot fixtures, CI eval benchmarks |
| **Clippy/fmt violations** | Low | Low | CI enforces `-D warnings` and `fmt --check` |

---

## 11. Open Questions

### 11.1 Curriculum Learning

**Question:** Use curriculum (start easy, increase difficulty)?

**Options:**
1. **Flat:** Always mix 70/20/10 from step 0
2. **Curriculum:**
   - 0-100k: 100% vs EasyLegacy
   - 100k-500k: 50% self-play, 50% vs NormalHeuristic
   - 500k+: 70/20/10 final mix

**Recommendation:** TBD based on initial training experiments.

### 11.2 Pass Head Architecture

**Question:** Top-3 selection vs. combinatorial?

**Current:** Top-3 from pass head logits

**Alternative:** Combinatorial (C(13, 3) = 286 output logits)

**Recommendation:** Stick with top-3 (simplest). Revisit in v1.2 if problematic.

---

## 12. Appendices

### Appendix A: Example Usage

**Interactive Play:**
```bash
# Run with embedded policy
mdhearts play --ai embedded

# Run with heuristic baseline
mdhearts play --ai normal

# Run with fixed seed
mdhearts play --ai embedded --seed 12345
```

**Training:**
```bash
# Train with Python
python tools/train.py --steps 1000000 --envs 512

# Generate Rust weights
cargo run --features dev-model-io -- gen-weights --from out/policy.npz

# Rebuild with new weights
cargo build --release

# Verify
./target/release/mdhearts info model
```

**Evaluation:**
```bash
mdhearts eval --games 10000 --opponent normal

# Output:
# Embedded Policy vs NormalHeuristic
# Games: 10000
# Win Rate: 68.5%
# Avg Score: 52.3 vs 61.8
# Elo Difference: +142
```

### Appendix B: File Structure

```
crates/hearts-app/src/
├── main.rs
├── cli.rs (clap-based)
├── controller.rs
├── build.rs (NEW: schema hash)
│
├── policy/
│   ├── mod.rs
│   ├── adapter.rs
│   ├── heuristic.rs
│   └── embedded.rs
│
├── rl/
│   ├── mod.rs
│   ├── env.rs
│   ├── observation.rs
│   ├── trainer.rs
│   └── eval.rs
│
├── weights/
│   ├── mod.rs
│   ├── generated.rs (AUTO-GENERATED)
│   └── manifest.rs
│
├── tools/
│   └── gen_weights.rs
│
└── bot/ (EXISTING, extended)
    └── tracker.rs (+ infer_voids)

tools/ (workspace root)
├── train.py (PyTorch PPO)
├── gen_dummy_weights.py
└── requirements.txt
```

### Appendix C: Dependencies

```toml
# crates/hearts-app/Cargo.toml

[dependencies]
hearts-core = { path = "../hearts-core" }
hearts-ui = { path = "../hearts-ui" }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
rand = { version = "0.8", features = ["small_rng"] }
clap = { version = "4.5", features = ["derive"] }
rayon = "1.10"

# Windows dependencies (unchanged)
windows = { version = "0.62.1", ... }
windows-numerics = "0.3"

[dev-dependencies]
npyz = "0.8"  # For reading .npz (dev-model-io only)
proptest = "1.4"

[build-dependencies]
sha2 = "0.10"  # For schema hash computation

[features]
default = []
dev-model-io = []
```

**Python Requirements:**
```
# tools/requirements.txt
torch>=2.0.0
numpy>=1.24.0
stable-baselines3>=2.0.0
tensorboard>=2.14.0
```

### Appendix D: Binary Size Breakdown (CORRECTED)

| Component | fp32 Size | int8 Size |
|-----------|-----------|-----------|
| Existing code | ~500KB | ~500KB |
| Policy infrastructure | ~50KB | ~50KB |
| RL environment | ~30KB | ~30KB |
| MLP inference kernels | ~20KB | ~30KB |
| **Embedded weights** | **~2.2MB** | **~0.56MB** |
| **Total (uncompressed)** | **~2.8MB** | **~1.1MB** |
| **Total (LTO + strip)** | **~2.4MB** | **~0.9MB** |
| **Total (UPX)** | **~1.2MB** | **~0.6MB** |

### Appendix E: Schema Versioning Contract

**Schema Version:** `v1.1.0`

**Feature List (Ordered):**
```
hand_onehot[52]
seen_onehot[52]
trick_led_suit[4]
trick_cards[4][17]
trick_count[1]
my_trick_position[1]
scores_relative[4]
hearts_broken[1]
tricks_completed[1]
passing_phase[1]
passing_direction[4]
opp_voids[12]
last_4_cards[68]
```

**Total Dimension:** 270

**Migration Policy:**
- Minor version (v1.0 → v1.1): Breaking change (feature count changed)
- Patch version (v1.1.0 → v1.1.1): Add features at end (backward compatible)
- Major version (v1.x → v2.x): Complete redesign

---

## Document History

| Version | Date | Changes | Author |
|---------|------|---------|--------|
| 1.1 | 2025-10-05 | Fixed 25 issues from review | Senior Engineering Team |
| 1.0 | 2025-10-05 | Initial HLD | Senior Engineering Team |

---

## Approval

**Technical Review:** [ ] Pending
**Architecture Review:** [ ] Pending
**Security Review:** [ ] N/A (no external data, no network)
**Final Approval:** [ ] Pending

---

**Next Steps:**
1. Address remaining open questions (curriculum, pass head)
2. Create GitHub issues for each PR in roadmap
3. Assign Phase 1 PRs to team members
4. Begin implementation (PR1: Policy trait + adapter)
