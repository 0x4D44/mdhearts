Hard AI — Continuation Plan (v2)

Context
- Stage 2 is wrapped (void-aware follow-ups, leader targeting, endgame nuance, per-candidate logs, env-tunable weights) with tests green.
- Stage 3 (Hard) scaffold and polish landed: budgeted search with deterministic mode, top‑K continuation (Phase B) + monotonic fallback, selective branching/sampling under caps, CLI flags, disagreement and flipping goldens, benches, and docs.

Objectives (next 2–3 weeks)
- Tune continuation weights and promote safe defaults
  - Use explain-json + MDH_DEBUG_LOGS=1 and MDH_HARD_VERBOSE_CONT=1 to locate consistent improvements.
  - Adjust tiny defaults (QS risk, control hearts drift, control handoff penalty, moon relief) conservatively; re-run disagreement CSVs, ensure goldens stable.

- Expand robust goldens and determinism coverage
  - Add one more constructed flipping case where continuation reverses the top choice without adaptive/sampling.
  - Add multi-void sampling determinism tests; ensure explain and choose share seeded sequences.
  - Add tight time-cap stress tests for heavy legal sets; verify Phase A/B monotonic fallback.

- Refine adaptive limits (kept gated by env until stable)
  - Revisit thresholds in adaptive_limits() to correlate with leverage/score gaps and penalties on table.
  - Keep strict budget checks; no widening without benches in the loop.

- Performance guardrails and profiling
  - Maintain typical < 20–30ms envelope under widened caps; warn via benches using MDH_BENCH_WARN_US_*.
  - Capture typical/p95 and include in journal; watch long-tail spikes.

- CLI/docs polish and help verbosity
  - Ensure CLI help and README/docs list all Hard flags; keep popups opt-in via MDH_CLI_POPUPS.
  - Keep explain-once verbose continuation breakdown gated by env to avoid noisy output by default.

- Head-to-head evaluation
  - Use --match-batch to compare Normal vs Hard across seats/seeds; produce CSVs and brief analysis.
  - Decide on default promotion criteria (e.g., points/hand delta, no regression in key goldens).

Targeting Continuation Activation (added)
- Prioritize mid-trick/follow-up snapshots where penalties are on table or known voids exist; these better exercise continuation parts (feed/self/QS risk/handoff).
- If snapshot tooling is available, capture and explain with `--explain-snapshot`; else broaden compare ranges and harvest disagreements that correlate with penalty presence.
- Keep tuning changes gated and conservative until flips/improvements appear in these targeted contexts.

Deliverables
- Tuned default HardWeights in search.rs (if warranted) and updated README/docs.
- New tests: hard_multi_void_sampling.rs, hard_timecap_stress.rs, one more constructed flipping golden.
- Updated disagreement CSVs and explain JSONs in designs/tuning/ with brief summary.
- Journal entries summarizing tuning decisions and perf snapshots.

Risks & Mitigations
- Seed sensitivity across toolchains → rely on deterministic mode for tests/goldens; prefer constructed scenarios for flipping cases.
- Runtime regressions due to branching/sampling → budget guards at every layer and bench warnings.
- Double-counting leader-target effects → cap continuation’s leader-feed signals; keep Normal base_score nudge modest.

Seed Scanning Workflow (added)
- Use `--scan-explain-after <seat> <seed_start> <count> <plays_max> [--until-penalties|--until-void]` to find seeds with continuation activity (cont_nonzero>0).
- For each seat, shortlist 2–3 seeds, then run `--explain-once-after` and check verbose parts for differential continuation across candidates (feed/self/QS/handoff present for some, not all).
- Prefer these for tuning runs; avoid cases where singleton/next-lead bonuses apply uniformly.

Milestones & Checkpoints
1) Tuning pass A: collect logs, propose +5/+10 deltas; rerun compare-batch and goldens.
2) Determinism/tests: add sampling/time-cap tests; verify explain/choose parity under deterministic mode.
3) Adaptive thresholds: iterate with guarded flags and benches; keep off by default if not clearly beneficial.
4) Finalize defaults (optional): promote tuned weights and un-gate any clearly safe adaptive change.
5) Close out docs/journal with artifacts (CSVs/JSONs/bench numbers) and tag the work.

Quick Commands (reference)
- Explain/compare (deterministic):
  - mdhearts --compare-batch <seat> <start> <count> --only-disagree --out designs/tuning/compare_<seat>_<start>_<count>.csv --hard-deterministic --hard-steps 80
  - mdhearts --explain-once <seed> <seat> hard --hard-deterministic --hard-steps 80
- Match harness:
  - mdhearts --match-batch <seat> <seed_start> <games> normal hard --out designs/tuning/match_<seat>_<seed_start>_<games>.csv
- Benches:
  - cargo bench -p hearts-app --bench heuristic_decision
  - cargo bench -p hearts-app --bench hard_decision
