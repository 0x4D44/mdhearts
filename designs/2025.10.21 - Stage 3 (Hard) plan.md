Title: Stage 3 (Hard) – Shallow Search With Realistic Opponent Models

Context
- Stage 2 heuristic bot is in good shape with moon/void tracking, improved follow-ups, endgame nuance, structured logs, env weights, benches, and goldens.
- Goal for Stage 3 is an opt-in “Hard” difficulty that uses shallow lookahead (1–2 ply) with selective branching and the Stage 2 heuristics as a fast evaluation.

Objectives
- Implement shallow search that simulates the rest of the current trick (as now) plus 1–2 steps of the subsequent trick start when it’s cheap.
- Use per-seat suit-void knowledge and basic opponent policy for realism in response simulation.
- Keep Normal performance untouched; Hard can be slower but should stay responsive (<20–30ms typical on decision).

Design Outline
- Search driver (Hard only):
  - Node = (round snapshot, seat to play, trick state);
  - Evaluate leaf with current Stage 2 `base_score` + small continuation term (expected penalties next trick start if we lead/follow);
  - Alpha-beta pruning; iterative deepening to depth=2 with a strict time/branch budget.
- Move ordering & pruning:
  - Order by Stage 2 heuristic `PlayPlanner::explain_candidates` scores;
  - Limit branching per node (e.g., top 4–6 moves by heuristic unless forced/legal constraints);
  - Early cut when evaluation is clearly dominated under current alpha-beta window.
- Opponent model:
  - Follow suit lowest; if void, dump according to (a) leader-target policy if applicable, else (b) penalties preference from Stage 2 follow-up; respect tracker voids.
- Termination & budgets:
  - Per-decision wall-clock time cap (e.g., 10ms soft, 20ms hard) with graceful fallback to best-known principal variant;
  - Fallback to heuristic choice if budget exhausted or tree too shallow.

Integration Plan
- Add `PlayPlannerHard` behind `BotDifficulty::FutureHard` that delegates to a `SearchEngine` with a `Context` adapter.
- Reuse existing `simulate_trick`, `choose_followup_card`, and tracker APIs for rollouts; add a “next trick start” evaluation probe.
- Keep all Stage 2 logic intact; Hard path toggled by `MDH_BOT_DIFFICULTY=hard` or a UI toggle.

Validation
- Goldens: mirror a subset of Normal seeds and assert Hard either matches or produces strictly better score outcomes in constructed cases (e.g., avoid self-capture, better leader-target feeds).
- Micro-bench: add `benches/hard_decision.rs` and ensure time caps hold; watch worst-case tails.
- Logs: same structured per-candidate contributions plus search stats (nodes, depth, prunes) under `MDH_DEBUG_LOGS=1`.

Risks & Mitigations
- Performance variance across OS/toolchains – mitigate via strict caps + heuristic ordering;
- Overfitting to goldens – keep them diverse and platform-tolerant; allow fuzzy checks on score deltas vs. exact move equality where better outcomes are acceptable.

Next Steps (incremental)
1) Scaffolding: create `search` module with engine traits and a no-op that mirrors Normal
2) Wire Hard path toggling to call the new planner (feature-flag eligible)
3) Implement 1-ply principal variation with branching/budget
4) Add “next trick start” continuation evaluation
5) Add 1–2 focused goldens showing improved decisions
6) Bench and tune caps/branch limits

