Hard Performance HLD â€” Target +1â€“2 pts vs Normal (2025â€‘10â€‘22)

Context
- Current Hard (â€œFutureHardâ€) augments Normal with tiny continuation, a currentâ€‘trick rollout, and a shallow nextâ€‘trick probe under strict budgets.
- Deterministic mode + step budgets, topâ€‘K continuation, small AB margins, verbose explain/JSON, and eval tooling are in place.
- Mixedâ€‘seat evaluation (NNHH) shows Hard â‰ˆ Normal under conservative defaults; constructed goldens show continuation value in nearâ€‘ties.

Goals
- Practical, measurable improvement: Hard averages 1â€“2 fewer penalty points per hand than Normal across mixedâ€‘seat matchups (2 seats Hard vs 2 seats Normal), with strict deterministic budgets.
- Keep latency within envelope: typical < 20â€“30ms (wall clock) on commodity hardware with default caps; deterministic step budgets stable in CI.
- Maintain goldens stability; avoid regressions in Normal and existing Hard tests.

Nonâ€‘Goals
- Deep, multiâ€‘trick full search or PIMC; keep selective and budgetâ€‘aware.
- Perfect opponent modeling; keep O(L) policies and deterministic sampling.

Constraints
- Crossâ€‘platform determinism available; CI deterministic smoke preferred.
- Popups off by default; console/CSV/JSON only.
- Defaults conservative; new features gated by env until proven.

Issues Found (and Fixes Applied in This HLD)
- Leverage was underspecified. Fix: define a concrete formula, thresholds, and mapping to branch/budget tiers (see â€œLeverageâ€‘Aware Budgetingâ€).
- Plannerâ€‘level nudges could doubleâ€‘count with existing Normal leaderâ€‘feed logic. Fix: add guards and tiny deltas; skip if base already applied similar bias.
- Continuation scaling lacked adaptive behavior tied to penalties_on_table. Fix: scale parts by a small factor of penalties_on_table, with a strict symmetric cap.
- Explain/choose parity risks drift with AB margins. Fix: keep explain deterministic with topâ€‘K only; choose may use AB margin; add tests to ensure ordering parity inside topâ€‘K when budgets suffice.
- Evaluation acceptance was vague. Fix: staged acceptance (â‰¥0.5 pt/hand on curated sets before defaults; aim 1â€“2 on broader ranges postâ€‘promotion) and CI smoke with the mixed harness.
- Telemetry not actionable. Fix: specify Stats schema, include utilization, tier, and key limits; expose via CLI.

Proposed Design
1) Leverageâ€‘Aware Budgeting + Search (Core)
- Compute a leverage score L (0â€“100) for the current decision to focus compute where continuation matters:
  - penalties_on_table p (0â€“26): min(p, 13) Ã— 2 (max +26)
  - provisional winner is leaderboard (bool): +15
  - our score â‰¥ 85 (near 100): +10
  - hearts_broken (bool): +5
  - QS present & unseen (bool): +6
  - known void downstream (any opponent void in lead suit): +6
  - Clamp: L = min(100, sum).
- Map leverage â†’ tier with env thresholds (defaults):
  - MDH_HARD_LEVERAGE_THRESH_NARROW = 20, MDH_HARD_LEVERAGE_THRESH_NORMAL = 45
  - Tier selection: narrow if L < NARROW; normal if L < NORMAL; wide otherwise.
- Tier â†’ limits (defaults; envâ€‘tunable):
  - PhaseB topâ€‘K: {narrow: 4, normal: 6, wide: 8} via MDH_HARD_PHASEB_TOPK overriding perâ€‘tier default globally if set
  - Nextâ€‘trick probe leads M: {1, 2, 3} via MDH_HARD_NEXT_BRANCH_LIMIT
  - Thirdâ€‘opponent branch: {off, off, minimal} via MDH_HARD_NEXT3_ENABLE (wide only)
  - Early cutoff margin (chooseâ€‘only AB): {100, 150, 200} via MDH_HARD_AB_MARGIN
- Deterministic Budget:
  - Maintain a shared step budget; split into phase budgets with tierâ€‘dependent maxima:
    - Phase A (base): â‰¤ 40% of steps
    - Phase B (continuation): â‰¤ 40% of steps
    - Phase C (probe): â‰¤ 20% of steps
  - Always respect global step/time cap; never consume Phase C if Phase A/B underutilized does not justify it.
- Selective Deepening:
  - Enable nextâ€‘trick probe by default when tier â‰¥ normal with M from tier limits.
  - Minimal thirdâ€‘opponent branch (canonical + maxâ€‘penalty offâ€‘suit) under wide tier only; always budgetâ€‘checked.
- Early Cutoff + AB Margin:
  - Carry a dynamic alpha from best (base + cont + probe); skip candidates whose (base + max_possible_cont_by_cap) < alpha âˆ’ margin.

2) Plannerâ€‘Level Reinforcement (Tiny, Safe)
- Add a small planner score nudge mirroring continuation where safe:
  - When penalties_on_table > 0 AND provisional winner is leaderboard AND we wonâ€™t capture â†’ tiny positive (reinforces leaderâ€‘feed line Normal already likes). Guard: only if existing planner leaderâ€‘feed bonus is below a small threshold.
  - When will_capture AND penalties_on_table > 0 AND our score â‰¥ 85 â†’ tiny additional negative (reinforces avoid selfâ€‘capture). Guard: skip if nearâ€‘100 logic already imposed a large penalty.
- Gate with env defaults near 0 (e.g., MDH_HARD_PLANNER_LEADER_FEED_NUDGE=10 per penalty, MDH_HARD_PLANNER_SELF_CAPTURE_NUDGE=10 per penalty); promote after stable gains.

3) Opponent Modeling Refinements (O(L), Deterministic)
- Followâ€‘up policy: prefer safer midâ€‘rank discards when not feeding leader; when feeding leader, bias QS/hearts dump deterministically using seedâ€‘based PRNG tieâ€‘breaks.
- Respect known voids; continue avoiding selfâ€‘feed to origin in rollout.

4) Continuation Features (Capped, Adaptive)
- Scale continuation modestly with penalties_on_table and leaderâ€‘target alignment:
  - cont += (feed_perpen Ã— penalties) Ã— (1 + min(2, penalties_on_table / 5)) when leaderâ€‘target alignment holds.
  - cont âˆ’= (self_perpen Ã— penalties) Ã— (1 + (our_score â‰¥ 85 ? 1 : 0)).
- Keep strict symmetric cap (MDH_HARD_CONT_CAP) to prevent runaway effects; expose env knobs.
- Keep QS risk/hearts control/handoff minimal; only add small positive for â€œcapture to lead next when hearts broken & control goodâ€ under leverage.

5) Determinism & Parity
- Choose path may use early cutoff and AB margins; explain path remains exhaustive/deterministic (topâ€‘K only) to preserve tuning clarity.
- Deterministic seeded sampling for offâ€‘suit replies in rollout (unchanged), shared between explain/choose.

6) Config & Telemetry
- New env knobs (examples):
  - MDH_HARD_LEVERAGE_THRESH_{NARROW,NORMAL,WIDE}
  - MDH_HARD_PHASEB_TOPK (defaults 4/6/8 by tier), MDH_HARD_NEXT_BRANCH_LIMIT (defaults 1/2/3 by tier)
  - MDH_HARD_AB_MARGIN (chooseâ€‘only), MDH_HARD_PROBE_AB_MARGIN (probe pruning)
  - MDH_HARD_PLANNER_LEADER_FEED_NUDGE, MDH_HARD_PLANNER_SELF_CAPTURE_NUDGE
- Stats schema (exposed via last_stats/CLI):
  - scanned_total, scanned_phase_a/b/c, elapsed_ms, budget_steps, utilization=scanned_total/budget_steps
  - tier, topk, next_branch_limit, probe_third_enabled (bool), ab_margin

Algorithmic Flow (Choose)
- Phase 0: Evaluate leverage; pick tier (narrow/normal/wide); initialize Budget.
- Phase A: Rank candidates by base; early cutoff allowed.
- Phase B: Continuation for topâ€‘K; apply plannerâ€‘level nudges; compute totals.
- Phase C (if tier â‰¥ normal): Nextâ€‘trick probe for topâ€‘M (tierâ€‘dependent); minimal opponent branching under cap; update totals.
- Return best total with stable tieâ€‘breaks.

Performance Targets
- Deterministic step budgets: keep typical scanned steps within configured caps; no longâ€‘tail spikes.
- Wallâ€‘clock: typical < 20â€“30ms with defaults.

Evaluation Plan
- Mixedâ€‘seat harness `--match-mixed`:
  - Config: NNHH (two Normal vs two Hard); run per seat (West, South, North, East) over â‰¥200 seeds/seat; report hard_avg vs normal_avg.
  - Acceptance (staged):
    - Stage H1 (envâ€‘gated): â‰¥0.5 pt/hand improvement on curated sets emphasizing midâ€‘trick/endgame.
    - Stage H3 (promotion): target 1â€“2 pt/hand improvement on broader random ranges at the default caps.
  - Stats: report mean Â± stddev and 95% CI across seats; keep deterministic caps for reproducibility.
- Curated seeds: emphasize penaltyâ€‘rich midâ€‘tricks and endgame; derive from scan tools and journal.
- Regression checks: ensure Normal goldens stable; existing Hard goldens unaffected.

Risks & Mitigations
- Overfitting: keep weights small and capped; validate across unseen seeds; keep Normal planner untouched where possible.
- Latency spikes: strict budgets and leverage gating; profile typical/95th percentile; add nonâ€‘fatal CI smoke.
- Explain/choose divergence: documented and intentional; explain path remains deterministic for tuning.

Rollout Strategy
- Stage H1 (envâ€‘gated):
  - Implement leverage tiers + budget threading; enable small nextâ€‘trick probe by default under leverage.
  - Add plannerâ€‘level tiny nudges (env=off by default); expose knobs; add verbose logs for leverage decisions.
- Stage H2 (tuning):
  - Curate seeds; run mixedâ€‘seat eval; adjust continuation scaling and planner nudges minimally.
- Stage H3 (promotion):
  - If mixedâ€‘seat delta â‰¥0.5 on curated/broader ranges without regressions, promote small default increases (TOPK, NEXT_BRANCH_LIMIT, tiny planner nudge).
  - Keep all deeper branching and stronger scalings envâ€‘gated until further evidence.

Work Plan & File Anchors
- Budget/Leverage/Search: crates/hearts-app/src/bot/search.rs
  - compute_leverage(), adaptive_limits(), choose(): thread tiers â†’ limits â†’ budgets
  - next_trick_probe(): ensure tierâ€‘dependent branching; maintain explain parity
- Planner nudges: crates/hearts-app/src/bot/play.rs (base_score) â€” add envâ€‘gated tiny nudges; avoid doubleâ€‘count with existing leaderâ€‘target logic
- Opponent modeling tweaks: crates/hearts-app/src/bot/search.rs and bot/play.rs followâ€‘ups; deterministic midâ€‘rank preference maintained
- CLI/Docs: crates/hearts-app/src/cli.rs, docs/CLI_TOOLS.md â€” surface new flags; examples
- Tests:
  - Mixedâ€‘seat evaluation scripts (tools/run_eval.ps1/.sh) â€” add mixed snapshots
  - New constructed goldens for leverage scenarios (penaltyâ€‘rich midâ€‘trick; endgame with leader feed)
  - Deterministic parity tests for explain vs choose unaffected

Acceptance Criteria
- Hard improves over Normal by â‰¥0.5 pt/hand on curated mixedâ€‘seat sets; trending to 1â€“2 pts/hand after safe promotions
- All tests green; perf within targets; explain output stable
- Docs updated; CI smoke passing


Addendum — Detailed Specs and Resolutions

Known Gaps Addressed
- max_possible_cont_by_cap: Define as min(MDH_HARD_CONT_CAP, penalties_on_trick * (FEED or SELF per-pen) * adaptive_multiplier). Use penalties_on_trick from rollout sim; when unknown pre-rollout, bound by 13 and treat alignment pessimistically (assume no leader-feed) to keep skip safe.
- Hearts broken / legality guards: Planner nudges and continuation leader-feed are only considered when hearts are broken OR the leader is currently winning a non-hearts trick; never incentivize leading hearts before broken.
- QS presence check: Use tracker APIs to detect unseen QS; if unavailable in scope, infer from hand + trick plays; fall back to “unknown” = false to avoid optimistic bias.
- Seeded PRNG: Define xorshift64* seeded by (global_seed ⊕ seat_index ⊕ trick_fingerprint), where fingerprint hashes: lead_suit, plays (seat,r,s), and remaining hand counts per suit. Same seed for explain and choose.
- Double counting avoidance: Planner nudges only apply if base_score’s leader-feed or near-100 blocks did not already exceed a small guard threshold (env: MDH_HARD_PLANNER_MAX_BASE_FOR_NUDGE, default 60 per penalty). Skip nudge if threshold exceeded.
- Telemetry fields semantics: utilization = scanned_total / budget_steps (0..1); scanned_phase_* cover iterations (not unique states); tier reported as narrow|normal|wide.

Default Knobs (proposed)
- MDH_HARD_LEVERAGE_THRESH_NARROW=20, MDH_HARD_LEVERAGE_THRESH_NORMAL=45
- MDH_HARD_PHASEB_TOPK_BY_TIER=4,6,8 (overridden by MDH_HARD_PHASEB_TOPK if set)
- MDH_HARD_NEXT_BRANCH_LIMIT_BY_TIER=1,2,3 (overridden by MDH_HARD_NEXT_BRANCH_LIMIT if set)
- MDH_HARD_AB_MARGIN_BY_TIER=100,150,200 (overridden by MDH_HARD_AB_MARGIN if set)
- MDH_HARD_PLANNER_LEADER_FEED_NUDGE=10 (per penalty), MDH_HARD_PLANNER_SELF_CAPTURE_NUDGE=10 (per penalty)
- MDH_HARD_PLANNER_MAX_BASE_FOR_NUDGE=60 (per penalty)
- MDH_HARD_CONT_FEED_PERPEN=60, MDH_HARD_CONT_SELF_CAPTURE_PERPEN=80 (unchanged default)
- MDH_HARD_CONT_CAP=300 (symmetric)

Compute Leverage — Pseudocode
```
fn compute_leverage(ctx) -> (tier, L) {
  let mut L = 0;
  let p_on_table = penalties_on_current_trick(ctx); // 0..26
  L += min(p_on_table, 13) * 2;              // up to +26
  if provisional_winner_is_leader(ctx) { L += 15; }
  if our_score(ctx) >= 85 { L += 10; }
  if hearts_broken(ctx) { L += 5; }
  if queen_of_spades_unseen(ctx) { L += 6; }
  if any_opponent_void_in_lead_suit(ctx) { L += 6; }
  L = min(L, 100);
  let tN = env_or(20, MDH_HARD_LEVERAGE_THRESH_NARROW);
  let tM = env_or(45, MDH_HARD_LEVERAGE_THRESH_NORMAL);
  let tier = if L < tN { Narrow } else if L < tM { Normal } else { Wide };
  (tier, L)
}
```

Choose — Phase Budgets and Limits
- Phase A (base): up to 40% of steps; compute base order and dynamic alpha.
- Phase B (top‑K continuation): K = per‑tier (4/6/8) unless MDH_HARD_PHASEB_TOPK overrides.
- Phase C (probe): M = per‑tier (1/2/3) unless MDH_HARD_NEXT_BRANCH_LIMIT overrides; third‑opponent branch only in Wide.
- Early cutoff (choose‑only): AB margin = per‑tier (100/150/200) unless MDH_HARD_AB_MARGIN overrides; skip dominated candidates using safe bound `base + max_possible_cont_by_cap`.

Planner‑Level Nudge — Guards
- Leader‑feed nudge applies only if: penalties_on_table>0, provisional winner is leaderboard, will_capture=false, and base leader‑feed per‑penalty < MDH_HARD_PLANNER_MAX_BASE_FOR_NUDGE.
- Self‑capture nudge applies only if: will_capture=true, penalties_on_table>0, our_score≥85, and near‑100 base penalty per‑penalty < MDH_HARD_PLANNER_MAX_BASE_FOR_NUDGE.

Explain vs Choose — Parity Tests
- Add tests asserting:
  - For deterministic caps and tier=Normal, explain top‑K ordering equals choose ordering when AB margin is 0.
  - With non‑zero AB, ensure chosen top is present in explain top‑K and explain’s top total ≤ (choose’s top total + AB margin).

Evaluation Enhancements
- Mixed harness: Evaluate NNHH and HNNH permutations to reduce seating bias; report per‑seat means and overall means.
- Curated sets: Build a seed list focused on penalty‑rich and endgame positions; store under designs/tuning/curated_seeds.txt and consume via CLI.
- Reporters: Extend tools/run_eval.(ps1|sh) to support `--mixed <mix>` and curate file input; output consolidated MD.

Rollback & Safety
- Keep all new behaviors env‑gated initially; submit small PR enabling only per‑tier top‑K and probe for tier≥Normal.
- Promotion gates: if mixed delta ≥0.5 pt/hand on curated sets and tests stable, nudge defaults one step (e.g., PhaseB K+1 for Normal tier).
- Document knobs in README and CLI docs; add CI smoke to run a tiny mixed test to verify harness and determinism.

Review Round 2 — Refinements and Clarifications

Compatibility and Env Knobs
- Unify AB margin naming: existing code exposes `MDH_HARD_EARLY_CUTOFF_MARGIN` (choose only). HLD’s per‑tier AB margin should compute an effective value that overrides this env only when per‑tier logic is active; otherwise, respect the single global var. Implementation: read per‑tier defaults, then if `MDH_HARD_EARLY_CUTOFF_MARGIN` is set, use it for all tiers; else use per‑tier values.
- Tiered top‑K/next limits: keep existing global overrides (`MDH_HARD_PHASEB_TOPK`, `MDH_HARD_NEXT_BRANCH_LIMIT`) as master switches that replace per‑tier numbers when set.
- New tier bundle flag (optional): `MDH_HARD_TIERS_ENABLE` to gate all per‑tier logic. Defaults OFF; enable in env during tuning.

Edge Cases and Guards
- First trick / 2♣ enforcement: never apply planner nudges or continuation scale when the first trick lead must be 2♣; allow only base scoring.
- Hearts not broken: leader‑feed nudges and continuation feed parts apply only when hearts are broken OR the trick is non‑hearts; never incentivize breaking hearts in low‑leverage states.
- Ambiguous leaderboard: if multiple players tie for lead, treat “leader targeting” as off for planner nudges/continuation feed parts to avoid misdirected incentives.
- Multi‑void scenarios: in rollout/probe, if >1 opponent voids, restrict alternate branches to (canonical follow + max‑penalty off‑suit) per opponent, budget permitting; do not spawn combinatorial branches.

Leverage Stability
- Smoothing: when the same seat remains to play across successive decisions (e.g., leading after capturing), apply a small hysteresis to tier (e.g., don’t downshift wider‑to‑narrow unless L drops by ≥5). This reduces tier flapping and improves determinism.
- Hard cap check: if time budget exhausted but steps remain, record utilization and skip Phase C to preserve responsiveness; report via stats.

Max Possible Continuation Bound (for AB Skip)
- Define `max_possible_cont_by_cap` = min(MDH_HARD_CONT_CAP, penalties * FEED_PERPEN_scaled) in leader‑feed alignment; otherwise 0 for positive bound. For negative bound (self‑capture), use min(cap, penalties * SELF_PERPEN_scaled). When both could apply, take the greater magnitude consistent with known alignment; when unknown, use 0 to keep bound conservative.

Telemetry (Extended)
- Add `tier_hysteresis` (bool), `leverage_score` (0–100), and `branch_counts` ({probe_leads, reply2, reply3}) to stats. Expose in `--explain-json` for traceability.

Evaluation Methodology
- Seating permutations: run NNHH, HNNH, NHNH to spread Hard seats; average across two hard seats vs two normal seats each permutation, then average permutations.
- Curated seed list: maintain `designs/tuning/curated_seeds.txt` with lines `<seat> <seed>`; extend CLI to accept `--seeds-file <path>` in `--match-mixed`.
- Reporting: compute mean, stddev, and 95% CI (±1.96×std/sqrt(n)) for hard vs normal aggregates; include in summary MD.

Test Additions
- Parity: ensure explain top‑K includes choose’s top and, with AB margin=0, explain and choose totals/orderings match for top‑K under deterministic caps.
- Leverage tiers: unit tests asserting tier selection for crafted contexts (e.g., penalties=0, leader align=false → Narrow; penalties≥8 & leader align → Normal/Wide).
- Nudges: tests verifying nudges are skipped when base leader‑feed/near‑100 penalties exceed guard thresholds; tests that nudges apply when below thresholds.

Risk Controls
- Rollback switch: `MDH_HARD_TIERS_ENABLE=0` disables all tiering; `MDH_HARD_PROBE_ENABLE=0` can disable Phase C entirely.
- Golden preservation: new behaviors default OFF; if a future default promotion causes a golden change, add a new golden based on deterministic seeds and explain why (with CI notes).

Notes on Determinism
- PRNG seeds: prefer `u64` seeds and avoid non‑portable time queries in budgets when `MDH_HARD_DETERMINISTIC=1`; rely exclusively on step accounting.
- JSON explain parity: ensure fields are stable across platforms (field names & order fixed), and numeric outputs are integers or fixed precision.
