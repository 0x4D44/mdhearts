# Stage 6 — Evaluation & Stability Plan

Objectives
- Broaden evaluation breadth with deterministic tooling; add stability guardrails without changing defaults.
- Maintain explain/choose parity expectations while allowing choose‑only pruning for performance when env‑enabled.

Milestones
S6‑A: Broader Evaluation
- Run `--match-batch` across all seats (2× wider ranges) with deterministic caps; save CSVs and a consolidated summary.
- Track average points/hand per difficulty and seat; annotate seeds with large deltas.

S6‑B: Disagreement Harvesting
- Automate shortlists from `--compare-batch --only-disagree` for each seat; store top N seeds by base gap closeness.
- Write one additional constructed mid‑trick near‑tie golden (continuation decides tie) if any high‑value case emerges.

S6‑C: Guardrails (Perf/Determinism)
- Capture periodic bench snapshots (avg/p95) under deterministic caps; record in journal.
- Optional (env‑gated) perf warns in benches/CLI remain non‑fatal.

S6‑D: Docs & Hygiene
- Ensure CLI help strings and docs list Hard flags and deterministic workflow clearly.
- Add a short “Tuning Results Index” pointing to key CSV/JSON files.

Acceptance
- New CSVs and summaries saved under `designs/tuning/`.
- Journal updated with evaluation summaries and perf snapshots.
- No default weight changes; tests/goldens stay green; explain path remains deterministic.

Out‑of‑Scope (for future)
- Deeper search or default behavior changes; any such work stays behind env flags with tests.