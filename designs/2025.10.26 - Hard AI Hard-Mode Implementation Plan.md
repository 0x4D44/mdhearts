# Hard AI Hard-Mode Implementation Plan (2025-10-26)

This plan operationalizes the 2025-10-26 Rev C Hard-mode design. Each stage outlines goals, concrete tasks, dependencies, feature toggles, validation steps, and required artifacts. Owners are assigned later.

## Milestone Overview

| Stage | Target Outcome | Core Deliverables |
|-------|----------------|-------------------|
| A | Belief and telemetry scaffolding landed behind toggles | BeliefState, belief cache, telemetry sink, unit tests |
| B | Belief-sampled search integrated and stable in smoke runs | Sampler wiring, cache reuse, regression updates |
| C | Learning advisers captured, trained, and optionally applied | Dataset exporter, training scripts, adviser models |
| D | Deep tier leverage, adaptive budgets, extended DP, moon heuristics | DynamicBudget, tier thresholds, DP upgrades |
| E | Evaluation automation, scenario packs, telemetry dashboards | Tournament runner, scenario library, bench guardrails |
| F | Tuned defaults promoted after acceptance metrics confirmed | Final config, CI reports, docs, promotion write-up |

Global success: Hard must average >= +1.0 PPH versus Normal with 95% CI lower bound >= +0.3, maintain <= 40 ms p95 latency, and pass all regression packs. New capabilities remain gated until their stage exits.

## Stage A - Foundations (Belief & Telemetry)

- **Goals**
  - Implement belief tracking structures tied to `UnseenTracker`.
  - Create telemetry sink and storage with retention.
- **Tasks**
  1. Add `BeliefState`, `BeliefCacheKey`, and LRU `BeliefCache` guarded by `parking_lot::RwLock`.
  2. Thread Bayesian updates through controller and planners, syncing void flags in `UnseenTracker`.
  3. Introduce `hard::TelemetrySink` capturing belief entropy, cache hits, and placeholder fields for later phases.
  4. Add CLI `--show-hard-telemetry` and persist summaries under `designs/tuning/telemetry/` with rotating retention (keep-last default 20).
  5. Write unit tests for belief math, cache eviction, telemetry serialization, and CLI parsing.
- **Dependencies**: Existing tracker and controller update hooks.
- **Feature Toggles**: `MDH_HARD_BELIEF_ENABLE=0` (default off).
- **Validation**
  - `cargo test` new modules.
  - Smoke tournament (n=50 per seat) with belief disabled to confirm parity.
  - Manual review of telemetry output file.
- **Artifacts**
  - Journal entry summarizing Stage A landing.
  - README/CLI docs covering telemetry command and belief knobs.

## Stage B - Belief-Sampled Search

- **Goals**
  - Replace uniform determinisation with belief-guided sampling and history-filter caching.
- **Tasks**
  1. Implement `BeliefSampler::draw_deal` (top-K plus diversity samples) and filter hook.
  2. Integrate sampler into `rollout_current_trick`, `next_trick_probe`, and related helpers respecting budgets.
  3. Reuse cached deals via `BeliefCacheKey`; invalidate on entropy drop or deterministic mode.
  4. Expose env knobs `MDH_HARD_BELIEF_TOPK`, `_DIVERSITY`, `_FILTER`, `_CACHE_SIZE`; add CLI `--hard-belief on|off`.
  5. Extend regression tests for belief-sensitive scenarios (void inference, repeated trick contexts).
- **Dependencies**: Stage A types and telemetry.
- **Feature Toggles**: `MDH_HARD_BELIEF_ENABLE` remains default off.
- **Validation**
  - Smoke tournament (n=200 per seat) with toggle on; evaluate latency delta and decision variance.
  - Scenario tests verifying cache reuse and fallback logic.
  - Telemetry review of entropy trends and cache hit ratio.
- **Artifacts**
  - Telemetry CSV/MD summary for smoke run.
  - CLI docs updated with new flags and env knobs.

## Stage C - Learning Advisers (Pilot)

- **Goals**
  - Capture explain datasets, train phase advisers, and integrate optional priors.
- **Tasks**
  1. Extend explain tooling to emit structured rows (JSONL/Parquet) stored under `designs/data/YYYYMMDD/`.
  2. Author training script `tools/train_phase_advisers.py` and document Python dependencies (`docs/training.md`).
  3. Train initial LightGBM or XGBoost models for pass, lead, and follow phases; commit weight files under `assets/adviser/`.
  4. Implement `AdviserPrior` API that produces clamped bias adjustments; wire into planner ordering.
  5. Add env flags `MDH_HARD_ADVISER_PASS|LEAD|FOLLOW` and CLI toggles `--hard-adviser-pass`, etc.; log adviser influence in telemetry.
- **Dependencies**: Stage A/B telemetry fields and dataset exporter.
- **Feature Toggles**: Advisers off by default.
- **Validation**
  - Unit tests for bias clamping and candidate ordering.
  - Smoke tournament (n=200 per seat) adviser-on vs adviser-off; compare penalty stats and telemetry.
  - Document model validation metrics (loss, disagreement rate) in `designs/tuning/`.
- **Artifacts**
  - Training guide (`docs/training.md`).
  - Adviser evaluation report stored alongside datasets.

## Stage D - Deep Tier Enhancements

- **Goals**
  - Deploy leverage gating, dynamic budgets, extended DP (<=4 tricks), and moon-aware heuristics.
- **Tasks**
  1. Implement `LeverageScore` with configurable weights (`configs/leverage.yaml`) and tier thresholds.
  2. Replace `Budget` with `DynamicBudget` controlling nodes, samples, and elapsed time per tier.
  3. Extend DP solver to four tricks using belief-sampled deals, alpha-beta pruning, and contribution clamps.
  4. Enhance moon state machine with opponent probabilities and integrate blocking heuristics.
  5. Expand telemetry to log tier histograms, fallback reasons, DP hits, and moon triggers.
  6. Create scenario fixtures for moon attempts, moon aborts, and endgame DP flips.
- **Dependencies**: Stages A-C complete.
- **Feature Toggles**: `MDH_HARD_LEVERAGE_ENABLE`, `MDH_HARD_DP_EXTEND`, `MDH_HARD_MOON_HEUR` (all default off).
- **Validation**
  - Run scenario regression pack for moon/endgame cases.
  - Bench guardrails (heuristic and Hard benches) verifying p95 <= 40 ms when toggles on/off.
  - Smoke tournament (n=200 per seat) with deep tier toggles enabled to inspect tier distribution and fallback rate (<5% target).
- **Artifacts**
  - README and CLI updates describing leverage tiers and controls.
  - Telemetry summary capturing tier histogram and moon stats.

## Stage E - Evaluation Automation & Tooling

- **Goals**
  - Automate large-scale tournaments, scenario regression, and telemetry dashboards.
- **Tasks**
  1. Implement `mdhearts --tournament <config.toml>` runner with config validation (seed counts, window separation, tier coverage).
  2. Add telemetry rotation (keep-last default 20) and allow export via `--show-hard-telemetry --out <path>`.
  3. Organize scenario library in `designs/scenarios/` and add ignored tests to replay them.
  4. Extend mixed-seat commands with `--stats` to append tier and belief aggregates.
  5. Provide scripts/notebooks to convert telemetry CSV to Markdown dashboards for CI reports.
  6. Wire bench guardrail warnings into CI summary (non-blocking, but visible).
- **Dependencies**: Telemetry fields established by prior stages.
- **Feature Toggles**: None new; runner should set existing toggles per config.
- **Validation**
  - Test tournament runner on limited seeds; ensure validation errors fire on misconfigurations.
  - Execute scenario suite via `cargo test -- --ignored regression_scenarios`.
  - Verify telemetry rotation by simulating >20 runs and checking retention.
- **Artifacts**
  - Example tournament config and results persisted under `designs/tuning/`.
  - CI runbook entry documenting new commands and guardrails.

## Stage F - Tuning & Promotion

- **Goals**
  - Tune configuration and promote enhanced Hard mode once acceptance criteria hold.
- **Tasks**
  1. Run full deterministic tournament matrix (>=1000 seeds per seat across two windows) with candidate settings; record metrics and telemetry.
  2. Iterate on weights, sampler configuration, adviser bias, and DP caps using dashboard insights.
  3. Update goldens and regression packs to capture intentional behaviour shifts (belief flips, moon defense).
  4. When criteria met twice consecutively, flip default toggles: enable belief sampler, leverage, select advisers, extended DP, and moon heuristics.
  5. Produce promotion report summarizing metrics, telemetry, risks, and mitigation statuses.
  6. Finalize documentation (README, CLI tools, configs/hard_default.toml, journal entry).
- **Dependencies**: All previous stages complete and toggles available.
- **Feature Toggles**: Individually controlled until promotion commit adjusts defaults.
- **Validation**
  - Final acceptance tournament meeting PPH advantage and latency caps.
  - Regression packs and goldens re-run with new defaults on Linux and Windows.
  - Bench guardrails confirm latency envelope; telemetry fallback rate <5%.
- **Artifacts**
  - Promotion summary (`designs/2025.10.xx - Hard Promotion Summary.md`).
  - Updated configuration manifest and documentation.

## Cross-Cutting Workstreams

- Documentation: update README, CLI docs, changelog, and journal after each stage; record default toggle states.
- Testing infrastructure: maintain nightly deterministic tournament workflow and bench checks; prevent large dataset commits via git hooks.
- Telemetry management: implement cleanup job for telemetry rotation; provide aggregation script/notebook.
- Risk tracking: monitor telemetry storage, latency spikes, adviser overfitting; track open questions from design Section 8 with owners and due dates.

## Exit Criteria

Completion of Stage F, with acceptance metrics satisfied, regression suites passing, documentation updated, and risks assigned, constitutes readiness to promote the improved Hard difficulty as the default experience.



