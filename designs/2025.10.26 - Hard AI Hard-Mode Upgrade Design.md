# Hard AI Hard-Mode Upgrade Design (2025-10-26, Rev C)

## Revision History
- 2025-10-26 (Rev A): initial concept draft produced after research survey.
- 2025-10-26 (Rev B): expanded component specifications, clarified leverage thresholds, defined data and evaluation pipelines, documented dependencies, and added mitigation plans after design review.
- 2025-10-26 (Rev C): incorporated second-pass review feedback covering cache concurrency, telemetry retention, feature flag rollout, and testing strategy.

## 1. Purpose and Success Criteria
- Primary goal: ship a Hard difficulty that achieves at least +1.0 points-per-hand (PPH) advantage over the current Normal planner with a 95% confidence interval lower bound >= +0.3 PPH, measured on deterministic mixed-seat tournaments.
- Evaluation protocol: run two non-overlapping deterministic seed windows per seat (for example 1000-1999 and 2000-2999), with >=1,000 seeds per window and all four seat mixes (NNHH, HHNN, HNNH, NHNN). Report means, standard deviations, and 95% CI for each configuration.
- Performance constraint: keep per-move latency <=40 ms at the 95th percentile and <=5 ms at the 50th percentile on reference CI hardware. Explain paths must remain deterministic and audit-friendly.
- Stability constraint: prevent regressions in existing or new regression packs (moon, void, endgame, determinisation). Goldens must remain deterministic on supported platforms. Easy and Normal modes only change where shared infrastructure requires it.

## 2. Current State and Observed Gaps
1. Uniform determinisation: Hard determinisation draws nearly uniform samples with light void tracking; belief quality drops in long rounds, creating variance and diluted continuation gains.
2. Static leverage gating: search tiers exist but thresholds are hand-tuned constants; they do not incorporate belief entropy or moon state, and they cannot adapt budgets dynamically.
3. Limited data capture: explain logs provide qualitative insight but lack structured feature dumps, making it difficult to train data-driven priors or measure leverage distribution.
4. Endgame coverage: micro DP handles <=3 tricks with uniform sampling and rarely triggers due to conservative gating; DP telemetry is coarse.
5. Evaluation tooling: mixed-seat tournaments require manual setup, do not output telemetry aggregates by tier, and provide no automated gating on acceptance criteria.

These gaps prevent the Hard planner from capitalising on the research-informed techniques identified in the survey.

## 3. Design Drivers from Research
- Belief-aware sampling (GO-MCTS, history-filtering): prioritise belief-consistent deals, prune impossible continuations, and reuse subgames when contexts repeat.
- Staged decision making (hybrid heuristics plus deep search): protect latency by spending budget only when leverage is high while keeping low-leverage moves fast.
- Learning-guided heuristics: overlay supervised or RL advisers on top of heuristics to reduce human bias and unlock improvements where search is shallow.
- Endgame precision: deploy exact solvers in tractable endgames to convert small advantages reliably.
- Evaluation discipline: use regression packs and large deterministic tournaments to confirm that score gains are statistically meaningful and generalise.

## 4. Detailed Architecture

### 4.1 Belief Model and Sampling Layer
- Data structures
  - `BeliefState` (per seat) storing suit distribution vectors (probability of each remaining card), high-card weights, queen-of-spades risk, moon-attempt likelihood, and entropy.
  - `BeliefCacheKey` = (round_id, trick_index, leader_seat, cards_played_fingerprint). Cache stores `(BeliefState, sampler_seed)` pairs.
  - `BeliefCache` implemented as an LRU map guarded by `parking_lot::RwLock` to allow concurrent reads while keeping mutation order deterministic.
- Update pipeline
  1. Initialise with uniform distributions after the pass, conditioned on observed cards and passes.
  2. On each play, update using Bayesian rules: multiply prior probabilities by the likelihood of observing the card given void states, renormalise, update entropy, and sync results back into the existing `UnseenTracker` so void flags stay authoritative.
  3. Update queen-of-spades risk by tracking which seats still hold spades >= Q of spades (QS); adjust when QS is seen or strongly inferred.
  4. Update moon likelihoods using score gap, hearts captured, and control streak.
- Belief-sampled determinisation
  - Introduce `BeliefSampler::draw_deal(BeliefState, sampler_config)` that performs stratified sampling:
    - Top-K sampling: greedily assign cards based on highest probability to produce `K_primary` deals.
    - Diversity sampling: use low-discrepancy sequences for residual deals so exploration persists.
  - Optional filter: light-weight gradient boosted scoring (`BeliefFilter`) trained offline to discard low-probability partial deals before completion.
  - Determinisation integrates with `Budget::should_stop` and records sampler telemetry (entropy, unique deals explored, cache hits).
- History-filter cache
  - Before creating a new determinisation, look up `BeliefCacheKey`. If found within a freshness threshold (same trick context), reuse cached deals and stats.
  - Invalidate cache entries when a new card is revealed that reduces entropy by more than epsilon or when MDH_HARD_DETERMINISTIC=1 toggles deterministic step caps.
- Config and instrumentation
  - New env knobs: `MDH_HARD_BELIEF_TOPK`, `MDH_HARD_BELIEF_DIVERSITY`, `MDH_HARD_BELIEF_FILTER`, `MDH_HARD_BELIEF_CACHE_SIZE`.
  - Telemetry: log per-decision entropy, topK weight, number of reused deals, cache hit ratio, and filter acceptance ratio. Persist aggregated telemetry as newline-delimited JSON under `designs/tuning/telemetry/` with a configurable retention count (default keep-last 20).

### 4.2 Leverage-Gated Staged Search
- Leverage score computation
  - Components: score gap to leader, penalties on the table, number of tricks remaining, moon state (self and opponents), belief entropy, and DP availability.
  - Baseline formula (tunable constants `w_*`):  
    `leverage = w_score * clamp(score_gap / 10.0, 0.0, 5.0) + w_penalties * penalties_on_table + w_tricks * (max_tricks - tricks_remaining) + w_entropy * (1.0 - normalised_entropy) + w_moon * moon_factor`.
  - Store weights in `configs/leverage.yaml` and allow overrides through environment variables.
- Tier thresholds and behaviour
  1. Quick tier: leverage < 1.5 and penalties_on_table = 0. Use heuristics only; maintain deterministic ordering.
  2. Focused tier: 1.5 <= leverage < 3.5 or penalties_on_table > 0. Use current Hard search with belief determinisation (K_primary <= 3, next-three-opponent branching disabled).
  3. Deep tier: leverage >= 3.5 or moon/endgame flags. Expand branch limits, raise determinisation counts up to configured maximum (default 6), enable next-three-opponent branching and DP.
- Dynamic budget allocation
  - Replace static `Budget` with `DynamicBudget` containing `max_nodes`, `max_samples`, and `max_elapsed_ms` per tier.
  - Scale budgets with leverage and inverse entropy but clamp to a global ceiling to preserve latency guarantees.
  - If `Budget::should_stop` triggers early, record a monotonic fallback (tier downgrade) and return the best candidate scored so far.
- Telemetry
  - Record tier, leverage components, budgets consumed, fallback reasons, and candidate re-ordering signals in explain output and tournament statistics (tier histograms). Persist aggregated leverage stats alongside belief telemetry so tournament summaries can be regenerated offline.

### 4.3 Learning-Augmented Evaluation
- Data capture
  - Extend explain tooling to emit structured feature rows (JSONL or Parquet) for pass, lead, follow, moon, and endgame decisions.
  - Features include: hand encoding, suit counts, void map, belief-derived metrics, leverage score, candidate heuristic scores, continuation scores, final choice, outcome label (penalties captured on the next trick).
  - Store datasets under `designs/data/YYYYMMDD/seed_window/`. Exclude large raw outputs from Git via `.gitignore`.
- Model training pipeline
  - Provide `tools/train_phase_advisers.py` that consumes exported datasets.
  - Begin with gradient boosted decision trees (XGBoost or LightGBM) for low-latency inference; target lower log-loss than heuristic baseline on validation data.
  - Use k-fold validation across disjoint seed windows to avoid overfitting. Reserve the most recent window for final evaluation.
- Model integration
  - Serialize models as compact JSON weight files (for example `adviser_pass.json`) loaded via OnceLock.
  - Create `AdviserPrior` API that returns bias adjustments per candidate. Clamp bias magnitude to avoid overturning strong heuristic signals in low-leverage contexts.
  - Add environment toggles (`MDH_HARD_ADVISER_PASS`, `MDH_HARD_ADVISER_LEAD`, `MDH_HARD_ADVISER_FOLLOW`) and debug logging for prior contributions.
  - Surface the same toggles through CLI flags (`--hard-adviser-pass`, `--hard-adviser-lead`, `--hard-adviser-follow`) so QA can flip advisers without editing environment variables.
- Monitoring
  - During tournaments log adviser hit rate, disagreement rate with final choice, and cumulative penalty delta for decisions influenced by priors.

### 4.4 Enhanced Endgame and Moon Logic
- Endgame dynamic programming
  - Extend micro DP to handle up to four remaining tricks:
    - Build game tree using belief-sampled deals (topK only to avoid explosive branching).
    - Apply alpha-beta pruning with memoisation keyed by `(hand_masks, trick_leader, penalties_state)`.
    - Clamp DP-derived continuation contribution to `cont_cap` before blending with heuristic scoring.
  - When belief entropy remains high, fall back to heuristic continuation and log the fallback reason.
- Moon integration
  - Extend moon state machine with opponent moon probabilities derived from belief data (hearts ownership, control streak, score gaps).
  - Planner adjustments:
    - Increase feed-to-leader penalties when opponents are likely to shoot the moon.
    - When our seat is considering or committed, bias sampler draws toward opponents dumping hearts and adjust continuation heuristics to maintain control.
  - Telemetry: log moon probability per seat, commitment state transitions, and flags indicating decisions influenced by moon heuristics.
  - Testing: add scenario fixtures covering proactive opponent moons, abandoned moon attempts, and self-moon transitions to verify heuristics across edge cases.

### 4.5 Telemetry and Logging
- Centralise telemetry in `hard::TelemetrySink`:
  - Per-decision record containing tier, leverage components, belief stats, adviser bias, DP hits, continuation breakdown, cache usage, and fallback flags.
  - Aggregated tournament stats summarised automatically (histograms, percentiles, average bias, entropy trends).
- Provide `mdhearts --show-hard-telemetry [--out <path>]` to inspect aggregated metrics from the latest run.
- Ensure explain JSON includes new telemetry fields and document schema updates in `docs/CLI_TOOLS_ADDENDUM.md`.
- Store telemetry summaries using a rotating file scheme (configurable keep-last count, default 20) to avoid unbounded disk growth on CI runners.

### 4.6 Evaluation and Tooling Enhancements
- Scenario library
  - Store scenario JSON exported via existing tooling under `designs/scenarios/<category>/<name>.json` with metadata (category, leverage tier, expected outcome, source seed).
  - Add ignored cargo tests (`tests/scenarios/*.rs`) that replay each scenario and assert expected candidate rankings or chosen card.
- Tournament runner
  - Implement `mdhearts --tournament <config.toml>` with settings for seed windows, seat mixes, difficulty pairings, and environment overrides.
  - Output CSV plus Markdown telemetry summary to `designs/tuning/<timestamp>_<tag>.{csv,md}` including mean PPH, CI, tier histogram, adviser metrics, DP hits.
  - Extend existing mixed-seat commands with `--stats` to append leverage and belief aggregates to each row.
  - Add validation of tournament configuration (minimum seeds, distinct windows, tier coverage) and fail fast with descriptive errors when inputs are invalid.
- Benchmark guardrails
  - Extend benches to report p50 and p95 latency per tier and emit warnings when thresholds are exceeded.
  - Add `mdhearts --bench-check hard` to run heuristic and Hard benches with configured caps prior to long tournaments.

### 4.7 Configuration and Fallback Strategy
- Default behaviour keeps belief sampling and adviser priors disabled via environment toggles until each phase ships with validated metrics.
- Maintain a canonical configuration manifest (`configs/hard_default.toml`) describing all knobs and defaults. Provide `mdhearts --dump-hard-config` to print active settings for reproducibility.
- Document fallback behaviour (for example forced tier downgrade) in explain logs to simplify post-mortem analysis.

## 5. Implementation Roadmap

| Phase | Focus | Key Tasks | Exit Criteria |
|-------|-------|-----------|---------------|
| A. Foundations | Belief and telemetry scaffolding | Implement `BeliefState`, Bayesian updates, cache, telemetry sink; add unit tests for update math; expose leverage breakdown in explain logs | New tests green; telemetry visible; Hard parity maintained on smoke tournaments |
| B. Belief-Sampled Search | Integrate sampler with search | Wire sampler into rollout and next-trick probe; implement cache reuse; add env toggles and explain fields; run regression packs | Mixed-seat smoke (n=200 per seat) shows stability; latency within +/-5% of baseline |
| C. Learning Priors (Pilot) | Data capture and offline models | Export explain datasets, train initial advisers, integrate with bias caps, add toggles/logs | Adviser-on vs adviser-off smoke run shows no regressions; parity maintained |
| D. Deep Tier Enhancements | Adaptive budgets and extended DP | Implement leverage thresholds, dynamic budget, next-three branching, DP up to four tricks with belief sampling | Scenario packs show expected flips; benches <=40 ms p95; telemetry shows tier distribution within targets |
| E. Evaluation Automation | Tooling and CI integration | Build tournament runner, scenario library, telemetry summaries, bench guardrails | CI pipeline produces reports automatically; acceptance metrics computed per run |
| F. Tuning and Promotion | Quantitative tuning | Adjust weights, priors, and budgets using evaluation tooling until acceptance criteria met; document final configuration | Mixed-seat tournaments meet PPH goals; documentation updated; sign-off to promote defaults |

Dependency notes:
- Phase B depends on telemetry from Phase A.
- Phase C can overlap with B once datasets exist but should not ship until B stabilises.
- Phase D requires Phase B integration because DP relies on belief sampler outputs.
- Phases E and F run in parallel with late B-D but must finish before promoting defaults.

## 6. Dependencies and Integration Points
- Code modules: `crates/hearts-app/src/bot/{search.rs, play.rs, pass.rs, tracker.rs}`, `crates/hearts-app/src/controller.rs`, `crates/hearts-app/src/cli.rs`, new or expanded `crates/hearts-app/src/telemetry.rs` and `crates/hearts-app/src/config.rs`.
- Tooling: new training scripts under `tools/`, scenario JSON exporter, updated CLI documentation.
- Data: storage for explain snapshots (estimate 5-10 GB per full tournament run); ensure `.gitignore` covers raw datasets.
- CI pipeline: ability to run long deterministic tournaments (expect several minutes). Consider nightly jobs for full runs, smaller smoke suites for pull requests.
- Infrastructure: Python environment for training advisers; document prerequisites in `docs/training.md`.

## 7. Risks and Mitigations

| Risk | Impact | Mitigation | Detection |
|------|--------|------------|-----------|
| Belief sampler biases decisions incorrectly | Hard regresses vs Normal | Maintain diversity sampling, compare against uniform baseline, add scenario tests for tricky deals | Telemetry: entropy drops, belief-vs-uniform comparison scripts |
| Latency exceeds budget in Deep tier | CI failures, poor UX | Cap budgets per tier, record per-tier latency percentiles, automatically downgrade tier when caps are breached | Bench guardrail warnings, telemetry latency histograms |
| Adviser models overfit training seeds | Improvements fail to generalise | Use disjoint validation windows, retrain regularly with new datasets, keep adviser toggles off by default until validated | Tournament comparisons adviser-on vs adviser-off |
| DP double-counts continuation | Inflated scores, unstable rankings | Clamp DP contributions before blending with heuristics, assert caps in tests, log DP contribution percentages | Scenario tests verifying continuation sum <= cap |
| Documentation drift | Hard to reproduce results | Update README/CLI docs each phase, maintain `docs/changelog.md`, enforce checklist in PR template | Doc review in PRs, automated doc coverage (optional) |
| Telemetry storage grows without bound | CI hosts run out of disk | Rotate telemetry summaries (keep-last count), compress archives, document cleanup scripts | CI pipeline disk usage alerts, telemetry retention logs |

## 8. Open Questions and Follow-Ups
1. Belief filter choice: start with heuristic pruning or invest immediately in a trained classifier? Decide after Phase A telemetry.
2. Adviser model hosting: keep models in-repo (small JSON) or download on demand? Establish policy before Phase C.
3. CI cost: determine whether full 4x1000 tournaments run per PR or only nightly; may need smoke vs full tiers.
4. Moon opponent modelling: confirm metric thresholds (hearts count, control streak) using scenario analysis before coding heuristics.
5. Tooling ownership: assign maintainers for training scripts and evaluation runner to avoid bit rot.

## 9. Acceptance Criteria Checklist
- Mixed-seat deterministic tournaments (defined in Section 1) show Hard achieving >= +1.0 PPH advantage with 95% CI lower bound >= +0.3, and telemetry aggregates are reported.
- Scenario regression packs (moon, void, endgame, belief) all pass; goldens reproduce deterministically on Linux and Windows.
- Bench guardrails confirm <=40 ms p95 and <=5 ms p50 per Hard decision; budget fallback telemetry remains below 5% of decisions.
- Documentation and tooling updated: README, CLI docs, `docs/training.md`, design journal entries, configuration manifests.
- Explain tooling outputs leverage components, belief stats, adviser influence, DP hits, and sampler reuse metrics for each decision.
- Feature rollouts follow the testing and staging plan, with toggles documented and default states tracked in the journal.

## 10. Testing and Rollout Strategy
- Unit and property tests: cover belief updates, cache eviction, leverage score boundaries, adviser bias clamping, deterministic sampling behaviour, and DP contribution capping.
- Scenario suites: expand ignored regression packs to include belief-sensitive deals, moon transition edge cases, and endgame DP flips with advisers toggled on/off.
- Cross-seat goldens: maintain Hard vs Normal comparison tests per seat on deterministic seeds and ensure parity on Windows and Linux via nightly job.
- Rollout phases:
  1. Land feature behind an environment/CLI toggle set to off by default.
  2. Run smoke tournaments (n=200 per seat) with toggle enabled to confirm stability and latency envelopes.
  3. Promote to nightly full tournaments (n>=1000 per seat across two windows) and archive telemetry summaries.
  4. Flip default only after two consecutive nightly runs meet PPH and latency targets and documentation is updated.
- Monitoring: generate markdown dashboards from telemetry (tier distribution, entropy trends, adviser agreement, DP hit rate, latency percentiles) and surface anomalies via CI status checks.
