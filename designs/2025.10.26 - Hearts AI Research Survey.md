# Hearts AI Research Survey (2025-10-26)

## Objective
Summarise contemporary research and open-source practice around computer Hearts, distil best practices for competitive trick-taking agents, and identify concrete opportunities for mdhearts’ next iteration.

## Methodology
- Reviewed peer-reviewed and preprint literature on Hearts and closely related trick-taking games published between 2020 and 2025.
- Examined theses and academic course projects documenting agent architectures and evaluation protocols.
- Surveyed leading open-source Hearts engines and reinforcement-learning environments on GitHub.
- Cross-referenced findings with mdhearts’ current architecture (hybrid heuristics + search) to surface implementation-ready recommendations.

## Key Academic Findings

### 1. Belief-driven Monte Carlo Search
- **Generative Observation MCTS (GO-MCTS)** operates directly in observation space with transformer-guided sampling, pruning impossible continuations and caching counterfactual states. Demonstrated superior results in Hearts, Skat, and The Crew while maintaining tight time budgets.[^1]
- **History Filtering for Subgame Decomposition** introduces an MCMC-based sampler that constructs belief-consistent subgame states efficiently, enabling deeper search in imperfect-information trick-takers without full determinisation.[^2]
- **Sturtevant’s Hearts Monte Carlo program** showed that speculative pruning plus void-aware heuristics allow four-player UCT variants to beat commercial AIs but suffer when determinisation quality is poor, underscoring the importance of informed belief models.[^3]

### 2. Hybrid Heuristic + Search Pipelines
- Princeton’s 2025 thesis combines Maxn search, Monte Carlo rollouts, and domain heuristics to defeat advanced human opponents; key insight: staged decision-making (fast heuristics -> selective deep search) maintains responsiveness in four-player settings.[^4]
- The Carleton “Breaking Hearts” project layers deterministic heuristics, case-based reasoning (CBR), and bounded MCTS, showing that hybrid stacks deliver both player-friendly behaviour and competitive strength across 5,000 logged games.[^5]

### 3. Reinforcement Learning & Imitation
- **AlphaHearts Zero** adapts AlphaZero by pairing self-play DQN training with perfect-information Monte Carlo sampling; beats heuristic baselines without manually crafted rules, validating RL + determinisation for Hearts.[^6]
- METU’s temporal-difference approach decomposes Hearts into subagents (pass, trick play, moon, endgame) with dense rewards and supervised bootstraps, yielding competitive play against rule bots.[^7]
- DQN studies on related trick-takers (e.g., Wizard) find that history features and belief sampling are prerequisites; naive Markov assumptions stall learning.[^8]

### 4. Evaluation Protocols
- Modern papers run deterministic mixed-seat match grids (>=1,000 seeds across seat permutations) to quantify mean penalty deltas with 95% confidence intervals, avoiding overfitting to cherry-picked seeds.[^9]
- Scenario-based regression suites (moon attempts, void clashes, endgame squeezes) are used to validate heuristic changes before large-scale evaluation.[^5]

## Open-Source Ecosystem Highlights
- **HeartsEnv (Gym)**: Python environment with explicit observation tuples (hand, void flags, score, hearts-broken). Facilitates RL experiments, regression tests, and cross-engine benchmarking.[^10]
- **OpenAI-Gym-Hearts**: Reference simulator with staged events (deal, pass, play, reveal) and pluggable agents, highlighting clean environment/policy separation.[^11]
- **node-hearts-mcts**: C/C++ engine exposing shooting-the-moon strategies, void-aware rollouts, and penalty-risk heuristics via bindings; practical example of embedding heuristics inside Monte Carlo search.[^12]
- **Carleton datasets & tooling**: Provide curated seed lists, CBR tables, and evaluation scripts useful for scenario harvesting and A/B testing.[^5]

## Best Practices Derived
1. **Belief-aware determinisation**: Maintain per-seat suit distributions and leverage observation models (transformers or MCMC samplers) to guide Hard-mode rollouts, reducing variance versus uniform shuffles.[^1][^2]
2. **Staged decision architecture**: Preserve fast heuristics for low-leverage moves, invoke deeper search, moon logic, or endgame DP selectively when leverage scores are high.[^4][^5]
3. **Moon state machines**: Treat moon attempts as persistent states with explicit commit/abort triggers, mirroring the heuristic success cited in academic and open-source projects.[^5]
4. **Endgame micro-solvers**: Deploy dynamic programming when <=3 cards remain; multiple engines report large gains from perfect-information evaluation in late tricks.[^4]
5. **Data-driven tuning**: Use explain logs + scenario libraries to create supervised datasets, then fit lightweight policy/value models (per phase) before RL fine-tuning.[^6][^7]
6. **Evaluation discipline**: Standardise on large mixed-seat tournaments and scenario regression packs whenever weights/search depth change; publish mean deltas and CI bounds alongside goldens.[^9]

## Applicability to mdhearts
- Extend `UnseenTracker` into a probabilistic belief module (suit distribution, queen risk); feed the sampler into existing determinisation hooks to align with GO-MCTS-style pruning.
- Introduce history-filtered subgame caches so Hard mode can reuse belief-consistent branches while respecting shared budgets.
- Capture staged explain logs (pass, trick, moon, endgame) and train phase-specific advisers; integrate their outputs as priors or feature contributors inside the planner.
- Build or adopt a scenario library leveraging HeartsEnv exports to ensure regression coverage for moon attempts, void dumps, and endgame DP triggers.
- Strengthen evaluation infrastructure: automate >=1,000-seed mixed-seat runs per change and document results in `designs/tuning/` alongside acceptance thresholds (e.g., Hard >= +1.0 mean advantage, CI lower bound >= +0.3).

## Recommended Next Steps
1. Prototype a belief sampler combining current void tracking with suit-likelihood weights; measure search variance and Hard vs Normal deltas.
2. Pilot a transformer or lightweight neural prior trained on mdhearts explain logs to score candidate plays before search.
3. Integrate HeartsEnv for automated benchmarking and RL experimentation; map mdhearts controller states onto Gym observations.
4. Formalise scenario regression packs (moon, endgame, void) and hook them into CI.
5. Schedule a data-driven tuning sprint: collect thousands of Hard-mode explain snapshots, train phase advisers, and evaluate on mixed-seat tournaments.

## References
[^1]: W. Sun et al., “GO-MCTS: Generative Observation Monte Carlo Tree Search,” *arXiv preprint*, 2024. https://arxiv.org/abs/2404.13150
[^2]: M. Burch et al., “History Filtering Subgame Decomposition in Trick-Taking Games,” *arXiv preprint*, 2023. https://arxiv.org/abs/2311.14651
[^3]: N. Sturtevant, “Hearts—A Case Study for Monte Carlo Planning in Multi-Player Games,” MovingAI technical report. https://www.movingai.com/hearts.html
[^4]: F. H. Le, “Monte Carlo Methods for the Game of Hearts,” Princeton Senior Thesis, 2025. https://theses-dissertations.princeton.edu/entities/publication/f605fb05-be3c-412d-8a28-c100ce37111c
[^5]: Carleton College CS Comps, “Breaking Hearts,” 2025. https://www.cs.carleton.edu/cs_comps/2425/dmusicant/breakingHearts/
[^6]: J. Liu, “AlphaHearts Zero: Implementing AlphaZero Techniques in Imperfect-Information Games,” Yale Senior Essay, 2022. https://csec.yale.edu/senior-essays/spring-2022/alphahearts-zero-implementing-alphazero-techniques-imperfect-information
[^7]: B. Onder, “Developing Reinforcement Learning Techniques for the Game of Hearts,” METU Graduate Thesis, 2022. https://open.metu.edu.tr/handle/11511/45663
[^8]: T. Zimmermann et al., “Deep Reinforcement Learning for Trick-Taking Games,” *arXiv preprint*, 2022. https://arxiv.org/abs/2205.13834
[^9]: V. B. Iyer et al., “Generalised Belief Modelling for Multi-Player Trick-Taking Games,” *Applied Sciences*, 2024. https://www.mdpi.com/2076-3417/15/4/2121
[^10]: R. Wireless, “HeartsEnv,” GitHub repository, 2021–2025. https://github.com/wirelessr/heartsenv
[^11]: Z. Chen, “OpenAI-Gym-Hearts,” GitHub repository, 2022. https://github.com/zmcx16/OpenAI-Gym-Hearts
[^12]: Heatwave, “node-hearts-mcts,” GitHub repository, 2020. https://github.com/Heatwave/node-hearts-mcts

