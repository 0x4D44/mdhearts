# Expert-Level AI Improvement Plan for mdhearts

**Date**: 2025-11-16
**Objective**: Design and implement AI improvements to make mdhearts unbeatable by skilled human players
**Target**: Transform current intermediate-level AI into expert-level opponent

## Executive Summary

The current mdhearts AI uses a **hybrid heuristic + shallow search** architecture that is fundamentally limited by:
1. **Perfect information assumption** - treats all cards as visible during search
2. **Shallow depth** - only 1-ply lookahead (sees immediate trick outcome)
3. **Deterministic rollouts** - predictable opponent modeling
4. **Limited search time** - 10ms default budget constrains exploration

This plan proposes a **multi-phase upgrade path** combining:
- **Belief-state tracking and sampling** for imperfect information handling
- **Deeper search with transposition tables** for tactical sequence detection
- **Endgame perfect-play solver** for optimal late-game decisions
- **Opponent modeling and adaptation** to exploit human patterns
- **Opening book** for strong early-game fundamentals

**Expected outcome**: An AI that makes fewer exploitable mistakes, adapts to opponent strategies, and plays near-optimally in critical positions.

---

## Current Architecture Analysis

### What Works Well

1. **Strong heuristics**: The Normal AI's feature-based evaluation (lines 626-1145 in `play.rs`) captures most basic Hearts strategy
2. **Style detection**: Cautious/AggressiveMoon/HuntLeader switching adapts to game state (mod.rs:155-181)
3. **Void tracking**: UnseenTracker maintains accurate void states (tracker.rs:442-448)
4. **Moon detection**: Reasonable criteria for initiating moon shots (mod.rs:183-214)
5. **Tunable weights**: Extensive environment variable configuration enables experimentation

### Critical Weaknesses

#### 1. **Perfect Information Search** (SEVERITY: CRITICAL)

**Location**: `search.rs:1644-1762` (`choose_followup_search`)

```rust
// Current code assumes full knowledge:
fn choose_followup_search() {
    // Always plays lowest card when following suit (line 1659)
    // Deterministic discard strategy (lines 1663-1756)
    // NO belief state sampling
}
```

**Exploitation**: Skilled humans can:
- Count cards and deduce the AI's holdings
- Bait the AI into predictable discards
- Use void information the AI doesn't properly hide

**Impact**: ~3-5 points/game advantage for expert card counters

#### 2. **Shallow Search Depth** (SEVERITY: HIGH)

**Location**: `search.rs:300-488` (SearchConfig defaults)

- Default depth: 1-ply (current trick only)
- Optional 2-ply when enabled with 12s+ budget (disabled by default)
- Misses 3+ trick tactical sequences

**Exploitation**: Humans can set up multi-trick combinations:
- Endgame squeezes (force opponent to win unwanted trick in 2-3 tricks)
- Void creation sequences (dump suit over 2 tricks to create void for dump)
- Queen of Spades forcing plays (set up situation where opponent must take QS in 2 tricks)

**Impact**: ~2-3 points/game in endgame situations (last 5-6 tricks)

#### 3. **No Endgame Solver** (SEVERITY: HIGH)

**Observation**: Research shows endgame solvers (≤4 cards) provide large gains

**Current**: Relies on heuristics even with 1 card left in hand

**Exploitation**:
- Humans can calculate optimal endgame sequences
- AI makes suboptimal decisions in positions with 3-4 cards
- Misses opportunities to force penalties onto specific opponents

**Impact**: ~1-2 points/game in late-game situations

#### 4. **Deterministic Opponent Model** (SEVERITY: MEDIUM)

**Location**: `search.rs:1644-1762`

- Always assumes opponents play lowest legal card when following
- No adaptation to actual opponent behavior patterns
- No modeling of different skill levels

**Exploitation**:
- Predictable rollout outcomes allow humans to safely make risky plays
- AI doesn't detect when opponent is moon-shooting until late
- Fails to exploit weak opponents or defend against strong ones

**Impact**: ~1-2 points/game against varied opponents

#### 5. **Limited Time Budget** (SEVERITY: MEDIUM)

**Current**: 10ms default (Hard), configurable for Search

**Issue**: With only 6 candidates probed and 1-ply depth, misses better moves

**Impact**: ~0.5-1 point/game from suboptimal move selection

---

## Proposed Improvements

### Phase 1: Belief-State Foundation (HIGH IMPACT, MEDIUM EFFORT)

**Goal**: Make search aware of hidden information and sample from belief distributions

#### 1.1 Probabilistic Belief State

**Implementation** (`tracker.rs` extension):

```rust
pub struct BeliefState {
    // Probability distribution for each unseen card being in each opponent's hand
    card_probabilities: HashMap<Card, [f32; 4]>,  // [North, East, South, West]

    // Suit length distributions for each opponent
    suit_lengths: [[SuitDistribution; 4]; 4],  // [player][suit]

    // Certainties (known voids, revealed cards)
    known_voids: HashMap<PlayerPosition, HashSet<Suit>>,

    // Queen of Spades location belief
    qs_location_prob: [f32; 4],
}

pub struct SuitDistribution {
    // Probability mass function for 0-13 cards in suit
    pmf: [f32; 14],
}

impl BeliefState {
    // Update beliefs based on observed play
    pub fn update_from_trick(&mut self, trick: &Trick) {
        // Bayesian update:
        // - When player follows suit, they have that suit
        // - When player discards, they're void in led suit
        // - Update card probabilities for all unseen cards
    }

    // Sample a consistent hand distribution
    pub fn sample_world(&self, rng: &mut impl Rng) -> SampledWorld {
        // Monte Carlo sampling respecting:
        // - Known voids
        // - Revealed cards
        // - Inferred probabilities from past tricks
    }
}
```

**Key Features**:
- Track void information explicitly (already partially done)
- Maintain probability distribution for each unseen card
- Bayesian updates after each trick
- Consistent sampling for search rollouts

**Validation**: Test against known endgame positions where card locations are certain

#### 1.2 Belief-Aware Rollouts

**Modification** (`search.rs:1765-1836`):

```rust
fn rollout_with_belief_sampling(
    ctx: &BotContext<'_>,
    candidate: Card,
    belief: &BeliefState,
    samples: usize,
) -> f64 {
    let mut total_score = 0.0;

    for _ in 0..samples {
        // Sample a possible world from belief state
        let world = belief.sample_world(&mut thread_rng());

        // Simulate in this world
        let outcome = simulate_trick_with_world(ctx, candidate, &world);

        total_score += outcome.score;
    }

    total_score / (samples as f64)
}
```

**Configuration**:
- `MDH_BELIEF_SAMPLES`: Number of worlds to sample (default: 10-20)
- `MDH_BELIEF_UPDATE_THRESHOLD`: Minimum probability to consider (default: 0.01)

**Expected Impact**: +2-3 points/game improvement by exploiting uncertainty

---

### Phase 2: Deeper Search (HIGH IMPACT, HIGH EFFORT)

**Goal**: Look ahead 2-4 tricks to detect multi-trick tactical sequences

#### 2.1 Multi-Ply Alpha-Beta Search

**Implementation** (`search.rs` new module):

```rust
pub struct MultiPlySearch {
    max_depth: u8,
    transposition_table: TranspositionTable,
    time_budget: Duration,
}

impl MultiPlySearch {
    pub fn search(
        &mut self,
        ctx: &BotContext<'_>,
        belief: &BeliefState,
        depth: u8,
        alpha: i32,
        beta: i32,
    ) -> SearchResult {
        // Alpha-beta pruning with:
        // - Transposition table for repeated positions
        // - Move ordering (try best heuristic moves first)
        // - Iterative deepening
        // - Time-based cutoff

        if depth == 0 || ctx.hand().is_empty() {
            return self.evaluate_position(ctx, belief);
        }

        // Endgame solver takeover
        if ctx.hand().len() <= 4 {
            return self.solve_endgame(ctx, belief);
        }

        let legal_moves = ctx.round.current_trick().valid_plays(ctx.hand());
        let mut best_score = i32::MIN;

        for card in self.order_moves(&legal_moves, ctx) {
            let score = -self.search_opponent_response(
                ctx, belief, card, depth - 1, -beta, -alpha
            );

            best_score = best_score.max(score);
            alpha = alpha.max(score);

            if alpha >= beta {
                break;  // Beta cutoff
            }
        }

        SearchResult { best_score, pv: self.extract_pv() }
    }
}
```

**Transposition Table**:
```rust
pub struct TranspositionTable {
    entries: HashMap<PositionHash, TableEntry>,
    max_size: usize,
}

struct TableEntry {
    hash: u64,
    depth: u8,
    score: i32,
    best_move: Option<Card>,
    node_type: NodeType,  // Exact, LowerBound, UpperBound
}
```

**Key Features**:
- Alpha-beta pruning (orders of magnitude speedup)
- Transposition table (cache repeated positions)
- Iterative deepening (find quick answer, then improve)
- Move ordering (try promising moves first)
- Time management (abort when budget exhausted)

**Configuration**:
- `MDH_SEARCH_MAX_DEPTH`: Maximum ply depth (default: 3)
- `MDH_SEARCH_TT_SIZE`: Transposition table entries (default: 1M)
- `MDH_SEARCH_TIME_MS`: Search budget per move (default: 100ms)

**Expected Impact**: +3-4 points/game by detecting tactical sequences

#### 2.2 Iterative Deepening

```rust
pub fn choose_with_iterative_deepening(
    ctx: &BotContext<'_>,
    time_budget: Duration,
) -> Card {
    let start = Instant::now();
    let mut best_move = None;
    let mut depth = 1;

    while start.elapsed() < time_budget {
        let remaining = time_budget - start.elapsed();
        match self.search(ctx, belief, depth, i32::MIN, i32::MAX) {
            Ok(result) => {
                best_move = Some(result.best_move);
                depth += 1;
            }
            Err(Timeout) => break,
        }
    }

    best_move.unwrap_or_else(|| fallback_heuristic(ctx))
}
```

**Benefit**: Always have a valid move even if time runs out

---

### Phase 3: Endgame Perfect Play (MEDIUM IMPACT, MEDIUM EFFORT)

**Goal**: Play optimally when ≤4 cards remain

#### 3.1 Dynamic Programming Solver

**Implementation** (`bot/endgame.rs` new module):

```rust
pub struct EndgameSolver {
    memo: HashMap<EndgamePosition, EndgameResult>,
}

#[derive(Hash, Eq, PartialEq)]
struct EndgamePosition {
    hands: [SmallVec<Card>; 4],
    current_trick: Trick,
    scores: [u8; 4],
    hearts_broken: bool,
}

struct EndgameResult {
    best_move: Card,
    final_scores: [u8; 4],  // Expected final scores for each player
}

impl EndgameSolver {
    pub fn solve(&mut self, pos: &EndgamePosition) -> EndgameResult {
        // Check memo
        if let Some(result) = self.memo.get(pos) {
            return result.clone();
        }

        // Base case: no cards left
        if pos.hands.iter().all(|h| h.is_empty()) {
            return EndgameResult {
                best_move: None,
                final_scores: pos.scores,
            };
        }

        // Recursive case: try all legal moves
        let to_play = pos.current_trick.next_player();
        let legal = pos.current_trick.valid_plays(&pos.hands[to_play.index()]);

        let mut best = None;
        for card in legal {
            let next_pos = pos.apply_move(card);
            let result = self.solve(&next_pos);

            if best.is_none() || result.is_better_for(to_play, best.unwrap()) {
                best = Some((card, result));
            }
        }

        let result = EndgameResult {
            best_move: best.unwrap().0,
            final_scores: best.unwrap().1.final_scores,
        };

        self.memo.insert(pos.clone(), result.clone());
        result
    }
}
```

**Key Features**:
- Exact minimax solution for positions with ≤4 cards per player
- Memoization to avoid recomputing positions
- Considers all opponent responses
- Returns expected final scores, not just next trick outcome

**Integration**:
- Automatically invoked when hand size ≤ 4
- Can also pre-solve common 5-card positions offline

**Expected Impact**: +1-2 points/game from perfect endgame play

#### 3.2 Opening Book

**Complement to endgame**: Pre-computed optimal early plays

```rust
pub struct OpeningBook {
    positions: HashMap<OpeningHash, Vec<(Card, f32)>>,  // Card -> win rate
}

impl OpeningBook {
    // Hash first 3 tricks + passed cards
    pub fn lookup(&self, ctx: &BotContext<'_>) -> Option<Card> {
        if ctx.cards_played() > 12 {
            return None;  // Past opening phase
        }

        let hash = self.hash_opening_position(ctx);
        self.positions.get(&hash)
            .and_then(|moves| moves.iter().max_by_key(|(_, wr)| wr))
            .map(|(card, _)| *card)
    }
}
```

**Generation**: Run millions of self-play games, record outcomes

**Expected Impact**: +0.5-1 point/game from strong opening fundamentals

---

### Phase 4: Opponent Modeling (MEDIUM IMPACT, LOW-MEDIUM EFFORT)

**Goal**: Adapt to opponent behavior patterns and exploit weaknesses

#### 4.1 Opponent Play History

```rust
pub struct OpponentModel {
    // Track opponent tendencies
    pass_patterns: HashMap<PlayerPosition, PassStats>,
    play_patterns: HashMap<PlayerPosition, PlayStats>,
    aggression_level: HashMap<PlayerPosition, f32>,
    moon_frequency: HashMap<PlayerPosition, f32>,
}

struct PlayStats {
    // When leading
    lead_high_freq: f32,  // How often they lead high cards
    lead_hearts_early: f32,  // Willingness to break hearts

    // When following
    dump_aggression: f32,  // How aggressively they dump penalties
    void_creation: f32,  // Do they create voids opportunistically?

    // General
    risk_tolerance: f32,
}

impl OpponentModel {
    pub fn update(&mut self, player: PlayerPosition, trick: &Trick, ctx: &BotContext<'_>) {
        // Online learning: update stats after each observed play
    }

    pub fn predict_play(&self, player: PlayerPosition, situation: &Situation) -> Distribution<Card> {
        // Return probability distribution over likely plays
        // Used to bias belief sampling toward realistic opponent behaviors
    }
}
```

**Integration**:
- Use opponent model to bias belief sampling (more realistic worlds)
- Adjust search evaluation (if opponent is weak, take calculated risks)
- Detect moon attempts earlier (opponent playing high hearts aggressively)

**Expected Impact**: +1-2 points/game by exploiting opponent tendencies

#### 4.2 Counter-Strategies

```rust
impl OpponentModel {
    pub fn recommend_counter_strategy(&self, opponent: PlayerPosition) -> Strategy {
        if self.aggression_level[&opponent] > 0.7 {
            // Aggressive opponent: play conservatively, let them self-destruct
            Strategy::ExtraConservative
        } else if self.moon_frequency[&opponent] > 0.15 {
            // Moon-happy opponent: disrupt early, feed points to others
            Strategy::MoonDisruption
        } else {
            Strategy::Standard
        }
    }
}
```

---

### Phase 5: Advanced Techniques (LOWER PRIORITY)

#### 5.1 Counterfactual Regret Minimization (CFR)

**When**: If belief-state MCTS isn't strong enough

**What**: Compute Nash equilibrium strategy for Hearts
- Offline: Run CFR iterations to build strategy tables
- Online: Look up precomputed strategy for current information set

**Effort**: Very high (weeks of development)
**Impact**: +2-3 points/game (diminishing returns vs. good MCTS)

#### 5.2 Neural Network Policy/Value Functions

**When**: If search is too slow or needs stronger evaluation

**What**: Train neural networks on self-play data
- Policy network: Suggest candidate moves to search
- Value network: Evaluate non-terminal positions

**Approach**:
1. Generate 100k+ self-play games
2. Train on (position, move, outcome) tuples
3. Use network to guide search (AlphaZero style)

**Effort**: Very high (requires ML infrastructure)
**Impact**: +1-3 points/game (modern RL approach)

#### 5.3 Information Set MCTS (IS-MCTS)

**Alternative to determinization**:
- Build MCTS tree over information sets (what AI knows) not game states
- More theoretically sound than sampling multiple worlds
- Used in poker AIs

**Effort**: High
**Impact**: +1-2 points/game over belief-state MCTS

---

## Implementation Roadmap

### Stage 1: Foundation (2-3 weeks)

**Goals**:
- Implement probabilistic belief state tracking
- Add belief-state sampling to search
- Validate with tests on known positions

**Deliverables**:
1. `BeliefState` struct with Bayesian updates (tracker.rs extension)
2. `sample_world()` function for consistent sampling
3. Modified `rollout_with_belief_sampling()` in search.rs
4. Unit tests for belief updates (void detection, card counting)
5. Benchmark: compare deterministic vs. belief-aware search on 1000 seeds

**Acceptance Criteria**:
- Belief sampling produces valid, consistent hands
- Search with belief sampling shows +1 point/game improvement minimum
- No performance regression on deterministic mode

### Stage 2: Deeper Search (3-4 weeks)

**Goals**:
- Implement 2-4 ply alpha-beta search
- Add transposition table
- Enable iterative deepening

**Deliverables**:
1. `MultiPlySearch` struct with alpha-beta pruning
2. `TranspositionTable` with LRU eviction
3. Move ordering heuristics
4. Iterative deepening framework
5. Time management system
6. Benchmark: 3-ply search vs. current 1-ply on 1000 seeds

**Acceptance Criteria**:
- 3-ply search completes within 100ms average
- Transposition table hit rate >30%
- Shows +2 points/game improvement minimum

### Stage 3: Endgame Solver (1-2 weeks)

**Goals**:
- Perfect play when ≤4 cards remain
- Pre-compute common endgame positions

**Deliverables**:
1. `EndgameSolver` with DP memoization
2. Integration into main search (auto-invoke at ≤4 cards)
3. Offline generation of 5-card position database
4. Validation tests (known endgame puzzles)

**Acceptance Criteria**:
- Solver returns provably optimal moves for 4-card endgames
- Endgame positions resolve in <10ms
- Shows +1 point/game improvement in games reaching endgame

### Stage 4: Opponent Modeling (1-2 weeks)

**Goals**:
- Track opponent play patterns
- Bias belief sampling with opponent models
- Adapt strategy to opponent weaknesses

**Deliverables**:
1. `OpponentModel` struct with online learning
2. Integration with belief sampling
3. Counter-strategy selection
4. Evaluation against varied opponent skill levels

**Acceptance Criteria**:
- Model accurately predicts opponent plays (>40% top-1 accuracy)
- Shows +1 point/game improvement against weak opponents
- No regression against strong opponents

### Stage 5: Opening Book (1 week)

**Goals**:
- Pre-compute strong opening plays
- Handle first 3-4 tricks with book moves

**Deliverables**:
1. Opening book database (generated from self-play)
2. Position hashing and lookup
3. Integration with main search (use book when available)

**Acceptance Criteria**:
- Book covers 60%+ of opening positions
- Shows +0.5 point/game improvement

---

## Testing and Validation Strategy

### Regression Testing
- Maintain "golden" snapshot library of challenging positions
- Any change must not regress performance on goldens

### Performance Benchmarking
- Standard seed set: 1000-1999 (1000 games)
- Mixed-seat evaluation (3 baseline opponents, 1 test AI)
- Track mean penalty delta with 95% CI

### Human Play Testing
- Organize sessions with skilled Hearts players
- Track win rate and penalty differential
- Target: AI averages ≤6.5 points/game against expert humans

### Ablation Studies
- Test each component in isolation
- Measure incremental contribution
- Document in designs/tuning/

---

## Risk Mitigation

### Performance Regression
- **Risk**: New features slow down AI unacceptably
- **Mitigation**: Keep fast heuristic path for low-leverage positions; only invoke expensive search when critical
- **Threshold**: Average decision time ≤ 200ms

### Over-Optimization
- **Risk**: AI becomes too strong, not fun to play against
- **Mitigation**: Keep Normal difficulty unchanged; gate improvements behind Hard/Expert
- **Threshold**: Normal should remain beatable by intermediate players

### Code Complexity
- **Risk**: Codebase becomes unmaintainable
- **Mitigation**: Modular design, extensive documentation, unit tests
- **Threshold**: All public APIs must have doc comments

### Development Timeline
- **Risk**: Project takes too long, loses momentum
- **Mitigation**: Ship stages incrementally; each stage delivers measurable improvement
- **Threshold**: Each stage completes within estimated timeframe (± 1 week)

---

## Expected Final Strength

After implementing all stages:

| Metric | Current | Target | Delta |
|--------|---------|--------|-------|
| Points/game vs. expert human | ~8.0 | ~6.0 | -2.0 |
| Points/game vs. intermediate | ~6.5 | ~5.0 | -1.5 |
| Endgame win rate (≤4 cards) | ~65% | ~95% | +30% |
| Moon shot success rate | ~15% | ~25% | +10% |
| Tactical sequence detection | Poor | Excellent | - |

**Target**: AI should be as strong as or stronger than the best commercial Hearts games (MS Hearts, World of Card Games, etc.)

---

## Configuration Philosophy

All new features should be:
1. **Configurable via env vars** (consistency with existing system)
2. **Default to conservative settings** (opt-in for expensive features)
3. **Documented in CLI_TOOLS.md**
4. **Validated with evaluation scripts**

Example new variables:
```bash
# Belief state
MDH_BELIEF_ENABLED=1
MDH_BELIEF_SAMPLES=20
MDH_BELIEF_UPDATE_METHOD=bayesian  # or mcmc

# Search depth
MDH_SEARCH_MAX_DEPTH=3
MDH_SEARCH_TIME_MS=100
MDH_SEARCH_TT_SIZE_MB=64

# Endgame solver
MDH_ENDGAME_SOLVER_ENABLED=1
MDH_ENDGAME_THRESHOLD=4  # Cards or fewer

# Opponent modeling
MDH_OPPONENT_MODEL_ENABLED=1
MDH_OPPONENT_ADAPT_RATE=0.1
```

---

## Success Criteria

### Minimum Viable (Must Have)
- [ ] Belief-state sampling implemented and tested
- [ ] 2-ply search working with transposition table
- [ ] Endgame solver for ≤4 cards
- [ ] +4 points/game improvement vs. current Hard AI

### Target (Should Have)
- [ ] 3-ply search with <100ms average time
- [ ] Opponent modeling with adaptation
- [ ] Opening book with 60%+ coverage
- [ ] +6 points/game improvement vs. current Hard AI
- [ ] Expert humans struggle to beat the AI consistently

### Stretch (Nice to Have)
- [ ] 4-ply search in critical positions
- [ ] CFR-based strategy tables
- [ ] Neural network policy guidance
- [ ] +8 points/game improvement vs. current Hard AI
- [ ] AI wins tournaments against top online players

---

## Conclusion

The current mdhearts AI is a strong intermediate-level player but has exploitable weaknesses:
1. Assumes perfect information (doesn't account for hidden cards)
2. Only looks one trick ahead (misses multi-trick tactics)
3. Uses deterministic opponent models (predictable)
4. No perfect-play endgame solver
5. Doesn't adapt to opponent patterns

This plan addresses all five weaknesses through a **staged, incremental approach**:
- **Stage 1-2**: Core improvements (belief states, deeper search) - highest impact
- **Stage 3-4**: Specialized modules (endgame, opponent model) - medium impact
- **Stage 5**: Advanced techniques (CFR, neural networks) - lower priority

**Expected timeline**: 8-12 weeks for Stages 1-4
**Expected result**: AI that expert humans cannot reliably beat

The key insight from research is that **belief-aware search** (handling imperfect information properly) combined with **sufficient depth** (2-4 ply) is far more important than exotic techniques like deep RL. These two improvements alone should yield +5-6 points/game.

**Next Steps**:
1. Review and approve this plan
2. Set up evaluation infrastructure for continuous benchmarking
3. Begin Stage 1 implementation (belief states)
4. Establish weekly progress checkpoints

---

## References

1. **Hearts AI Research Survey** (2025-10-26) - designs/2025.10.26 - Hearts AI Research Survey.md
2. **Current Implementation Analysis** - Agent exploration of search.rs, play.rs, mod.rs
3. **Performance Analysis** - designs/2025.11.13 - Search vs Hard Performance Analysis.md
4. **Academic Papers**:
   - GO-MCTS (Sun et al., 2024)
   - History Filtering for Subgame Decomposition (Burch et al., 2023)
   - Princeton Monte Carlo Hearts thesis (Le, 2025)
   - AlphaHearts Zero (Liu, 2022)
5. **Open Source**:
   - HeartsEnv (Python Gym environment)
   - node-hearts-mcts (C++ MCTS engine)
   - Carleton Breaking Hearts (hybrid CBR + MCTS)
