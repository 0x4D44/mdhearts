# 2025.10.15 - Heuristic AI Improvement Plan

## Executive Summary

**Goal**: Improve the Hearts heuristic AI from "Hard" (regularly beatable by Arthur) to "Expert" level through systematic research, analysis, and implementation of advanced Hearts strategies.

**Current State**:
- BC Hard model trained on heuristic data achieves ~Hard-level performance
- Gen3/Gen4 RL experiments failed due to catastrophic forgetting
- Root cause: Training data quality limited by heuristic AI ceiling

**Strategy**: Improve the heuristic AI directly rather than using RL, then optionally use BC on improved heuristic for neural network inference.

**Success Criteria**:
1. Arthur can no longer consistently beat the improved AI
2. AI win rate vs current Hard baseline improves by >30%
3. AI demonstrates advanced strategies (card counting, opponent modeling, endgame play)

---

## Phase 1: Research & Discovery (Week 1)

### 1.1 Literature Review
**Objective**: Understand state-of-the-art Hearts AI strategies

**Tasks**:
- [ ] Search academic papers on Hearts AI (Google Scholar, arXiv)
- [ ] Review Hearts AI competitions/tournaments (if any exist)
- [ ] Study game theory optimal (GTO) strategies for Hearts
- [ ] Document key findings in research notes

**Deliverables**:
- `docs/hearts_ai_research.md` - Summary of academic research
- `docs/hearts_strategies.md` - Catalog of advanced strategies

### 1.2 Open Source Implementation Analysis
**Objective**: Learn from existing Hearts implementations

**Tasks**:
- [ ] Search GitHub for Hearts implementations (Java, C++, Python)
  - Keywords: "hearts game ai", "hearts bot", "hearts card game"
- [ ] Identify 3-5 high-quality implementations with strong AI
- [ ] Clone and analyze their bot logic:
  - Passing strategy
  - Lead selection
  - Follow strategy
  - Card counting/tracking
  - Risk assessment
  - Opponent modeling
- [ ] Document algorithms and techniques used

**Deliverables**:
- `docs/hearts_ai_survey.md` - Analysis of existing implementations
- Code snippets/pseudocode of interesting algorithms

### 1.3 Microsoft Hearts Reverse Engineering
**Objective**: Understand the original Microsoft Hearts AI (if possible)

**Tasks**:
- [ ] Research Microsoft Hearts implementation details
- [ ] Look for leaked source code, patents, or technical documents
- [ ] Analyze gameplay videos for strategy patterns
- [ ] Test against Microsoft Hearts to identify difficulty differences

**Deliverables**:
- `docs/microsoft_hearts_analysis.md` - Findings on original implementation

---

## Phase 2: Current Implementation Analysis (Week 1-2)

### 2.1 Code Audit
**Objective**: Deeply understand our current heuristic implementation

**Tasks**:
- [ ] Read and document `crates/hearts-app/src/bot/` modules:
  - `pass_planner.rs` - Passing strategy (line-by-line analysis)
  - `play_planner.rs` - Play strategy (line-by-line analysis)
  - `unseen_tracker.rs` - Card tracking (line-by-line analysis)
  - `bot_context.rs` - Context/state management
  - `difficulty.rs` - Difficulty levels
  - `style.rs` - Bot personalities
- [ ] Map out decision trees and heuristics
- [ ] Identify magic numbers and tuning parameters
- [ ] Document assumptions and limitations

**Deliverables**:
- `docs/current_bot_architecture.md` - Detailed documentation of existing bot
- Annotated source code with inline design notes

### 2.2 Weakness Identification Through Testing
**Objective**: Empirically identify weak points in current AI

**Tasks**:
- [ ] Play 50 games against Hard bot, taking notes on:
  - Situations where bot makes obvious mistakes
  - Predictable patterns that can be exploited
  - Missed opportunities for shooting the moon
  - Poor passing decisions
  - Suboptimal lead/follow choices
- [ ] Create test scenarios (snapshots) for specific situations:
  - Leading from weak suits
  - Defending against moon shots
  - Endgame with Queen unplayed
  - Critical passing decisions
- [ ] Run automated evaluation: Hard vs Easy/Normal (10k games each)
  - Measure win rate
  - Measure average score
  - Identify high-variance situations

**Deliverables**:
- `docs/bot_weaknesses.md` - Catalog of identified weaknesses
- `snapshots/test_cases/` - Snapshot files for regression testing
- `docs/baseline_metrics.md` - Statistical baseline of current performance

### 2.3 Strategy Gap Analysis
**Objective**: Compare our implementation to best practices from research

**Tasks**:
- [ ] Create comparison matrix:
  - Strategies from research vs strategies we implement
  - Identify missing strategies
  - Identify poorly implemented strategies
- [ ] Prioritize improvements by impact:
  - High impact, low complexity (quick wins)
  - High impact, high complexity (major features)
  - Low impact (defer)

**Deliverables**:
- `docs/strategy_gap_analysis.md` - Prioritized improvement backlog

---

## Phase 3: Implementation (Weeks 2-4)

### 3.1 Development Methodology
**Approach**: Iterative, test-driven improvements

**Process**:
1. Select one improvement from prioritized backlog
2. Write failing test case (snapshot or unit test)
3. Implement improvement
4. Verify test passes
5. Run full regression suite (all snapshot tests)
6. Benchmark against baseline (1000 games)
7. If improvement >= 5% win rate: commit and continue
8. If improvement < 5%: analyze why, iterate or revert
9. Repeat

**Testing Infrastructure**:
- [ ] Create automated benchmark script
  - `tools/benchmark_bot.sh` - Run N games, output statistics
  - Tracks: win rate, avg score, score variance, moon shot frequency
- [ ] Create regression test suite
  - `tests/bot_regression/` - Collection of snapshot tests
  - Run before/after each change to ensure no regressions
- [ ] Add bot decision logging
  - Debug mode that explains why bot chose each action
  - Helps identify faulty logic

### 3.2 High-Priority Improvements (Weeks 2-3)
**Based on common Hearts AI weaknesses - to be refined after Phase 1-2**

#### Improvement 1: Advanced Card Counting
**Current**: Basic unseen tracker
**Target**: Full probabilistic inference

- [ ] Track which cards have been played
- [ ] Infer likely card distributions based on:
  - Passing (who passed what direction)
  - Void detection (failed to follow suit)
  - High card plays (signals hand strength)
- [ ] Compute probability distributions for unseen cards
- [ ] Use distributions in decision making

**Expected Impact**: 10-15% win rate improvement

#### Improvement 2: Opponent Modeling
**Current**: Bots don't model opponent strategies
**Target**: Predict opponent behavior

- [ ] Track opponent tendencies:
  - Aggressive vs cautious
  - Moon shot frequency
  - Lead patterns
  - Passing patterns
- [ ] Adjust strategy based on opponent model:
  - Block aggressive moon shooters
  - Exploit overly cautious players
- [ ] Bayesian updating of opponent models during game

**Expected Impact**: 5-10% win rate improvement

#### Improvement 3: Passing Strategy Overhaul
**Current**: Simple high-value card dumping
**Target**: Strategic passing based on position, hand, opponents

- [ ] Evaluate multiple passing plans:
  - Dump high hearts
  - Pass complete suits (create voids)
  - Keep control cards
  - Set up moon shot
- [ ] Consider passing direction:
  - Right: your left-hand opponent (plays before you)
  - Left: your right-hand opponent (plays after you)
  - Across: your partner in 2v2 situations
- [ ] Simulate expected value of different passing strategies

**Expected Impact**: 8-12% win rate improvement

#### Improvement 4: Moon Shot Detection & Defense
**Current**: Basic moon detection, reactive defense
**Target**: Proactive detection and coordinated defense

- [ ] Early moon shot detection:
  - Analyze passed cards
  - Track player void patterns
  - Detect "dangerous" hands early
- [ ] Coordinated defense strategy:
  - Force moon shooter to take early points
  - Lead low cards in moon shooter's void suits
  - Save high cards to block
- [ ] Moon shot opportunity recognition:
  - Detect when we can shoot
  - Calculate risk/reward
  - Execute moon shot plan

**Expected Impact**: 10-15% win rate improvement

#### Improvement 5: Lead Strategy Optimization
**Current**: Simple heuristics (lead low, avoid dangerous suits)
**Target**: Context-aware lead selection

- [ ] Opening lead strategy:
  - 2 of Clubs mandatory, but which suit next?
  - Set up favorable trick patterns
- [ ] Mid-game lead strategy:
  - Lead from safe suits (Queen already played)
  - Lead to void suits if known
  - Lead to extract high cards
- [ ] End-game lead strategy:
  - Minimize expected points taken
  - Control trick flow
  - Force opponents into bad positions

**Expected Impact**: 5-8% win rate improvement

#### Improvement 6: Follow Strategy Refinement
**Current**: Basic point avoidance
**Target**: Strategic following based on position and trick state

- [ ] Position-aware following:
  - Leading position: set trick value
  - Middle position: signal or dump
  - Last position: take or dodge
- [ ] Trick state analysis:
  - Who will take this trick?
  - Can I safely dump high cards?
  - Should I take to prevent opponent moon shot?
- [ ] Signaling system:
  - High-low signals (like real Hearts players)
  - Communicate hand strength

**Expected Impact**: 5-8% win rate improvement

### 3.3 Code Quality & Architecture (Week 4)
**Objective**: Ensure maintainability and extensibility

- [ ] Refactor bot code for clarity
  - Extract complex logic into well-named functions
  - Add comprehensive documentation
  - Reduce code duplication
- [ ] Add comprehensive unit tests
  - Test individual decision functions
  - Test edge cases
  - Achieve >80% code coverage for bot modules
- [ ] Performance optimization
  - Profile hot paths
  - Optimize expensive computations
  - Ensure <100ms decision time even for complex analysis

---

## Phase 4: Validation & Tuning (Week 4-5)

### 4.1 Automated Testing
**Objective**: Measure improvement against baseline

**Tasks**:
- [ ] Run 10,000 game tournament: Improved vs Hard baseline
  - Track detailed statistics
  - Statistical significance testing (t-test, p-value < 0.05)
  - Expected: >30% win rate improvement
- [ ] Run 10,000 game tournament: Improved vs Easy
  - Ensure no regressions against weaker opponents
- [ ] Run 10,000 game tournament: Improved vs Normal
  - Full spectrum validation

**Deliverables**:
- `docs/improved_bot_results.md` - Statistical analysis of results
- Confidence that improvements are real and significant

### 4.2 Human Playtesting
**Objective**: Validate against human expert (Arthur)

**Tasks**:
- [ ] Arthur plays 50 games against improved bot
- [ ] Track:
  - Arthur's win rate (target: <50%, ideally <40%)
  - Arthur's subjective difficulty rating
  - Specific situations where bot still fails
- [ ] Iterate on remaining weaknesses

**Deliverables**:
- `docs/human_playtest_results.md` - Arthur's feedback and results

### 4.3 Parameter Tuning
**Objective**: Fine-tune heuristic weights and thresholds

**Tasks**:
- [ ] Identify key tunable parameters:
  - Risk thresholds for moon shots
  - Card value weights
  - Opponent model learning rates
  - Strategy selection thresholds
- [ ] Grid search or Bayesian optimization for best parameters
  - Run tournaments across parameter space
  - Select parameters that maximize win rate
- [ ] Document final parameter choices with rationale

**Deliverables**:
- Optimally tuned bot configuration
- `docs/parameter_tuning_results.md` - Tuning methodology and results

---

## Phase 5: Difficulty Levels (Week 5)

### 5.1 Difficulty Hierarchy
**Objective**: Create meaningful difficulty progression

**Current Levels**:
- Easy: Legacy simple bot
- Normal: Heuristic planner
- Hard: Advanced heuristic (future mode)

**New Levels** (after improvements):
- **Easy**: Intentionally weak (random-ish, mistakes)
  - No card counting
  - Simple passing (dump high cards)
  - No opponent modeling
  - Occasional mistakes
- **Normal**: Competent player
  - Basic card counting
  - Decent passing strategy
  - Simple moon defense
  - No advanced tactics
- **Hard**: Strong player (current baseline + improvements)
  - Full card counting
  - Good passing strategy
  - Moon shot detection
  - Some opponent modeling
- **Expert**: Very strong player (all improvements enabled)
  - Advanced card counting (probabilistic inference)
  - Optimal passing strategy
  - Advanced moon shot offense/defense
  - Full opponent modeling
  - Endgame optimization

**Tasks**:
- [ ] Define feature sets for each difficulty level
- [ ] Implement difficulty as feature flags or parameter sets
- [ ] Test each difficulty level for appropriate challenge
- [ ] Ensure smooth difficulty curve (Easy < Normal < Hard < Expert)

**Deliverables**:
- Four distinct difficulty levels
- `difficulty.rs` refactored to support new levels

### 5.2 Adaptive Difficulty (Optional/Future)
**Objective**: AI that adapts to player skill

**Concept**:
- Track player win rate over last N games
- If player winning >60%: increase difficulty
- If player winning <40%: decrease difficulty
- Target: keep player in 45-55% win rate range (optimal challenge)

**Tasks** (if time permits):
- [ ] Implement performance tracking
- [ ] Implement dynamic difficulty adjustment
- [ ] Test for smooth transitions

---

## Phase 6: BC Training on Improved Heuristic (Week 6)

### 6.1 Data Collection
**Objective**: Generate high-quality training data from improved heuristic

**Tasks**:
- [ ] Run improved Expert bot in self-play: 50k games
  - Save game trajectories to JSONL
  - Only keep games where all players perform well (balanced scores)
- [ ] Run improved Expert vs Hard/Normal/Easy: 30k games each
  - Diverse opponent styles improves generalization
- [ ] Total dataset: ~150k games (~7.5M experiences)

**Deliverables**:
- `bc_expert_data.jsonl` - High-quality training data

### 6.2 BC Training
**Objective**: Train neural network to clone improved heuristic

**Tasks**:
- [ ] Train BC model on expert data:
  - 20-30 epochs
  - Same architecture as BC Hard (270->256->128->52)
  - Learning rate: 1e-3
  - Batch size: 512
- [ ] Evaluate BC Expert vs Heuristic Expert:
  - Should be roughly equivalent performance
  - BC may have slight degradation (90-95% of heuristic)
- [ ] Evaluate BC Expert vs BC Hard:
  - Should show >30% improvement (matching heuristic improvement)

**Deliverables**:
- `bc_expert_weights.json` - Trained neural network
- `docs/bc_expert_results.md` - Training and evaluation results

### 6.3 Deployment
**Objective**: Ship improved AI to users

**Tasks**:
- [ ] Update embedded AI to BC Expert (or Heuristic Expert)
- [ ] Update difficulty levels in UI
- [ ] Test in full game application
- [ ] Document AI capabilities in user-facing docs

**Deliverables**:
- Shipped improved AI in game

---

## Phase 7: Future Work (Post-Plan)

### 7.1 Online Learning
- Track game outcomes against human players
- Collect anonymized game data (with consent)
- Periodically retrain BC on human+bot data
- Continuous improvement loop

### 7.2 Multi-Agent RL (Revisit)
- With improved heuristic baseline, revisit RL
- Use League Training approach:
  - Main agent
  - Main Exploiter (finds weaknesses)
  - League Exploiter (prevents forgetting)
- Or try different RL algorithm (e.g., Rainbow, SAC)

### 7.3 Neural Architecture Search
- Experiment with different network architectures
- Transformer-based models for sequential decision making
- Attention mechanisms for card tracking

---

## Timeline Summary

| Week | Phase | Deliverables |
|------|-------|--------------|
| 1 | Research & Analysis | Research docs, code audit, weakness catalog |
| 2 | Implementation (Part 1) | Card counting, opponent modeling |
| 3 | Implementation (Part 2) | Passing, moon shot, lead/follow strategies |
| 4 | Code Quality & Validation | Refactored code, test suite, benchmarks |
| 5 | Tuning & Difficulty Levels | Optimized parameters, difficulty progression |
| 6 | BC Training & Deployment | BC Expert model, shipped to production |

**Total Duration**: 6 weeks for full plan execution

---

## Success Metrics

### Quantitative Metrics
- [ ] Win rate improvement vs Hard baseline: >30%
- [ ] Arthur's win rate vs improved AI: <50% (ideally <40%)
- [ ] BC Expert accuracy: >90% match of heuristic expert decisions
- [ ] Decision time: <100ms per move (real-time performance)

### Qualitative Metrics
- [ ] AI demonstrates advanced strategies (card counting, moon shots, blocking)
- [ ] AI feels challenging but fair to human players
- [ ] Code is well-documented and maintainable
- [ ] Test suite catches regressions

---

## Risks & Mitigation

### Risk 1: Research yields no new strategies
**Mitigation**: Focus on Phase 2 (analyzing our own weaknesses) and empirical testing

### Risk 2: Improvements don't translate to win rate gains
**Mitigation**: Test each improvement individually with A/B testing, revert if no improvement

### Risk 3: Complexity makes bot too slow
**Mitigation**: Profile and optimize hot paths, cache expensive computations, use approximations

### Risk 4: Arthur still beats improved AI
**Mitigation**: Analyze specific games where Arthur wins, identify patterns, iterate

---

## Open Questions

1. **How much improvement is realistic?**
   - Unknown without research phase
   - Conservative estimate: 20-30% win rate improvement
   - Optimistic: 40-50% improvement

2. **Should we implement adaptive difficulty?**
   - Depends on user feedback
   - May be overkill for initial release

3. **What if heuristic improvements still don't reach expert level?**
   - Fall back to collecting human expert data
   - Or revisit RL with better foundation

4. **How do we measure "expert" level objectively?**
   - Arthur's subjective assessment
   - Comparison to other Hearts AI implementations (if available)
   - Tournament-style competition against other bots

---

## Resources Needed

- **Time**: ~6 weeks of focused development
- **Compute**: Minimal (heuristic dev doesn't need GPUs)
  - BC training: ~1-2 hours on CPU or 15 min on GPU
- **Testing**: Arthur's time for playtesting (5-10 games/week)

---

## Next Steps

1. **Immediate**: Begin Phase 1 - Research & Discovery
   - Search for Hearts AI papers and implementations
   - Start code audit of current bot

2. **This Week**: Complete Phase 1 and Phase 2
   - Finish research
   - Identify weaknesses
   - Create prioritized backlog

3. **Week 2-4**: Implementation sprint
   - Implement improvements iteratively
   - Test after each change
   - Track progress against baseline

4. **Week 5**: Validation
   - Arthur playtesting
   - Final tuning

5. **Week 6**: BC training and deployment
   - Train neural network on improved heuristic
   - Ship to production

---

## Appendix A: Research Keywords

**Academic Search**:
- "Hearts card game AI"
- "Imperfect information game AI"
- "Trick-taking game strategy"
- "Monte Carlo tree search Hearts"
- "Opponent modeling card games"

**GitHub Search**:
- "hearts game ai"
- "hearts bot"
- "hearts card game java"
- "hearts card game python"
- "card game artificial intelligence"

**Related Games** (transferable strategies):
- Spades (similar trick-taking)
- Bridge (advanced trick-taking, opponent modeling)
- Euchre (trump mechanics)

---

## Appendix B: Code Modules to Study

**Primary Focus**:
- `crates/hearts-app/src/bot/pass_planner.rs` - Passing strategy
- `crates/hearts-app/src/bot/play_planner.rs` - Play strategy
- `crates/hearts-app/src/bot/unseen_tracker.rs` - Card tracking

**Supporting Modules**:
- `crates/hearts-app/src/bot/bot_context.rs` - Context/state
- `crates/hearts-app/src/bot/difficulty.rs` - Difficulty levels
- `crates/hearts-app/src/bot/style.rs` - Bot personalities
- `crates/hearts-core/src/model/` - Game state, rules, cards

**Test Infrastructure**:
- `crates/hearts-core/tests/` - Existing unit tests
- `tools/eval_checkpoint.py` - Evaluation harness (adapt for heuristic)

---

## Document Control

- **Version**: 1.0
- **Author**: Claude & Arthur
- **Date**: 2025-10-15
- **Status**: Draft - Awaiting Arthur's review and approval
- **Next Review**: After Phase 1 completion
