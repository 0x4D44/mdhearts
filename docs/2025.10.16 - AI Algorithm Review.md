# AI Algorithm Review — Heuristic & Behavioral Cloning Bots (October 16, 2025)

## Scope
- Reviewed deterministic heuristic planners in `crates/hearts-app/src/bot` for passing, trick play, style selection, and tracker support.
- Inspected behavioral cloning (BC) policy pipeline across runtime integration (`crates/hearts-app/src/controller.rs`, `crates/hearts-app/src/policy/embedded.rs`) and training assets under `ai_training/`.
- Focused on practical strengths, failure modes, and near-term improvement opportunities.

## Heuristic Planner (Normal/Hard Difficulties)
### Architecture Highlights
- `BotContext` centralizes per-turn state, including unseen-card tracking and difficulty, feeding both pass and play planners (`bot/mod.rs:68-107`).
- Style selection toggles between *Cautious*, *AggressiveMoon*, and *HuntLeader* via score heuristics and moon-shot gating (`bot/mod.rs:109-163`). Hard difficulty biases toward hunting the leader once someone reaches 80 points.
- Passing logic enumerates every card, scoring with large tunable coefficients for penalty shedding, void creation, and scoreboard context before returning the top three (`bot/pass.rs:11-153`).
- Trick play evaluates each legal move by cloning round state, simulating a full trick with scripted opponent responses, then combining outcome data with style-aware scoring terms (`bot/play.rs:12-219`). Simulation reuses `UnseenTracker::infer_voids` to guess who is void in a suit before picking default responses (`bot/play.rs:22-291`, `bot/tracker.rs:11-82`).

### Strengths
- **Style-driven Objectives**: Separate scoring branches for moon attempts, leader hunting, and conservative play keeps behavior explainable and easy to tune per persona (`bot/play.rs:199-217`).
- **Score Awareness**: Passing/playing adjust to scoreboard gaps and late-round risk via `cards_played` and “near 90 points” urgency multipliers (`bot/pass.rs:145-152`, `bot/play.rs:72-88`).
- **Tracker Usage**: `UnseenTracker` supplies unseen counts and inferred voids, allowing better void-forcing passes/plays than a naive heuristic would (`bot/mod.rs:104-106`, `bot/tracker.rs:46-82`).
- **Unit Coverage**: Targeted tests lock key behaviors such as queen dumping, moon pursuit gating, and cautious trick avoidance (`bot/pass.rs:193-200`, `bot/play.rs:395-476`, `bot/mod.rs:209-295`).

### Weaknesses & Risks
- **Opponent Simulation Simplification**: During trick simulation, opponents always play lowest-following or highest-penalty otherwise, ignoring style differences or score context (`bot/play.rs:259-291`). This biases evaluations toward optimistic dumps and underestimates opponents protecting themselves.
- **Static Weight Scale**: Scores rely on hardcoded integer constants; difficulty modes share identical `BotParams`, limiting differentiation and requiring manual tuning for any behavioral change (`bot/params.rs:9-147`).
- **Sparse Tracker Knowledge**: `UnseenTracker::infer_voids` only marks voids when an off-suit card was seen, missing inference opportunities from passes, lead restrictions, or high-card exhaustion (`bot/tracker.rs:46-82`).
- **Moon Gating Heuristics**: The moon check expects ≥7 hearts including four high ranks plus Ace of Spades, which may miss viable moon hands (e.g., strong voids) and ignores remaining trick distribution (`bot/mod.rs:132-163`).
- **Costly Cloning**: Every candidate play clones the entire `RoundState` and replays a trick (`bot/play.rs:27-246`). While acceptable today, this may scale poorly for future deeper lookahead or mobile targets without caching.

### Improvement Opportunities
1. **Richer Opponent Models**: Incorporate opponent style inference or probability-weighted responses instead of deterministic lowest/highest dumps, improving trick outcome estimates.
2. **Difficulty Differentiation**: Introduce per-difficulty `BotParams` or scaling factors so “Hard” can actually pursue riskier lines beyond earlier HuntLeader switches.
3. **Expanded Tracker**: Track suit counts per opponent and integrate pass visibility to infer voids earlier; feed those probabilities directly into pass/play scoring.
4. **Moon Evaluator**: Add features such as void potential, point deficit, and hearts control pacing before flipping to AggressiveMoon to avoid premature switches and missed windows.
5. **Performance Profiling**: Cache legal moves and partial trick simulations within a turn to cut down repeated cloning when multiple cards share similar futures.

## Behavioral Cloning (BC) Policy
### Runtime Architecture
- `AiMode::BehavioralCloning` loads `EmbeddedPolicy` with compile-time weights `bc_hard_20ep_10k.json` (trained on 10k Hard games) at controller initialization (`controller.rs:10-121`).
- During play, BC builds a 270-feature observation via `ObservationBuilder` (hand, seen cards, trick context, score, void flags, recent history) before a 3-layer MLP selects the highest-logit legal card (`policy/embedded.rs:217-231`, `rl/observation.rs:1-153`).
- Passing in BC mode currently falls back to the Hard heuristic planner regardless of weight availability, because `GameController::simple_pass_for` instantiates a heuristic policy when `ai_mode` is BC (`controller.rs:339-375`).
- `EmbeddedPolicy::choose_pass` itself simply drops the three highest “danger score” cards (hearts/spades heavy), with no learned component (`policy/embedded.rs:233-247`).

### Training Pipeline Snapshot
- `ai_training/bc/hard_demos_10k.jsonl` stores ~520k supervised examples captured from the Hard heuristic bot; the production model (`bc_hard_20ep_10k.json`) reaches 93.94% top-1 accuracy and achieves Hard-level win rates per the README benchmarks (`ai_training/README.md:11-27`).
- `python/train_supervised.py` and `hearts_rl/supervised_trainer.py` train the shared actor MLP with cross-entropy on card IDs, exporting JSON weights compatible with Rust at the end (`python/train_supervised.py:1-118`, `python/hearts_rl/supervised_trainer.py:1-195`).
- Observations encode void flags via tracker lookups and a short event history but omit deeper opponent tendencies or long-horizon features (`rl/observation.rs:94-153`).

### Strengths
- **Schema Validation**: Runtime schema hash/version checks guard against mismatched observation encodings (`policy/embedded.rs:35-104`).
- **Data Efficiency**: BC achieves near-Hard performance with short training time and reasonably sized datasets (520k samples) per the recorded metrics (`ai_training/README.md:11-27`).
- **Shared Architecture**: The network mirrors the Rust inference layers, simplifying export/import without conversion errors (`policy/embedded.rs:112-188`, `python/hearts_rl/model.py:12-108`).
- **Forward With Critic Hook**: `forward_with_critic` already returns logits/log_probs, easing future RL fine-tuning (`policy/embedded.rs:250-280`).

### Weaknesses & Risks
- **Passing Mismatch**: BC gameplay still relies on heuristic passing, creating a distribution shift between training (Hard bot passes) and runtime (Hard pass heuristics + BC play) that the network never sees (`controller.rs:339-371`).
- **Deterministic Argmax**: `select_card_from_logits` always picks the highest logit, yielding zero exploration or stochasticity; this can make the bot predictable and amplify overconfident mistakes (`policy/embedded.rs:189-208`).
- **Observation Gaps**: Encoding lacks opponent score differentials beyond relative ordering and does not track who is likely to win the hand or pass outcomes, limiting strategic planning like coordinated moon defense (`rl/observation.rs:94-153`).
- **Training Bias**: Dataset consists solely of Hard heuristic actions, so BC inherits that policy’s blind spots and cannot exceed it without additional data diversity (`ai_training/README.md:11-27`).
- **Controller Fallback Fragility**: If weight loading fails, controller silently downgrades to Hard heuristic, which is safe but masks errors; lack of user-facing telemetry may hinder debugging (`controller.rs:104-120`, `controller.rs:292-303`).

### Improvement Opportunities
1. **Integrate Learned Passing**: Either train a dedicated passing head or pipe the BC policy through `EmbeddedPolicy::choose_pass`; update controller logic to reuse the BC instance instead of rehydrating heuristics.
2. **Legal-Masked Softmax Selection**: Apply a temperature-controlled softmax over legal cards (or Boltzmann exploration) to smooth predictions and support stochastic difficulty scaling.
3. **Data Augmentation**: Mix BC demonstrations with self-play snapshots or adversarial positions to expose the model to non-heuristic mistakes and rare edge cases (e.g., moon defense, endgame squeezes).
4. **Feature Expansion**: Extend observations with per-opponent penalty risk, moon attempt indicators, and pass outcomes to provide enough context for nuanced decisions.
5. **Telemetry**: Emit debug summaries when BC loads/fails and optionally expose inference confidence for QA.

## Cross-Cutting Notes
- Environment variable `MDH_BOT_DIFFICULTY` doubles as both difficulty selector and BC toggle (`controller.rs:20-30`), so documentation should clarify that `behavioral`/`bc` opts into the neural policy.
- Hard-coded parameter magnitudes suggest a future need for automated tuning (grid search or gradient-free optimizers) rather than manual edits to `BotParams` (`bot/params.rs:9-199`).
- Current test coverage focuses on deterministic expectations; consider adding regression tests that replay tricky logged scenarios to protect future heuristic or BC changes.

## Suggested Next Steps
1. Prototype a learned passing head and adjust controller plumbing to keep BC end-to-end self-consistent.
2. Add logging hooks for trick simulations to diagnose mis-evaluations and calibrate new opponent models.
3. Schedule a tuning sprint to derive per-difficulty parameter sets and evaluate impact vs. BC baseline.
4. Gather qualitative playtest feedback specifically on BC predictability to guide exploration/temperature work.

## vNext Implementation Plan Review (Senior Engineer Draft)

### Framing & Alignment
- Objectives (lift playing strength via beliefs + lookahead, maintain low-latency default, ship reproducible benchmarks) align with the weaknesses already cataloged: we identified the tracker limits, simplistic opponent modeling, and lack of instrumentation as core blockers—Workstreams A–F directly target them.
- Non-goals (full deep RL, production GO-MCTS) are appropriate given current BC stack maturity; pursuing calibrated heuristics + cheap search first should yield measurable wins while retaining deterministic fallbacks.

### Metrics & Benchmarking (Section 1 of plan)
- The proposal for a reusable tournament harness (`hearts-bench`) matches the need to validate our earlier improvement ideas (passing, moon defense, stochastic BC). Today we only have targeted unit tests; adopting ≥3k-match Wilcoxon protocols will supply statistical backing absent from current workflow.
- Ensure harness integrates existing replay serialization under `crates/hearts-app/src/game/serialization.rs` so belief-driven planners can be regression tested on identical deals. Add latency counters around `GameController::autoplay_one` to populate the requested `speed_ms_turn`.

### Workstream A — Belief Tracker
- Matches our “Expanded Tracker” recommendation: moving from boolean unseen flags to probabilities lets planners reason about likely heart/spade holders. The suggested API slots naturally beside `UnseenTracker`; we should plan an incremental migration where `Belief` coexists with legacy tracker until play/planner parity is proven.
- Soft policy updates should start with hand-authored likelihood tables derived from current heuristics (e.g., queen avoidance) and later absorb logged BC moves. Maintain unit tests similar to existing tracker tests to cover void inference.
- Hash-based caching will help contain the “Costly Cloning” concern we raised; reuse the proposed `hash_key` to dedupe sample worlds across sibling nodes in Workstreams D/E.

### Workstream B — Passing Overhaul
- Directly addresses the “Passing Mismatch” and “Static Weight Scale” issues: direction-aware heuristics combined with belief-driven liability estimation will reduce queen retention and support moon setups. Ensure the API hooks into `PassPlanner::choose` path; we can initially gate new logic behind a runtime flag for controlled A/B runs in the harness.
- Grid search tooling should output configs consumable by `BotParams` (or a new `PassParams`) to avoid manual constant editing.

### Workstream C — Moon Detection & Coalition Defense
- Aligns with our “Moon Evaluator” suggestion. Hooking the probability output into `determine_style` (or the future objective switch) can prevent late recognition of moon threats.
- Pay attention to BC parity: if heuristics begin coalition switching, BC baseline should either learn from these traces or the plan should log objective transitions so BC datasets can be refreshed later.

### Workstream D — Search-lite on Beliefs
- The bounded depth determinization aligns with our recommendation for richer opponent models without jumping straight to MCTS. Re-using `Belief::sample_world` keeps simulation work contained.
- Need to manage cloning overhead: before launching this workstream, profile `RoundState::clone` and consider a scratchpad pool or incremental update API to avoid repeated allocations flagged earlier. Instrument tree node counts via Workstream F guardrails.

### Workstream E — Endgame Solver
- Tackles a known pain point (endgame mishandling) that current heuristics and BC both suffer from. Leveraging exact search once hands shrink is a pragmatic compromise that should integrate with Search-lite infrastructure.
- Remember to feed solver rollouts back into the harness for targeted regression suites (e.g., curated endgame datasets under `docs/` or `assets/`).

### Workstream F — Instrumentation & Tuning
- Direct response to our telemetry gaps: logging belief entropy, moon probability, and sampled world hashes enables repro of questionable plays, satisfying both plan and earlier “Telemetry” recommendation.
- Latency guardrails will be critical when integrating belief sampling and search; implement per-move budgets at the `GameController` level so heuristics can bail out gracefully.

### Optional Analysis Hooks
- Deferred GO-MCTS/RIS-MCTS integration is sensible; ensure Search-lite APIs expose enough context (beliefs, objective) that a future analysis binary can swap in heavier planners without rewriting the core.

### Risk & Dependency Highlights
1. **Belief Accuracy Before Search**: Workstreams D/E depend on A. Schedule smoke tests (10k deals as suggested) before enabling search-lites globally.
2. **Passing vs. BC Divergence**: When PASS‑100 lands, regenerate BC datasets or fence the new passing logic behind a toggle during BC matches to avoid distribution drift.
3. **Benchmark Harness Critical Path**: BENCH‑001 should complete before other streams to validate every change; treat the harness as a shared dependency for acceptance criteria B–F.
4. **Telemetry Storage**: Structured logs may be large; plan retention/rotation strategy (e.g., default to ring buffer, optional disk dump via CLI flag).

### Recommended Execution Order
1. **BENCH‑001** → Provides reproducible validation.
2. **BELIEF‑010/BELIEF‑020** → Unlocks passing/search improvements.
3. **PASS‑100 & MOON‑200** → Biggest heuristic wins, aligns with our high-impact findings.
4. **INST‑500** → Add observability before layering stochastic search.
5. **SEARCH‑300 / END‑400** → Activate once beliefs + instrumentation are stable.
6. **BASE‑XINXIN & DOC‑SOTA** → Update documentation and baselines concurrently to communicate progress.

### Updated Action Items
1. Implement BENCH‑001 harness and wire into CI smoke targets; include sample YAML and reproducibility docs.
2. Prototype `Belief` struct alongside existing `UnseenTracker`, with tests mirroring Workstream acceptance criteria.
3. Stand up direction-aware pass planner using belief probabilities, benchmarking via BENCH harness.
4. Add moon-probability estimator feeding into both heuristic objectives and logs; plan BC dataset updates post-release.
5. Profile and optimize `RoundState` cloning ahead of Search-lite rollout, introducing time budgets & logging from INST‑500.
