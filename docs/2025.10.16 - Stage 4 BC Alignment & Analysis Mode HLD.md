# Stage 4 HLD — Behavioral Cloning Alignment & Analysis Hooks

## Overview
- Realign behavioral cloning (BC) models with the upgraded heuristic/planning stack introduced in Stages 1–3.
- Provide infrastructure for advanced analysis modes (RIS-MCTS, GO-MCTS integration) without impacting default gameplay.
- Ensure telemetry and datasets capture belief hashes, pass choices, and search rationale to facilitate offline training.

## Goals
- Regenerate BC datasets that reflect new belief-informed passing, coalition defense, and search-lite decisions.
- Update runtime policy integration to interoperate with belief structures, moon objectives, and analysis mode toggles.
- Expose opt-in analysis mode interface enabling slower, stronger planners (RIS-/GO-MCTS) for future experimentation.
- Validate that refreshed BC policies match or exceed Hard difficulty performance under Stage 3 baseline metrics.

## Non-Goals
- Directly implementing GO-MCTS within this stage (provide hooks/specs only).
- Revisiting low-level belief or search algorithms (assumed stable from prior stages).
- Delivering production-grade UX for analysis tools (CLI/flags acceptable).

## Architecture
- Dataset pipeline (`python/`):
  - `python/hearts_rl/data_capture.py`: consumes Stage 3 telemetry logs (`belief_hash`, `pass_decision`, `search_log`) to build supervised examples.
  - `python/hearts_rl/dataset_v2/`: new schema storing belief summaries, objective states, and chosen actions.
  - `python/hearts_rl/train_bc.py`: retrain MLP or transformer policy leveraging updated features; support curriculum mixing heuristics and search-lite traces.
- Runtime (`crates/hearts-app/src/policy/`):
  - `embedded.rs`: expand observation builder to include belief entropy bins, moon probability, objective flag, and search-lite hints (bounded for backwards compatibility).
  - `analysis.rs`: new module exposing trait `AdvancedPlanner` with implementations for RIS-MCTS adapter and GO-MCTS process bridge.
  - `controller.rs`: orchestrates switching between BC, heuristic, search-lite, and analysis planners based on `TableMode` (`Default`, `Analysis`).
- Configuration:
  - `config/policy.toml`: toggles for selecting BC model version, enabling belief features, or forcing heuristic fallback.
  - `config/analysis.toml`: endpoints and resource budgets for RIS-/GO-MCTS connectors.

## Dataset Capture & Training Flow
1. During Stage 3 rollouts, enable telemetry to record for each decision:
   - `belief_hash`, entropy, sampled world IDs.
   - Selected move, top alternatives with scores.
   - Objective state and moon probability.
2. `data_capture.py` ingests JSONL logs, normalizes features, and writes shard files (`.npz`/`.parquet`).
3. Training script:
   - Splits data by scenario (pass/play/endgame).
   - Applies class balancing and label smoothing to prevent overfitting to deterministic choices.
   - Outputs model artifacts (`bc_v2_default.json`, `bc_v2_analysis.json`) plus metadata (training seeds, accuracy metrics).
4. Validation:
   - Offline evaluation on hold-out deals; Stage 0 harness playback using new policies for regression.
   - Speed tests to confirm inference latency remains under 2 ms/decision on target hardware.

## Analysis Mode Hooks
- Define `AdvancedPlanner` trait with async interface to accommodate external engines:
  ```rust
  #[async_trait]
  pub trait AdvancedPlanner {
      async fn evaluate(&mut self, state: &GameState, belief: &Belief, cfg: &AnalysisCfg) -> PlannedMove;
  }
  ```
- RIS-MCTS adapter (stub):
  - Lives in `crates/hearts-app/src/analysis/ris_mcts.rs`.
  - Uses belief determinization pipeline to supply consistent determinized states.
  - Configurable rollout count, tree depth; respects time budget (default 10 s).
- GO-MCTS bridge:
  - `tools/go_mcts_proxy`: spawns external process, communicates via gRPC or JSON IPC.
  - Provide spec doc (`docs/analysis/GO-MCTS-interface.md`) detailing protocol.
- Controller integration:
  - `TableMode::Analysis` toggled via CLI or debug menu.
  - When enabled, controller defers to advanced planner for move selection; falls back to search-lite if planner unavailable.

## Telemetry & Observability
- Broadcast dataset version and planner mode in structured logs.
- Record analysis planner latency, node counts, and final selection rationale.
- Capture BC inference entropy and disagreement with heuristic/search-lite for offline debugging.

## Testing Strategy
- Dataset integrity tests ensuring new feature columns align between runtime logs and training pipeline.
- Policy regression tests: ensure old observation builder still functions when new fields absent.
- Analysis mode smoke test using mock planner that returns scripted moves; verifies async control flow and fallbacks.
- Performance benchmark: run 1000 inferences with BC v2 to confirm latency budgets.
- Harness validation: Stage 0 benchmark comparing BC v2 vs. Hard heuristic; require parity or improvement in PPH.

## Risks & Mitigations
- **Distribution shift**: Blend historical heuristics with new search-lite traces during training; monitor evaluation metrics before promoting models.
- **Model size creep**: Keep BC architecture lightweight (e.g., 3-layer MLP with LayerNorm) unless analysis mode explicitly requests heavier models.
- **External planner instability**: Keep advanced planners optional; gate behind feature flags and document fallback path.
- **Data privacy/log volume**: Allow log sampling during capture; rotate output directories to prevent disk exhaustion.

## Acceptance Alignment
- New BC models deliver equal or better performance than Hard difficulty in benchmark harness.
- Runtime successfully toggles between default play, search-lite, and analysis modes without crashes.
- Documentation updated with dataset schema, training commands, and analysis mode usage guidelines.
