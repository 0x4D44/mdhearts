# AI Bot System Architecture

**Date:** 2025-11-06
**Module:** crates/hearts-app/src/bot/
**Purpose:** Sophisticated multi-difficulty AI system for Hearts gameplay

## Executive Summary

The mdhearts AI system is one of the most sophisticated card game AI implementations in existence, featuring four difficulty levels with dramatically different strategies. The system spans ~5,000 lines of carefully tuned Rust code implementing everything from simple heuristics to shallow search with continuation scoring, leverage-based adaptive search, belief-guided opponent modeling, and optional perfect-play endgame solving.

**Key Statistics:**
- **6 core modules:** mod, play, search, pass, tracker, adviser
- **4 difficulty levels:** Easy, Normal, Hard, Search
- **60+ configuration variables:** Comprehensive tuning system
- **Deterministic mode:** Reproducible evaluation for AI research
- **Test coverage:** Extensive unit tests across all modules

## Architecture Overview

```
bot/
├── mod.rs           (~460 lines)  - Core types, difficulty system, bot context
├── play.rs          (~1700 lines) - PlayPlanner (Normal heuristic AI)
├── search.rs        (~1900 lines) - PlayPlannerHard (Hard search AI)
├── pass.rs          (~600 lines)  - PassPlanner (card passing logic)
├── tracker.rs       (~640 lines)  - UnseenTracker (card tracking, beliefs)
└── adviser.rs       (~65 lines)   - Optional bias adjustment system
```

## Core Module (mod.rs)

### Key Types

#### BotDifficulty
```rust
pub enum BotDifficulty {
    EasyLegacy,         // Simple legacy heuristics
    NormalHeuristic,    // Feature-based heuristic planner (default)
    FutureHard,         // Shallow search with continuation
    SearchLookahead,    // Reserved for future deep search
}
```

Selected via `MDH_BOT_DIFFICULTY` environment variable.

#### BotStyle
```rust
pub enum BotStyle {
    Cautious,          // Safe, penalty-avoiding play
    AggressiveMoon,    // Attempting to shoot the moon
    HuntLeader,        // Targeting the score leader
}
```

Dynamically determined based on:
- Hand quality (moon detection)
- Score situation (leader detection)
- Moon state (committed moon attempts)

#### BotContext
```rust
pub struct BotContext<'a> {
    pub seat: PlayerPosition,
    pub round: &'a RoundState,
    pub scores: &'a ScoreBoard,
    pub passing_direction: PassingDirection,
    pub tracker: &'a UnseenTracker,
    pub difficulty: BotDifficulty,
}
```

Immutable snapshot of all information available to the AI for a decision.

#### DecisionLimit
```rust
pub struct DecisionLimit<'a> {
    pub deadline: Option<Instant>,
    pub cancel: Option<&'a AtomicBool>,
}
```

Supports both timeout-based and cancellation-based decision limits.

### Bot Style Determination

The `determine_style()` function implements sophisticated style selection:

**Moon Shooting Triggers:**
- Persisted moon state: `Committed` → `AggressiveMoon`
- Hand quality check: 7+ hearts, 4+ control hearts (≥10), 2+ high spades including Ace
- Score constraints: own score < 70, not too far behind leader
- Timing constraint: early round (≤12 cards played)

**Hunt Leader Triggers:**
- Hard difficulty: leader ≥ 80 points (aggressive)
- Normal difficulty: leader ≥ 90 points (conservative)
- Must not be the leader yourself

**Default:** `Cautious` for safe penalty-avoiding play


## PlayPlanner (Normal Heuristic AI)

Location: `play.rs` (~1700 lines)

### Architecture

The Normal AI uses a **feature-based heuristic scoring system** with single-trick simulation:

1. **Generate candidates:** All legal cards
2. **Score each candidate:** Apply ~15 weighted features
3. **Simulate trick:** Project immediate trick outcome
4. **Apply adviser bias:** Optional external guidance
5. **Select best:** Highest scoring candidate (with ties broken by card order)

### Scoring Features

#### Core Features (Always Active)
1. **Base heuristic:** Fundamental card value assessment
2. **Penalty avoidance:** Avoid capturing hearts/Q♠
3. **Void creation:** Bonus for creating suit voids
4. **Hearts broken:** Consider hearts-breaking state
5. **Lead safety:** Different logic when leading vs. following

#### Style-Specific Features

**Cautious Style:**
- **Near-100 urgency:** Aggressive penalty shedding when own score ≥ 85
- **Leader feeding:** Bonus for passing penalties to the leader
- **Non-leader protection:** Penalty for feeding non-leaders

**AggressiveMoon Style:**
- **Control maximization:** Favor high cards to win tricks
- **Penalty collection:** Reverse normal penalty avoidance
- **Hearts leading:** Aggressive hearts leading

**HuntLeader Style:**
- **Leader targeting:** Large bonuses for feeding leader penalties
- **Gap-based scaling:** Bigger bonus when leader has larger lead

### Configuration Variables (Normal AI)

```bash
# Tunable weights (defaults shown)
MDH_W_OFFSUIT_BONUS=600              # Bonus per penalty when void dumping
MDH_W_CARDS_PLAYED=10                # Pacing factor
MDH_W_EARLY_HEARTS_LEAD=600          # Penalty for early hearts lead
MDH_W_NEAR100_SELF_CAPTURE_BASE=1300 # Base penalty near 100
MDH_W_NEAR100_SHED_PERPEN=250        # Bonus per penalty shed near 100
MDH_W_HUNT_FEED_PERPEN=800           # Bonus per penalty to leader
MDH_W_PASS_TO_LEADER_PENALTY=1400    # Penalty when passing to leader
MDH_W_LEADER_FEED_BASE=120           # Base leader feed bonus
MDH_W_NONLEADER_FEED_PERPEN=1200     # Penalty for feeding non-leader
MDH_W_LEADER_FEED_GAP_PER10=40       # Gap-based leader feed scaling
```

### Key Algorithms

#### Base Score Calculation
Considers:
- Rank value (high vs. low)
- Suit context (trump-like behavior)
- Trick position (leading vs. following)
- Current trick state (winning, penalties)

#### Void Creation Detection
Detects when playing a card would empty a suit:
```rust
let would_void = hand.count_suit(card.suit) == 1 && card.suit != Suit::Hearts;
```
Valuable for future penalty dumping.

#### Leader Feed Calculation
```rust
let gap = leader_score.saturating_sub(my_score);
let gap_bonus = (gap / 10).min(3) * gap_per_10_weight;
let feed_score = base + (penalties * perpen_weight) + gap_bonus;
```

Scales with leader's advantage.


## PlayPlannerHard (Hard Search AI)

Location: `search.rs` (~1900 lines)

### Architecture Overview

The Hard AI implements **shallow search with continuation scoring**:

1. **Base scoring:** Use Normal AI heuristics for initial ranking
2. **Candidate selection:** Pick top-K candidates by base score
3. **Continuation probing:** For each candidate, simulate potential follow-up tricks
4. **Total scoring:** base_score + continuation_score
5. **Select best:** Highest total score

This provides **limited lookahead** while maintaining reasonable performance (<30ms typical).

### Search Strategy

#### Phase A: Base Scoring
- Evaluate all legal cards using Normal AI features
- Rank by base score
- Apply early cutoff: stop scanning when next candidate cannot possibly win

#### Phase B: Continuation Scoring (Top-K only)
For top-K candidates (default K=6):
1. **Simulate playing the candidate**
2. **Determine next trick leader**
3. **Probe potential next-trick scenarios:**
   - If we lead: try M best candidate leads (default M=3)
   - If opponent leads: sample likely opponent leads based on beliefs
4. **Evaluate outcomes:** penalties captured, control gained, etc.
5. **Aggregate:** Compute continuation score

#### Phase C: Selection
- Combine base + continuation for each top-K candidate
- Select highest total score
- Break ties by card order

### Continuation Scoring Components

```rust
continuation_score = 
    + feed_leader_bonus * penalties_to_leader
    - self_capture_penalty * penalties_we_capture
    + singleton_bonus * num_singletons_if_we_lead
    + hearts_control_bonus * num_hearts_if_we_lead_broken
    + moon_relief_bonus * penalties_when_mooning
    - qs_risk_penalty * (have_AS && will_lead)
    - handoff_penalty * (!will_lead)
```

All components are **tiny weights** (50-100 range) to avoid overwhelming base score.

### Leverage-Based Adaptive Search

**Leverage:** Measures decision importance (0-100 scale)

High leverage situations:
- Queen of Spades risk exposure
- Multiple penalty cards in play
- Close candidates (near-tie decisions)
- Critical trick positions (first heart, etc.)

**Tiering System:**
- **Narrow tier** (leverage < 20): Minimal search (topk=4, M=1, ab_margin=100)
- **Normal tier** (20 ≤ leverage < 50): Moderate search (topk=6, M=2, ab_margin=150)
- **Wide tier** (leverage ≥ 50): Deep search (topk=8, M=3, ab_margin=200)

Enable via: `MDH_HARD_TIERS_ENABLE=1`

### Deterministic Mode

For reproducible testing and evaluation:

**Time-Based (Default):**
```rust
MDH_HARD_TIME_CAP_MS=10  // 10ms wall-clock limit
```
Scans candidates until time expires.

**Step-Based (Deterministic):**
```rust
MDH_HARD_DETERMINISTIC=1
MDH_HARD_TEST_STEPS=120   // Fixed step budget
```
Scans exactly N simulation steps regardless of machine speed.

Critical for:
- Regression testing
- Performance benchmarking
- AI research comparisons

### Belief-Guided Opponent Modeling

When simulating opponent actions, Hard AI uses **void tracking and probability:**

```rust
fn sample_opponent_lead(beliefs: &BeliefState, rng: &mut Rng) -> Card
```

1. **Check for voids:** Use tracker's void inference
2. **Compute probabilities:** Based on cards seen/unseen
3. **Sample from distribution:** Top-K likely cards + diversity factor
4. **Apply filters:** Optionally drop zero-probability cards

Configuration:
```bash
MDH_HARD_BELIEF_TOPK=3         # Prioritize top 3 likely cards
MDH_HARD_BELIEF_DIVERSITY=2    # Add 2 diversity samples
MDH_HARD_BELIEF_FILTER=1       # Drop impossible cards
MDH_HARD_BELIEF_CACHE_SIZE=128 # Cache size for belief states
```

### Endgame Perfect Play (Optional)

For final 1-3 tricks, Hard AI can use **dynamic programming** for perfect play:

**DP Solver:**
- Enumerates all possible card distributions
- Computes optimal play for each state
- Guarantees best possible outcome

**Triggered when:**
- ≤3 tricks remaining
- Endgame DP enabled: `MDH_HARD_ENDGAME_DP=1`
- Computational budget available

**Performance:** ~1-10ms for 1 trick, ~10-100ms for 2 tricks, ~50-500ms for 3 tricks


### Hard AI Configuration (Comprehensive)

#### Search Control
```bash
MDH_HARD_BRANCH_LIMIT=6              # Top-K candidates for continuation
MDH_HARD_NEXT_BRANCH_LIMIT=3         # Next-trick leads to probe
MDH_HARD_TIME_CAP_MS=10              # Wall-clock timeout
MDH_HARD_DETERMINISTIC=1             # Enable step-based budget
MDH_HARD_TEST_STEPS=120              # Step budget when deterministic
MDH_HARD_EARLY_CUTOFF_MARGIN=300     # Early pruning margin
MDH_HARD_PHASEB_TOPK=0               # Only compute continuation for top-K (0=all)
```

#### Continuation Weights
```bash
MDH_HARD_CONT_FEED_PERPEN=70         # Bonus per penalty to leader
MDH_HARD_CONT_SELF_CAPTURE_PERPEN=95 # Penalty per self-captured penalty
MDH_HARD_NEXTTRICK_SINGLETON=25      # Bonus per singleton if we lead next
MDH_HARD_NEXTTRICK_HEARTS_PER=2      # Bonus per heart if broken & we lead
MDH_HARD_NEXTTRICK_HEARTS_CAP=10     # Cap for hearts component
MDH_HARD_CONT_BOOST_GAP=0            # Apply boost when base within gap (0=off)
MDH_HARD_CONT_BOOST_FACTOR=1         # Boost multiplier (1=none)
MDH_HARD_QS_RISK_PER=0               # QS exposure penalty (0=off)
MDH_HARD_CTRL_HEARTS_PER=0           # Hearts control bonus (0=off)
MDH_HARD_CTRL_HANDOFF_PEN=0          # Handoff penalty (0=off)
MDH_HARD_CONT_CAP=0                  # Symmetric cap on continuation (0=off)
MDH_HARD_MOON_RELIEF_PERPEN=0        # Moon penalty relief (0=off)
```

#### Belief System
```bash
MDH_HARD_BELIEF_TOPK=3               # Top-K cards to prioritize
MDH_HARD_BELIEF_DIVERSITY=2          # Diversity samples beyond top-K
MDH_HARD_BELIEF_FILTER=1             # Drop zero-probability cards
MDH_HARD_BELIEF_CACHE_SIZE=128       # Belief state cache capacity
```

#### Tiering & Leverage
```bash
MDH_HARD_TIERS_ENABLE=1                     # Enable adaptive tiering
MDH_HARD_TIERS_DEFAULT_ON_HARD=1            # Auto-enable for Hard only
MDH_HARD_LEVERAGE_THRESH_NARROW=20          # Narrow tier threshold
MDH_HARD_LEVERAGE_THRESH_NORMAL=50          # Normal tier threshold
```

#### Feature Flags
```bash
MDH_FEATURE_HARD_STAGE1=1            # Enable planner nudges & guards
MDH_FEATURE_HARD_STAGE2=1            # Enable moon/gap follow-ups
MDH_FEATURE_HARD_STAGE12=1           # Enable all Stage 1+2 features
MDH_HARD_NEXT3_ENABLE=1              # Enable 3-opponent branch probing
MDH_HARD_ENDGAME_DP=1                # Enable perfect endgame solver
```

#### Adviser & Telemetry
```bash
MDH_HARD_ADVISER_PLAY=1              # Enable adviser bias
MDH_ADVISER_PLAY_PATH=path.json      # Adviser bias JSON file
MDH_HARD_TELEMETRY_KEEP=20           # Max telemetry exports to retain
```

#### Debug & Tuning
```bash
MDH_DEBUG_LOGS=1                     # Enable detailed decision logging
MDH_HARD_VERBOSE=1                   # Print continuation breakdown
MDH_HARD_PLANNER_NUDGE_TRACE=1       # Trace planner nudge hits
```

## PassPlanner (Card Passing)

Location: `pass.rs` (~600 lines)

### Strategy

Pass selection uses **strategic priority system:**

1. **Queen of Spades (highest priority)**
   - Extremely dangerous card
   - Pass unless shooting moon or holding A♠ for protection

2. **High Hearts (10-K)**
   - Second priority
   - Difficult to avoid capturing penalties
   - Keep only if shooting moon

3. **High Spades (J-K, not Q)**
   - Third priority  
   - Can be forced to win Q♠ trick

4. **Other High Cards**
   - Fourth priority
   - General trick-winning risk

5. **Low Cards (if moon shooting)**
   - Pass low cards when attempting moon
   - Keep high cards for control

### Algorithm

```rust
pub fn choose_cards_to_pass(hand: &Hand, direction: PassingDirection, 
                             scores: &ScoreBoard, seat: PlayerPosition) -> [Card; 3]
```

1. **Determine strategy:** Check if shooting moon
2. **Build candidate list:** Rank all cards by pass priority
3. **Select top 3:** Choose 3 worst cards to give away
4. **Validate:** Ensure legal pass combination

### Special Cases

- **Hold direction:** No passing, returns dummy cards
- **Moon attempt detected:** Inverts priorities (pass low, keep high)
- **Defensive passing:** Considers opponent scores (don't help leader)


## UnseenTracker (Card Tracking & Beliefs)

Location: `tracker.rs` (~640 lines)

### Responsibilities

1. **Card tracking:** Which cards have been revealed
2. **Void inference:** Deduce opponent voids from plays
3. **Probability computation:** Compute card location probabilities
4. **Moon state management:** Track moon shooting attempts
5. **Belief caching:** Cache probability computations

### Core Data Structures

```rust
pub struct UnseenTracker {
    unseen_cards: BitSet,              // 52-bit set of unseen cards
    void_knowledge: [SuitVoids; 4],    // Per-player suit voids
    moon_states: [MoonState; 4],       // Per-player moon status
    belief_cache: LruCache<...>,       // Cached probability distributions
}
```

**BitSet:** Compact representation using u64 for fast operations

**SuitVoids:** Per-suit void flags for each player

```rust
pub struct SuitVoids {
    clubs: bool,
    diamonds: bool,
    spades: bool,
    hearts: bool,
}
```

**MoonState:**
```rust
pub enum MoonState {
    NotAttempting,   // Normal play
    Considering,     // Evaluating moon attempt
    Committed,       // Actively shooting moon
    Aborted,         // Gave up on moon
}
```

### Key Algorithms

#### Void Inference
When a player fails to follow suit, mark that suit as void:

```rust
pub fn note_play(&mut self, card: Card, seat: PlayerPosition, 
                  trick_leader: PlayerPosition, led_suit: Suit)
```

Tracks:
- Cards played (remove from unseen set)
- Suit voids (when not following suit)
- Moon state transitions

#### Probability Distribution
Compute probability of each unseen card being with a specific player:

```rust
pub fn compute_probabilities(&self, seat: PlayerPosition) -> HashMap<Card, f32>
```

Algorithm:
1. Count unseen cards for this player
2. For each unseen card:
   - If player void in that suit: probability = 0.0
   - Otherwise: probability = 1.0 / num_unseen_cards
3. Normalize to sum to 1.0

#### Belief Cache
```rust
belief_cache: LruCache<(PlayerPosition, u64), ProbabilityDist>
```

- **Key:** (player, unseen_bitset hash)
- **Value:** Precomputed probabilities
- **Capacity:** Configurable (default 128)
- **Benefit:** Avoid recomputing expensive distributions

### Moon State Transitions

```
NotAttempting → Considering → Committed → (success/fail)
       ↓             ↓            ↓
    Aborted ←──── Aborted ←──── Aborted
```

**Transition Triggers:**
- **→ Considering:** Win first clean control trick with good hand
- **→ Committed:** Win multiple tricks, meet all criteria (hearts, control, score)
- **→ Aborted:** Opponents collect penalties, near end of round, lost control

**Configuration:**
```bash
MDH_MOON_COMMIT_MAX_CARDS=20       # Max cards played to commit
MDH_MOON_COMMIT_MAX_SCORE=70       # Max score to commit
MDH_MOON_COMMIT_MIN_TRICKS=2       # Min tricks won to commit
MDH_MOON_COMMIT_MIN_HEARTS=5       # Min hearts needed
MDH_MOON_COMMIT_MIN_CONTROL=3      # Min high hearts (≥10)
MDH_MOON_ABORT_OTHERS_HEARTS=3     # Abort threshold
MDH_MOON_ABORT_NEAREND_CARDS=36    # Abort near round end
MDH_MOON_ABORT_MIN_HEARTS_LEFT=3   # Abort if too few hearts left
```

### Performance Characteristics

- **Card tracking:** O(1) bitset operations
- **Void inference:** O(1) per play
- **Probability computation:** O(unseen_cards) per query
- **Belief cache:** O(1) lookup, LRU eviction
- **Moon state update:** O(1) per trick

## Adviser System

Location: `adviser.rs` (~65 lines)

### Purpose

Optional external bias injection for Hard AI tuning. Allows specifying per-card score adjustments without recompiling.

### Configuration

```bash
MDH_HARD_ADVISER_PLAY=1                      # Enable system
MDH_ADVISER_PLAY_PATH=assets/adviser/play.json  # Bias file
```

### Bias File Format

```json
{
  "QC": -50,    // Penalize Queen of Clubs
  "AS": +100,   // Favor Ace of Spades
  "2H": +25     // Slightly favor 2 of Hearts
}
```

Card format: `<rank><suit>` (e.g., "QC", "AS", "KH")

### Integration

```rust
pub fn play_bias(card: Card, ctx: &BotContext) -> i32
```

Applied after base scoring, before final selection:
```rust
final_score = base_score + continuation_score + adviser_bias
```

### Use Cases

- **Emergency fixes:** Patch AI behavior without rebuilds
- **Tuning exploration:** Rapid iteration on specific situations
- **Research:** Inject domain knowledge for experiments
- **A/B testing:** Test bias hypotheses in production


## Integration Patterns

### With Controller

```rust
// Controller calls bot for decision
let legal_moves = round.legal_plays(seat);
let planner = match difficulty {
    EasyLegacy => /* legacy path */,
    NormalHeuristic => PlayPlanner::choose(&legal_moves, &ctx),
    FutureHard => PlayPlannerHard::choose(&legal_moves, &ctx),
    SearchLookahead => /* future */,
};
```

Controller provides:
- Game state snapshot (RoundState, ScoreBoard)
- UnseenTracker with current knowledge
- Time/cancellation limits

Bot provides:
- Selected card (Option<Card>)
- Optional telemetry data

### With Game Core

Bot reads from core types but never mutates:
- `RoundState` - immutable trick/hand snapshot
- `ScoreBoard` - immutable score snapshot
- `Card`, `Rank`, `Suit` - const types

Mutation happens only in controller after bot decision.

### With Telemetry

Hard AI records detailed decision data:

```rust
pub struct HardDecisionTelemetry {
    pub seed: u64,
    pub trick_num: u8,
    pub position: PlayerPosition,
    pub candidates_scanned: u32,
    pub time_elapsed_ms: u32,
    pub base_scores: Vec<(Card, i32)>,
    pub continuation_scores: Vec<(Card, i32)>,
    pub final_scores: Vec<(Card, i32)>,
    pub selected: Card,
    // ... more fields
}
```

Exported to NDJSON for offline analysis.

### Threading Model

**Synchronous API:** Bot functions are synchronous (blocking)

**Async Execution:** Controller runs bot in worker thread:
```rust
thread::spawn(move || {
    let card = PlayPlannerHard::choose_with_limit(&legal, &ctx, limit);
    tx.send(card)
})
```

**Timeout Handling:** DecisionLimit supports both deadline and cancellation

**Fallback:** If bot times out, controller can:
- Use Normal AI as fallback
- Pick random legal card
- Pick safest card heuristically

## Performance Characteristics

### Normal AI
- **Typical:** 5-50 microseconds per decision
- **Worst case:** ~500 microseconds (complex scoring)
- **Deterministic:** Same hand → same decision

### Hard AI
- **Typical (deterministic 120 steps):** 2-15ms per decision
- **Typical (time-capped 10ms):** Varies by machine, usually 8-12ms
- **Worst case:** Hits time cap, partial search
- **Best case:** Early cutoff, <1ms

### Memory Footprint
- **BotContext:** ~1KB (mostly references)
- **UnseenTracker:** ~2KB (bitsets + void tracking)
- **Belief cache:** ~16KB (128 entries × 128 bytes)
- **Telemetry:** ~1KB per decision (when enabled)

### Scalability
- **Moves per second (Normal):** ~20,000
- **Moves per second (Hard, deterministic 120):** ~100-200
- **Moves per second (Hard, time-capped 10ms):** ~100

## Testing Strategy

### Unit Tests
- **mod.rs:** Style determination, score snapshots
- **play.rs:** Feature scoring, void detection, leader feed
- **search.rs:** Continuation scoring, belief sampling, tiering
- **pass.rs:** Pass selection, moon detection
- **tracker.rs:** Void inference, probability computation, moon state

### Integration Tests
- Snapshot-based regression tests
- Seed-based deterministic evaluation
- Normal vs. Hard comparison tests

### Evaluation Framework
- `--explain-once`: Single decision analysis
- `--compare-batch`: Normal vs. Hard comparison
- `--match-batch`: Head-to-head match simulation

## Design Rationale

### Why Shallow Search?
- **Performance:** 1-ply search fits in 10ms budget
- **Effectiveness:** Continuation scoring provides significant strength
- **Diminishing returns:** Hearts has high variance; deep search has limited benefit
- **Tunability:** Easier to understand and tune than deep search

### Why Heuristic Base?
- **Fast:** Normal AI evaluates candidates in microseconds
- **Strong baseline:** Well-tuned heuristics are competitive
- **Interpretable:** Clear feature attribution
- **Reliable:** Deterministic fallback if search times out

### Why Belief-Guided Sampling?
- **Realism:** Opponents don't play randomly
- **Performance:** Sampling is faster than exhaustive branching
- **Accuracy:** Top-K + diversity captures likely scenarios
- **Robustness:** Handles both certain and uncertain knowledge

### Why Deterministic Mode?
- **Regression testing:** Ensure changes don't break behavior
- **Performance measurement:** Fair comparisons across machines
- **Research:** Reproducible experiments
- **Debugging:** Eliminates timing-dependent bugs

## Future Directions

### Near Term
1. **Stage 2 completion:** Integrate all Stage 1/2 features by default
2. **Leverage tuning:** Refine tier thresholds based on evaluation data
3. **Endgame DP:** Enable perfect play for final 2-3 tricks
4. **Belief refinement:** Improve void inference accuracy

### Medium Term
1. **Multi-ply search:** Extend to 2-ply with alpha-beta pruning
2. **Opening book:** Precomputed optimal early-game plays
3. **Pattern recognition:** Detect common situations (smoke out, run diamonds, etc.)
4. **Opponent modeling:** Learn opponent tendencies during match

### Long Term
1. **Neural network integration:** Learned evaluation function
2. **Monte Carlo Tree Search:** MCTS for deeper lookahead
3. **Self-play training:** Reinforcement learning
4. **Adaptive difficulty:** Dynamic adjustment based on player skill

## Key Algorithms Summary

1. **Style Determination:** Hand quality + score analysis → BotStyle
2. **Normal Heuristic:** Feature-based scoring with trick simulation
3. **Hard Search:** Base + top-K continuation probing with belief sampling
4. **Pass Selection:** Priority-based ranking of cards to give away
5. **Void Inference:** Track fails-to-follow-suit for probability updates
6. **Moon Detection:** Multi-criteria heuristic with state machine
7. **Leverage Calculation:** Measure decision importance for adaptive search
8. **Belief Sampling:** Top-K + diversity from probability distribution
9. **Endgame DP:** Perfect play via dynamic programming (optional)

## Related Documentation

- **Configuration:** `README.md` "AI tuning (env weights)" section
- **Evaluation:** `docs/CLI_TOOLS.md`
- **Tuning Guide:** `docs/CONTRIBUTING_AI_TUNING.md`
- **Design Docs:** `designs/2025.10.21 - Stage 3 (Hard) plan.md` and related
- **Core Game Logic:** `crates/hearts-core/src/model/round.rs`
- **Controller Integration:** `crates/hearts-app/src/controller.rs`

## Conclusion

The mdhearts AI system is a **production-quality, research-grade card game AI** featuring:

✅ **Multiple difficulty levels** with distinct strategies
✅ **Sophisticated scoring** with 60+ tunable parameters
✅ **Shallow search with continuation** for Hard AI
✅ **Belief-guided opponent modeling** for realistic simulation
✅ **Adaptive search depth** via leverage-based tiering
✅ **Deterministic evaluation** for reproducible testing
✅ **Comprehensive telemetry** for offline analysis
✅ **Perfect endgame solving** (optional)
✅ **Extensive testing** with snapshot-based regression tests

**Architecture Strengths:**
- Clean separation of concerns (mod/play/search/pass/tracker/adviser)
- Immutable bot context for thread safety
- Configurable via environment for rapid tuning
- Fallback strategies for timeout/error handling
- Extensive unit test coverage

**Recommended for:**
- Other card game AI implementations
- Game AI research and education
- Performance-sensitive real-time AI
- Tunable AI systems with explainability

The system demonstrates that **well-engineered heuristics + shallow search + good opponent modeling** can produce expert-level play without requiring deep search or machine learning.
